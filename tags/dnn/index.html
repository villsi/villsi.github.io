<h1>This is taxonomy template!</h1><div class=res-cons><h3 class=archive-title>包含标签
<span class=keyword>DNN</span></h3><article class=post><header><h1 class=post-title><a href=https://blog.villsi.net/post/2023/efficientdnns/ target=_blank>EfficientDNNs</a></h1></header><div class=meta><div class=meta__left><span class=meta__catrgory><i class="bi bi-text-left"></i>
<span>Uncategorized</span>
</span><span class=meta__date><i class="bi bi-calendar3"></i>
<span><time>2023-08-03</time></span>
</span><span class="meta__words d-none d-sm-inline"><i class="bi bi-book"></i>
<span>6420 字</span>
</span><span class="meta__minutes d-none d-md-inline"><i class="bi bi-alarm"></i>
<span>31 分钟</span></span></div><div class="meta__right d-none d-xl-block"><span class=meta__tags><i class="bi bi-hash"></i>
<span>Deep Learning</span>
<i class="bi bi-hash"></i>
<span>DNN</span></span></div></div><div class=post-content>A collection of recent methods on DNN compression and acceleration. There are mainly 5 kinds of methods for efficient DNNs:
neural architecture re-design or search (NAS) maintain accuracy, less cost (e.g., #Params, #FLOPs, etc.): MobileNet, ShuffleNet etc. maintain cost, more accuracy: Inception, ResNeXt, Xception etc. pruning (including structured and unstructured) quantization matrix/low-rank decomposition knowledge distillation (KD) Note, this repo is more about pruning (with lottery ticket hypothesis or LTH as a sub-topic), KD, and quantization.……<p class=readmore><a href=https://blog.villsi.net/post/2023/efficientdnns/ target=_blank></a></p></div></article></div>