<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on</title><link>https://blog.villsi.net/tags/deep-learning/</link><description>Recent content in Deep Learning on</description><generator>Hugo</generator><language>en</language><lastBuildDate>Mon, 01 Apr 2024 10:02:41 +0800</lastBuildDate><atom:link href="https://blog.villsi.net/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>EfficientDNNs</title><link>https://blog.villsi.net/post/2023/efficientdnns/</link><pubDate>Thu, 03 Aug 2023 23:00:00 +0800</pubDate><guid>https://blog.villsi.net/post/2023/efficientdnns/</guid><description>A collection of recent methods on DNN compression and acceleration. There are mainly 5 kinds of methods for efficient DNNs:
neural architecture re-design or search (NAS) maintain accuracy, less cost (e.g., #Params, #FLOPs, etc.): MobileNet, ShuffleNet etc. maintain cost, more accuracy: Inception, ResNeXt, Xception etc. pruning (including structured and unstructured) quantization matrix/low-rank decomposition knowledge distillation (KD) Note, this repo is more about pruning (with lottery ticket hypothesis or LTH as a sub-topic), KD, and quantization.</description></item></channel></rss>