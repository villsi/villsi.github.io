<!doctype html><html lang=zh><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.123.7"><link rel=preconnect href=https://cdn.jsdelivr.net><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css2?display=swap&family=Inter:wght@300;400;500&family=Noto+Sans+SC:wght@300;400;500&family=Noto+Sans+JP:wght@300;400;500&family=Fira+Code:wght@300;400;500"><title>Learn Data Science - 04 ML & Text Mining | 上海红茶馆</title>
<meta name=author content="我喜欢煎蛋卷"><meta name=description content="This Repository Consists of Free Resources needed for a person to learn Data Science from the beginning to end. This repository is divided into four main Parts. And This post contains mechine learning & text mining."><meta name=keywords content='Github,Programming'><link rel=icon href=https://img.villsi.net/2024/01/6774b317d88c2bd3f564143548d8683e.png sizes=any><link rel=icon sizes=192x192 href=https://img.villsi.net/2024/01/6774b317d88c2bd3f564143548d8683e.png><link rel=icon sizes=512x512 href=https://img.villsi.net/2024/01/6774b317d88c2bd3f564143548d8683e.png><link rel=apple-touch-icon sizes=180x180 href=https://img.villsi.net/2024/01/6774b317d88c2bd3f564143548d8683e.png><meta property="og:title" content="Learn Data Science - 04 ML & Text Mining | 上海红茶馆"><meta name=twitter:title content="Learn Data Science - 04 ML & Text Mining | 上海红茶馆"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.villsi.net/post/2023/learn-data-science-for-free_04/"><meta property="og:description" content="This Repository Consists of Free Resources needed for a person to learn Data Science from the beginning to end. This repository is divided into four main Parts. And This post contains mechine learning & text mining."><meta name=twitter:description content="This Repository Consists of Free Resources needed for a person to learn Data Science from the beginning to end. This repository is divided into four main Parts. And This post contains mechine learning & text mining."><meta property="og:image" content="https://img.villsi.net/2023/12/fd029546dac00eb0a7db2781444cb6c2.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://img.villsi.net/2023/12/fd029546dac00eb0a7db2781444cb6c2.jpg"><meta property="article:published_time" content="2023-12-03T19:00:00+08:00"><meta property="article:modified_time" content="2024-02-09T12:29:09+08:00"><link rel=alternate type=application/atom+xml href=https://blog.villsi.net/index.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha2/dist/css/bootstrap.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.0/font/bootstrap-icons.css><link rel=stylesheet href=https://blog.villsi.net/assets/main.min.css><script defer src=https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/twemoji@14.0.2/dist/twemoji.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/quicklink@2.3.0/dist/quicklink.umd.js></script><script defer src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha2/dist/js/bootstrap.bundle.min.js></script><script defer src=https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js></script><script defer src=https://blog.villsi.net/assets/main.min.js></script></head><body data-theme=auto data-section=single><header class=header id=my-header><nav class="navbar shadow-sm fixed-top navbar-expand-md navbar-light bg-light"><div class="container p-0"><a class=navbar-brand href=https://blog.villsi.net/><img src=https://img.villsi.net/2024/01/6774b317d88c2bd3f564143548d8683e.png alt=Logo width=40 height=40 class=align-middle>
<span class="d-inline align-middle">上海红茶馆</span>
</a><button class="navbar-toggler border border-0" type=button data-bs-toggle=collapse data-bs-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav me-auto mb-2 mb-md-0 fw-normal"><li class=nav-item><a class="nav-link ms-4 text-center active" href=https://blog.villsi.net/post/>文章</a></li><li class=nav-item><a class="nav-link ms-4 text-center" href=https://blog.villsi.net/tagcloud/>标签</a></li><li class=nav-item><a class="nav-link ms-4 text-center" href=https://blog.villsi.net/code/>笔记</a></li><li class=nav-item><a class="nav-link ms-4 text-center" href=https://blog.villsi.net/echart/>关于</a></li><li class=nav-item><a class="nav-link ms-4 text-center" href=https://blog.villsi.net/playlist/>番剧</a></li></ul></div></div></nav></header><main class="main fix-padding-top"><div class=container><div class="row g-3"><div class="col-xl-2 col-lg-3 px-0 d-none d-lg-block"><aside class="sticky-top fix-sidebar-top"><div class=sidebar_sticky><section class="d-none d-sm-block"><div class="card shadow-sm border-0 card-body mb-3"><div class="pb-1 text-center"><img class="img-fluid rounded w-50" loading=lazy src=https://img.villsi.net/2023/12/fd029546dac00eb0a7db2781444cb6c2.jpg alt=头像></div><div class="pb-2 text-center"><div class=text-primary>我喜欢煎蛋卷</div><div class="text-secondary fst-italic fw-lighter small">Per aspera ad astra.</div></div><div class="pb-2 small text-center row m-0"><div class="col px-0"><div class=text-primary>308</div><div class=text-secondary>千字</div></div><div class="col px-0"><div class=text-primary>37</div><div class=text-secondary>文章</div></div><div class="col px-0"><div class=text-primary>15</div><div class=text-secondary>标签</div></div></div><div class=btn-group role=group><a class="btn btn-link link-dark p-0" href=https://github.com/villsi target=_blank rel=noopener title=GitHub><i class="bi bi-github"></i>
</a><a class="btn btn-link link-dark p-0" href=https://twitter.com/villsi target=_blank rel=noopener title=Twitter><i class="bi bi-twitter"></i>
</a><a class="btn btn-link link-dark p-0" href=https://steamcommunity.com/id/villsi/ target=_blank rel=noopener title=Steam><i class="bi bi-steam"></i></a></div></div></section><section class="d-none d-sm-block"><div class="card shadow-sm border-0 mb-3"><div class="card-header bg-white border-light-subtle"><span class="text-uppercase fw-medium">Categories</span></div><div class="card-body py-2"><ul class="list-group list-group-flush small"><li class="list-group-item d-flex justify-content-between align-items-center px-0 border-light-subtle"><a class="text-truncate link-underline-primary link-offset-0 link-underline-opacity-0 link-underline-opacity-75-hover" href=https://blog.villsi.net/categories/blog/>Blog
</a><span class="badge bg-primary rounded-pill">2</span></li><li class="list-group-item d-flex justify-content-between align-items-center px-0 border-light-subtle"><a class="text-truncate link-underline-primary link-offset-0 link-underline-opacity-0 link-underline-opacity-75-hover" href=https://blog.villsi.net/categories/uncategorized/>Uncategorized
</a><span class="badge bg-primary rounded-pill">15</span></li><li class="list-group-item d-flex justify-content-between align-items-center px-0 border-light-subtle"><a class="text-truncate link-underline-primary link-offset-0 link-underline-opacity-0 link-underline-opacity-75-hover" href=https://blog.villsi.net/categories/%E5%8D%9A%E5%AE%A2%E8%A3%85%E4%BF%AE/>博客装修
</a><span class="badge bg-primary rounded-pill">1</span></li><li class="list-group-item d-flex justify-content-between align-items-center px-0 border-light-subtle"><a class="text-truncate link-underline-primary link-offset-0 link-underline-opacity-0 link-underline-opacity-75-hover" href=https://blog.villsi.net/categories/%E8%B7%91%E5%9B%A2%E8%AE%B0%E5%BD%95/>跑团记录
</a><span class="badge bg-primary rounded-pill">10</span></li></ul></div></div></section><section class="d-none d-sm-block"><div class="card shadow-sm border-0 mb-3"><div class="card-header bg-white border-light-subtle"><span class="text-uppercase fw-medium">Tag Cloud</span></div><div class=card-body><div class=grid><a href=https://blog.villsi.net/tags/blog/ title=Blog class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">Blog</span>
</a><a href=https://blog.villsi.net/tags/cloudflare/ title=Cloudflare class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">Cloudflare</span>
</a><a href=https://blog.villsi.net/tags/deep-learning/ title="Deep Learning" class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">Deep Learning</span>
</a><a href=https://blog.villsi.net/tags/dnn/ title=DNN class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">DNN</span>
</a><a href=https://blog.villsi.net/tags/game/ title=Game class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">Game</span>
</a><a href=https://blog.villsi.net/tags/github/ title=GitHub class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">GitHub</span>
</a><a href=https://blog.villsi.net/tags/katex/ title=Katex class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">Katex</span>
</a><a href=https://blog.villsi.net/tags/markdown/ title=Markdown class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">Markdown</span>
</a><a href=https://blog.villsi.net/tags/obsidian/ title=Obsidian class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">Obsidian</span>
</a><a href=https://blog.villsi.net/tags/programming/ title=Programming class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">Programming</span>
</a><a href=https://blog.villsi.net/tags/trpg/ title=TRPG class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">TRPG</span>
</a><a href=https://blog.villsi.net/tags/typora/ title=Typora class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">Typora</span>
</a><a href=https://blog.villsi.net/tags/%E5%86%B0%E9%A3%8E%E8%B0%B7/ title=冰风谷 class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">冰风谷</span>
</a><a href=https://blog.villsi.net/tags/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/ title=经验总结 class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">经验总结</span>
</a><a href=https://blog.villsi.net/tags/%E9%9A%8F%E6%89%8B%E6%91%98%E5%BD%95/ title=随手摘录 class="btn btn-sm btn-outline-primary py-0 my-1"><span class="d-flex text-uppercase fw-light small">随手摘录</span></a></div></div></div></section></aside></div><div class="col-xl-8 col-lg-9 px-3"><div class=content><div class="card mb-3 shadow-sm border-0 overflow-hidden"><div class=header-image><div class="ratio ratio-21x9 overflow-hidden bg-black"><a href=https://blog.villsi.net/post/2023/learn-data-science-for-free_04/><img class="card-img-top opacity-50" loading=lazy src=https://picsum.photos/seed/Learn%20Data%20Science%20-%2004%20ML%20&%20Text%20Mining/1600/1200 alt=博客文章头图></a><div class="card-img-overlay row align-items-center"><div class="text-light text-center"><h2>Learn Data Science - 04 ML & Text Mining</h2><div><div class="d-block small"><div class="d-inline pe-3"><i class="bi bi-calendar3 pe-2"></i><time>2023-12-03</time></div><div class="d-inline pe-3"><i class="bi bi-book pe-1"></i>
<span>4290 字</span></div><div class=d-inline><i class="bi bi-alarm pe-1"></i>
<span>21 分钟</span></div></div><div class="d-none d-xl-block small"><div class="d-md-inline ps-2"><i class="bi bi-hash"></i>
<span>Github</span></div><div class="d-md-inline ps-2"><i class="bi bi-hash"></i>
<span>Programming</span></div></div></div></div></div></div></div><article class="card-body markdown"><h1 id=4-machine-learning>4 Machine learning</h1><h2 id=1-what-is-ml->1 What is ML ?</h2><h3 id=definition>Definition</h3><p>Machine Learning is part of the Artificial Intelligences study. It concerns the conception, devloppement and implementation of sophisticated methods, allowing a machine to achieve really hard tasks, nearly impossible to solve with classic algorithms.</p><p>Machine learning mostly consists of three algorithms:</p><p><a href=https:/img.villsi.net/2023/12/30891e7d83e210cbc1128fb41dd8392e.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=https:/img.villsi.net/2023/12/30891e7d83e210cbc1128fb41dd8392e.png alt=ml loading=lazy></a></p><h3 id=utilisation-examples>Utilisation examples</h3><ul><li>Computer vision</li><li>Search engines</li><li>Financial analysis</li><li>Documents classification</li><li>Music generation</li><li>Robotics &mldr;</li></ul><h2 id=2-numerical-var>2 Numerical var</h2><p>Variables which can take continous integer or real values. They can take infinite values.</p><p>These types of variables are mostly used for features which involves measurements. For example, hieghts of all students in a class.</p><h2 id=3-categorical-var>3 Categorical var</h2><p>Variables that take finite discrete values. They take a fixed set of values, in order to classify a data item.</p><p>They act like assigned labels. For example: Labelling the students of a class according to gender: &lsquo;Male&rsquo; and &lsquo;Female&rsquo;</p><h2 id=4-supervised-learning>4 Supervised learning</h2><p>Supervised learning is the machine learning task of inferring a function from <strong>labeled training data</strong>.</p><p>The training data consist of a <strong>set of training examples</strong>.</p><p>In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal).</p><p>A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples.</p><p>In other words:</p><p>Supervised Learning learns from a set of labeled examples. From the instances and the labels, supervised learning models try to find the correlation among the features, used to describe an instance, and learn how each feature contributes to the label corresponding to an instance. On receiving an unseen instance, the goal of supervised learning is to label the instance based on its feature correctly.</p><p><strong>An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances</strong>.</p><h2 id=5-unsupervised-learning>5 Unsupervised learning</h2><p>Unsupervised machine learning is the machine learning task of inferring a function to describe hidden structure _<em>from &ldquo;unlabeled&rdquo; data</em> (a classification or categorization is not included in the observations).</p><p>Since the examples given to the learner are unlabeled, there is no evaluation of the accuracy of the structure that is output by the relevant algorithm—which is one way of distinguishing unsupervised learning from supervised learning and reinforcement learning.</p><p>Unsupervised learning deals with data instances only. This approach tries to group data and form clusters based on the similarity of features. If two instances have similar features and placed in close proximity in feature space, there are high chances the two instances will belong to the same cluster. On getting an unseen instance, the algorithm will try to find, to which cluster the instance should belong based on its feature.</p><p>Resource:</p><p><a href=https://towardsdatascience.com/a-dive-into-unsupervised-learning-bf1d6b5f02a7 target=_blank rel=noopener>Guide to unsupervised learning</a></p><h2 id=6-concepts-inputs-and-attributes>6 Concepts, inputs and attributes</h2><p>A machine learning problem takes in the features of a dataset as input.</p><p>For supervised learning, the model trains on the data and then it is ready to perform. So, for supervised learning, apart from the features we also need to input the corresponding labels of the data points to let the model train on them.</p><p>For unsupervised learning, the models simply perform by just citing complex relations among data items and grouping them accordingly. So, unsupervised learning do not need a labelled dataset. The input is only the feature section of the dataset.</p><h2 id=7-training-and-test-data>7 Training and test data</h2><p>If we train a supervised machine learning model using a dataset, the model captures the dependencies of that particular data set very deeply. So, the model will always perform well on the data and it won&rsquo;t be proper measure of how well the model performs.</p><p>To know how well the model performs, we must train and test the model on different datasets. The dataset we train the model on is called Training set, and the dataset we test the model on is called the test set.</p><p>We normally split the provided dataset to create the training and test set. The ratio of splitting is majorly: 3:7 or 2:8 depending on the data, larger being the trining data.</p><h3 id=sklearnmodel_selectiontrain_test_split-is-used-for-splitting-the-data>sklearn.model_selection.train_test_split is used for splitting the data.</h3><p>Syntax:</p><pre><code>from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
</code></pre><p><a href=https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html target=_blank rel=noopener>Sklearn docs</a></p><h2 id=8-classifiers>8 Classifiers</h2><p>Classification is the most important and most common machine learning problem. Classification problems can be both suprvised and unsupervised problems.</p><p>The classification problems involve labelling data points to belong to a particular class based on the feature set corresponding to the particluar data point.</p><p>Classification tasks can be performed using both machine learning and deep learning techniques.</p><p>Machine learning classification techniques involve: Logistic Regressions, SVMs, and Classification trees. The models used to perform the classification are called classifiers.</p><h2 id=9-prediction>9 Prediction</h2><p>The output generated by a machine learning models for a particuolar problem is called its prediction.</p><p>There are majorly two kinds of predictions corresponding to two types of problen:</p><ol><li><p>Classification</p></li><li><p>Regression</p></li></ol><p>In classiication, the prediction is mostly a class or label, to which a data points belong</p><p>In regression, the prediction is a number, a continous a numeric value, because regression problems deal with predicting the value. For example, predicting the price of a house.</p><h2 id=10-lift>10 Lift</h2><h2 id=11-overfitting>11 Overfitting</h2><p>Often we train our model so much or make our model so complex that our model fits too tghtly with the training data.</p><p>The training data often contains outliers or represents misleading patterns in the data. Fitting the training data with such irregularities to deeply cause the model to lose its generalization. The model performs very well on the training set but not so good on the test set.</p><p><a href=/images/9e198f756486877ce26ad40a3a656ad1.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=/images/9e198f756486877ce26ad40a3a656ad1.png alt=overfitting loading=lazy></a></p><p>As we can see on training further a point the training error decreases and testing error increases.</p><p>A hypothesis h1 is said to overfit iff there exists another hypothesis h where h gives more error than h1 on training data and less error than h1 on the test data</p><h2 id=12-bias--variance>12 Bias & variance</h2><p>Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.</p><p>Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.</p><p>Basically High variance causes overfitting and high bias causes underfitting. We want our model to have low bias and low variance to perform perfectly. We need to avoid a model with higher variance and high bias</p><p><a href=/images/e64ecc664dcf2e360189e592df09cd33.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=/images/e64ecc664dcf2e360189e592df09cd33.png alt=bias&amp;amp;variance loading=lazy></a></p><p>We can see that for Low bias and Low Variance our model predicts all the data points correctly. Again in the last image having high bias and high variance the model predicts no data point correctly.</p><p><a href=https:/img.villsi.net/2023/12/33e0283ebb40e1347ab23e9f7d0b0208.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=https:/img.villsi.net/2023/12/33e0283ebb40e1347ab23e9f7d0b0208.png alt=B&amp;amp;v2 loading=lazy></a></p><p>We can see from the graph that rge Error increases when the complex is either too complex or the model is too simple. The bias increases with simpler model and Variance increases with complex models.</p><p>This is one of the most important tradeoffs in machine learning</p><h2 id=13-tree-and-classification>13 Tree and classification</h2><p>We have previously talked about classificaion. We have seen the most used methods are Logistic Regression, SVMs and decision trees. Now, if the decision boundary is linear the methods like logistic regression and SVM serves best, but its a complete scenerio when the decision boundary is non linear, this is where decision tree is used.</p><p><a href=/images/286220d265dbef31fe74d7d89bde1ca9.jpeg target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=/images/286220d265dbef31fe74d7d89bde1ca9.jpeg alt=tree loading=lazy></a></p><p>The first image shows linear decision boundary and second image shows non linear decision boundary.</p><p>Ih the cases, for non linear boundaries, the decision trees condition based approach work very well for classification problems. The algorithm creates conditions on features to drive and reach a decision, so is independent of functions.</p><p><a href=/images/6afa84ccc72a72f0534cc8e76a73a65d.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=/images/6afa84ccc72a72f0534cc8e76a73a65d.png alt=tree2 loading=lazy></a></p><p>Decision tree approach for classification</p><h2 id=14-classification-rate>14 Classification rate</h2><h2 id=15-decision-tree>15 Decision tree</h2><p>Decision Trees are some of the most used machine learning algorithms. They are used for both classification and Regression. They can be used for both linear and non-linear data, but they are mostly used for non-linear data. Decision Trees as the name suggests works on a set of decisions derived from the data and its behavior. It does not use a linear classifier or regressor, so its performance is independent of the linear nature of the data.</p><p>One of the other most important reasons to use tree models is that they are very easy to interpret.</p><p>Decision Trees can be used for both classification and regression. The methodologies are a bit different, though principles are the same. The decision trees use the CART algorithm (Classification and Regression Trees)</p><p>Resource:</p><p><a href=https://towardsdatascience.com/a-dive-into-decision-trees-a128923c9298 target=_blank rel=noopener>Guide to Decision Tree</a></p><h2 id=16-boosting>16 Boosting</h2><h4 id=ensemble-learning>Ensemble Learning</h4><p>It is the method used to enhance the performance of the Machine learning models by combining several number of models or weak learners. They provide improved efficiency.</p><p>There are two types of ensemble learning:</p><p><strong>1. Parallel ensemble learning or bagging method</strong></p><p><strong>2. Sequential ensemble learning or boosting method</strong></p><p>In parallel method or bagging technique, several weak classifiers are created in parallel. The training datasets are created randomly on a bootstrapping basis from the original dataset. The datasets used for the training and creation phases are weak classifiers. Later during predictions, the reults from all the classifiers are bagged together to provide the final results.</p><p><a href=https:/img.villsi.net/2023/12/d919cf917fa2862b10159ba0ce6a1186.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=https:/img.villsi.net/2023/12/d919cf917fa2862b10159ba0ce6a1186.png alt=bag loading=lazy></a></p><p>Ex: Random Forests</p><p>In sequential learning or boosting weak learners are created one after another and the data sample set are weighted in such a manner that during creation, the next learner focuses on the samples that were wrongly predicted by the previous classifier. So, at each step, the classifier improves and learns from its previous mistakes or misclassifications.</p><p><a href=/images/45908f0b9604400adaef661dfe0d870c.jpg target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=/images/45908f0b9604400adaef661dfe0d870c.jpg alt=boosting loading=lazy></a></p><p>There are mostly three types of boosting algorithm:</p><p><strong>1. Adaboost</strong></p><p><strong>2. Gradient Boosting</strong></p><p><strong>3. XGBoost</strong></p><p>_<em>Adaboost</em> algorithm works in the exact way describe. It creates a weak learner, also known as stumps, they are not full grown trees, but contain a single node based on which the classification is done. The misclassifications are observed and they are weighted more than the correctly classified ones while training the next weak learner.</p><p>_<em>sklearn.ensemble.AdaBoostClassifier</em> is used for the application of the classifier on real data in python.</p><p><a href=/images/c97e86a66999f75c90eb2cb9bce37697.jpg target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=/images/c97e86a66999f75c90eb2cb9bce37697.jpg alt=adaboost loading=lazy></a></p><p>Reources:</p><p><a href="https://blog.paperspace.com/adaboost-optimizer/#:~:text=AdaBoost%20is%20an%20ensemble%20learning,turn%20them%20into%20strong%20ones." target=_blank rel=noopener>Understanding</a></p><p>_<em>Gradient Boosting</em> algorithm starts with a node giving 0.5 as output for both classification and regression. It serves as the first stump or weak learner. We then observe the Errors in predictions. Now, we create other learners or decision trees to actually predict the errors based on the conditions. The errors are called Residuals. Our final output is:</p><p><strong>0.5 (Provided by the first learner) + The error provided by the second tree or learner.</strong></p><p>Now, if we use this method, it learns the predictions too tightly, and loses generalization. In order to avoid that gradient boosting uses a learning parameter <em>alpha</em>.</p><p>So, the final results after two learners is obtained as:</p><p>_<em>0.5 (Provided by the first learner) + <em>alpha X (The error provided by the second tree or learner.)</em></em></p><p>We can see that using the added portion we take a small leap towards the correct results. We continue adding learners until the point we are very close to the actual value given by the training set.</p><p>Overall the equation becomes:</p><p>__0.5 (Provided by the first learner) + <em>alpha X (The error provided by the second tree or learner.)+ <em>alpha X (The error provided by the third tree or learner.)+&mldr;&mldr;&mldr;&mldr;.</em></em></p><p>_<em>sklearn.ensemble.GradientBoostingClassifier</em> used to apply gradient boosting in python</p><p><a href=/images/c7c2c45acaf3b5fe8aa9e4aed493aebc.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=/images/c7c2c45acaf3b5fe8aa9e4aed493aebc.png alt=GBM loading=lazy></a></p><p>Resource:</p><p><a href=https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d target=_blank rel=noopener>Guide</a></p><h2 id=17-naïves-bayes-classifiers>17 Naïves Bayes classifiers</h2><p>The Naive Bayes classifiers are a collection of classification algorithms based on <strong>Bayes’ Theorem.</strong></p><p>Bayes theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. It is given by:</p><p><a href=https:/img.villsi.net/2023/12/92547bc4531e15a256286d3baca05b02.svg target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=https:/img.villsi.net/2023/12/92547bc4531e15a256286d3baca05b02.svg alt=bayes loading=lazy></a></p><p>Where P(A|B) is the probabaility of occurrence of A knowing B already occurred and P(B|A) is the probability of occurrence of B knowing A occurred.</p><p><a href=https://github.com/abr-98/data-scientist-roadmap/edit/master/04_Machine-Learning/README.md target=_blank rel=noopener>Scikit-learn Guide</a></p><p>There are mostly two types of Naive Bayes:</p><p><strong>1. Gaussian Naive Bayes</strong></p><p><strong>2. Multinomial Naive Bayes.</strong></p><h4 id=multinomial-naive-bayes>Multinomial Naive Bayes</h4><p>The method is used mostly for document classification. For example, classifying an article as sports article or say film magazine. It is also used for differentiating actual mails from spam mails. It uses the frequency of words used in different magazine to make a decision.</p><p>For example, the word &ldquo;Dear&rdquo; and &ldquo;friends&rdquo; are used a lot in actual mails and &ldquo;offer&rdquo; and &ldquo;money&rdquo; are used a lot in &ldquo;Spam&rdquo; mails. It calculates the prorbability of the occurrence of the words in case of actual mails and spam mails using the training examples. So, the probability of occurrence of &ldquo;money&rdquo; is much higher in case of spam mails and so on.</p><p>Now, we calculate the probability of a mail being a spam mail using the occurrence of words in it.</p><h4 id=gaussian-naive-bayes>Gaussian Naive Bayes</h4><p>When the predictors take up a continuous value and are not discrete, we assume that these values are sampled from a gaussian distribution.</p><p><a href=https:/img.villsi.net/2023/12/018b143af3e48317b78f5b448f69506e.gif target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=https:/img.villsi.net/2023/12/018b143af3e48317b78f5b448f69506e.gif alt=gnb loading=lazy></a></p><p>It links guassian distribution and Bayes theorem.</p><p>Resources:</p><p><a href=https://youtu.be/H3EjCKtlVog target=_blank rel=noopener>GUIDE</a></p><h2 id=18-k-nearest-neighbor>18 K-Nearest neighbor</h2><p>K-nearest neighbour algorithm is the most basic and still essential algorithm. It is a memory based approach and not a model based one.</p><p>KNN is used in both supervised and unsupervised learning. It simply locates the data points across the feature space and used distance as a similarity metrics.</p><p>Lesser the distance between two data points, more similar the points are.</p><p>In K-NN classification algorithm, the point to classify is plotted on the feature space and classified as the class of its nearest K-neighbours. K is the user parameter. It gives the measure of how many points we should consider while deciding the label of the point concerned. If K is more than 1 we consider the label that is in majority.</p><p>If the dataset is very large, we can use a large k. The large k is less effected by noise and generates smooth boundaries. For small dataset, a small k must be used. A small k helps to notice the variation in boundaries better.</p><p><a href=https://img.villsi.net/2023/12/9483bd7d8e73e98e35e2d2deb58a59c8.jpg target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=https://img.villsi.net/2023/12/9483bd7d8e73e98e35e2d2deb58a59c8.jpg alt=knn loading=lazy></a></p><p>Resource:</p><p><a href=https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761 target=_blank rel=noopener>GUIDE</a></p><h2 id=19-logistic-regression>19 Logistic regression</h2><p>Regression is one of the most important concepts used in machine learning.</p><p><a href=https://towardsdatascience.com/a-deep-dive-into-the-concept-of-regression-fb912d427a2e target=_blank rel=noopener>Guide to regression</a></p><p>Logistic Regression is the most used classification algorithm for linearly seperable datapoints. Logistic Regression is used when the dependent variable is categorical.</p><p>It uses the linear regression equation:</p><p><strong>Y= w1x1+w2x2+w3x3……..wkxk</strong></p><p>in a modified format:</p><p><strong>Y= 1/ 1+e^-(w1x1+w2x2+w3x3……..wkxk)</strong></p><p>This modification ensures the value always stays between 0 and 1. Thus, making it feasible to be used for classification.</p><p>The above equation is called _<em>Sigmoid</em> function. The function looks like:</p><p><a href=/images/31d7244bc2578896d82022271e26e828.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=/images/31d7244bc2578896d82022271e26e828.png alt=Logreg loading=lazy></a></p><p>The loss fucnction used is called logloss or binary cross-entropy.</p><p><strong>Loss= —Y_actual. log(h(x)) —(1 — Y_actual.log(1 — h(x)))</strong></p><p>If Y_actual=1, the first part gives the error, else the second part.</p><p><a href=/images/27fa80c136cab260786f647fa81f2bc7.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=/images/27fa80c136cab260786f647fa81f2bc7.png alt=loss loading=lazy></a></p><p>Logistic Regression is used for multiclass classification also. It uses softmax regresssion or One-vs-all logistic regression.</p><p><a href=https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc target=_blank rel=noopener>Guide to logistic Regression</a></p><p>_<em>sklearn.linear_model.LogisticRegression</em> is used to apply logistic Regression in python.</p><h2 id=20-ranking>20 Ranking</h2><h2 id=21-linear-regression>21 Linear regression</h2><p>Regression tasks deal with predicting the value of a dependent variable from a set of independent variables i.e, the provided features. Say, we want to predict the price of a car. So, it becomes a dependent variable say Y, and the features like engine capacity, top speed, class, and company become the independent variables, which helps to frame the equation to obtain the price.</p><p>Now, if there is one feature say x. If the dependent variable y is linearly dependent on x, then it can be given by y=mx+c, where the m is the coefficient of the feature in the equation, c is the intercept or bias. Both M and C are the model parameters.</p><p>We use a loss function or cost function called Mean Square error of (MSE). It is given by the square of the difference between the actual and the predicted value of the dependent variable.</p><p><strong>MSE=1/2m * (Y_actual — Y_pred)²</strong></p><p>If we observe the function we will see its a parabola, i.e, the function is convex in nature. This convex function is the principle used in Gradient Descent to obtain the value of the model parameters</p><p><a href=https:/img.villsi.net/2023/12/1c5548e494c12860ee0dfc4685e41f56.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=https:/img.villsi.net/2023/12/1c5548e494c12860ee0dfc4685e41f56.png alt=loss loading=lazy></a></p><p>The image shows the loss function.</p><p>To get the correct estimate of the model parameters we use the method of <strong>Gradient Descent</strong></p><p><a href=https://towardsdatascience.com/an-introduction-to-gradient-descent-and-backpropagation-81648bdb19b2 target=_blank rel=noopener>Guide to Gradient Descent</a></p><p><a href=https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86 target=_blank rel=noopener>Guide to linear Regression</a></p><p>_<em>sklearn.linear_model.LinearRegression</em> is used to apply linear regression in python</p><h2 id=22-perceptron>22 Perceptron</h2><p>The perceptron has been the first model described in the 50ies.</p><p>This is a <strong>binary classifier</strong>, ie it can&rsquo;t separate more than 2 groups, and thoses groups have to be <strong>linearly separable</strong>.</p><p>The perceptron <strong>works like a biological neuron</strong>. It calculate an activation value, and if this value if positive, it returns 1, 0 otherwise.</p><h2 id=23-hierarchical-clustering>23 Hierarchical clustering</h2><p>The hierarchical algorithms are so-called because they create tree-like structures to create clusters. These algorithms also use a distance-based approach for cluster creation.</p><p>The most popular algorithms are:</p><p><strong>Agglomerative Hierarchical clustering</strong></p><p><strong>Divisive Hierarchical clustering</strong></p><p><strong>Agglomerative Hierarchical clustering</strong>: In this type of hierarchical clustering, each point initially starts as a cluster, and slowly the nearest or similar most clusters merge to create one cluster.</p><p><strong>Divisive Hierarchical Clustering</strong>: The type of hierarchical clustering is just the opposite of Agglomerative clustering. In this type, all the points start as one large cluster and slowly the clusters get divided into smaller clusters based on how large the distance or less similarity is between the two clusters. We keep on dividing the clusters until all the points become individual clusters.</p><p>For agglomerative clustering, we keep on merging the clusters which are nearest or have a high similarity score to one cluster. So, if we define a cut-off or threshold score for the merging we will get multiple clusters instead of a single one. For instance, if we say the threshold similarity metrics score is 0.5, it means the algorithm will stop merging the clusters if no two clusters are found with a similarity score less than 0.5, and the number of clusters present at that step will give the final number of clusters that need to be created to the clusters.</p><p>Similarly, for divisive clustering, we divide the clusters based on the least similarity scores. So, if we define a score of 0.5, it will stop dividing or splitting if the similarity score between two clusters is less than or equal to 0.5. We will be left with a number of clusters and it won’t reduce to every point of the distribution.</p><p>The process is as shown below:</p><p><a href=https:/img.villsi.net/2023/12/f1ef8de72ffd51b8888956839777879b.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=https:/img.villsi.net/2023/12/f1ef8de72ffd51b8888956839777879b.png alt=HC loading=lazy></a></p><p>One of the most used methods for the measuring distance and applying cutoff is the dendrogram method.</p><p>The dendogram for above clustering is:</p><p><a href=https:/img.villsi.net/2023/12/7f1b81c99f560c71e88d1490fe094c71.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=https:/img.villsi.net/2023/12/7f1b81c99f560c71e88d1490fe094c71.png alt=Dend loading=lazy></a></p><p><a href=https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec target=_blank rel=noopener>Guide</a></p><h2 id=24-k-means-clustering>24 K-means clustering</h2><p>The algorithm initially creates K clusters randomly using N data points and finds the mean of all the point values in a cluster for each cluster. So, for each cluster we find a central point or centroid calculating the mean of the values of the cluster. Then the algorithm calculates the sum of squared error (SSE) for each cluster. SSE is used to measure the quality of clusters. If a cluster has large distances between the points and the center, then the SSE will be high and if we check the interpretation it allows only points in the close vicinity to create clusters.</p><p>The algorithm works on the principle that the points lying close to a center of a cluster should be in that cluster. So, if a point x is closer to the center of cluster A than cluster B, then x will belong to cluster A. Thus a point enters a cluster and as even a single point moves from one cluster to another, the centroid changes and so does the SSE. We keep doing this until the SSE decreases and the centroid does not change anymore. After a certain number of shifts, the optimal clusters are found and the shifting stops as the centroids don’t change any more.</p><p>The initial number of clusters ‘K’ is a user parameter.</p><p>The image shows the method</p><p><a href=https:/img.villsi.net/2023/12/7e79e4efd1c72a27fdea8de49fbdec7f.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=https:/img.villsi.net/2023/12/7e79e4efd1c72a27fdea8de49fbdec7f.png alt=Kmeans loading=lazy></a></p><p>We have seen that for this type of clustering technique we need a user-defined parameter ‘K’ which defines the number of clusters that need to be created. Now, this is a very important parameter. To, find this parameter a number of methods are used. The most important and used method is the elbow method.
For smaller datasets, k=(N/2)^(1/2) or the square root of half of the number of points in the distribution.</p><p><a href=https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1 target=_blank rel=noopener>Guide</a></p><h2 id=25-neural-networks>25 Neural networks</h2><p>Neural Networks are a set of interconnected layers of artificial neurons or nodes. They are frameworks that are modeled keeping in mind, the structure and working of the human brain. They are meant for predictive modeling and applications where they can be trained via a dataset. They are based on self-learning algorithms and predict based on conclusions and complex relations derived from their training sets of information.</p><p>A typical Neural Network has a number of layers. The First Layer is called the Input Layer and The Last layer is called the Output Layer. The layers between the Input and Output layers are called Hidden Layers. It basically functions like a Black Box for prediction and classification. All the layers are interconnected and consist of numerous artificial neurons called Nodes.</p><p><a href=https://medium.com/ai-in-plain-english/neural-networks-overview-e6ea484a474e target=_blank rel=noopener>Guide to nueral Networks</a></p><p>Neural networks are too complex to work on Gradient Descent algorithms, so it works on the principles of Backproapagations and Optimizers.</p><p><a href=https://towardsdatascience.com/an-introduction-to-gradient-descent-and-backpropagation-81648bdb19b2 target=_blank rel=noopener>Guide to Backpropagation</a></p><p><a href=https://towardsdatascience.com/introduction-to-gradient-descent-weight-initiation-and-optimizers-ee9ae212723f target=_blank rel=noopener>Guide to optimizers</a></p><h2 id=26-sentiment-analysis>26 Sentiment analysis</h2><p>Text Classification and sentiment analysis is a very common machine learning problem and is used in a lot of activities like product predictions, movie recommendations, and several others.</p><p>Text classification problems like sentimental analysis can be achieved in a number of ways using a number of algorithms. These are majorly divided into two main categories:</p><p>A bag of Word model: In this case, all the sentences in our dataset are tokenized to form a bag of words that denotes our vocabulary. Now each individual sentence or sample in our dataset is represented by that bag of words vector. This vector is called the feature vector. For example, ‘It is a sunny day’, and ‘The Sun rises in east’ are two sentences. The bag of words would be all the words in both the sentences uniquely.</p><p>The second method is based on a time series approach: Here each word is represented by an Individual vector. So, a sentence is represented as a vector of vectors.</p><p><a href=https://towardsdatascience.com/a-guide-to-text-classification-and-sentiment-analysis-2ab021796317 target=_blank rel=noopener>Guide to sentimental analysis</a></p><h2 id=27-collaborative-filtering>27 Collaborative filtering</h2><p>We all have used services like Netflix, Amazon, and Youtube. These services use very sophisticated systems to recommend the best items to their users to make their experiences great.</p><p>Recommenders mostly have 3 components mainly, out of which, one of the main component is Candidate generation. This method is responsible for generating smaller subsets of candidates to recommend to a user, given a huge pool of thousands of items.</p><p>Types of Candidate Generation Systems:</p><p><strong>Content-based filtering System</strong></p><p><strong>Collaborative filtering System</strong></p><p><strong>Content-based filtering system</strong>: Content-Based recommender system tries to guess the features or behavior of a user given the item’s features, he/she reacts positively to.</p><p><strong>Collaborative filtering System</strong>: Collaborative does not need the features of the items to be given. Every user and item is described by a feature vector or embedding.</p><p>It creates embedding for both users and items on its own. It embeds both users and items in the same embedding space.</p><p>It considers other users’ reactions while recommending a particular user. It notes which items a particular user likes and also the items that the users with behavior and likings like him/her likes, to recommend items to that user.</p><p>It collects user feedbacks on different items and uses them for recommendations.</p><p><a href=https://towardsdatascience.com/introduction-to-recommender-systems-1-971bd274f421 target=_blank rel=noopener>Guide to collaborative filtering</a></p><h2 id=28-tagging>28 Tagging</h2><h2 id=29-support-vector-machine>29 Support Vector Machine</h2><p>Support vector machines are used for both Classification and Regressions.</p><p>SVM uses a margin around its classifier or regressor. The margin provides an extra robustness and accuracy to the model and its performance.</p><p><a href=/images/c617f6ac99c7c557b14984bfc22db1c6.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=/images/c617f6ac99c7c557b14984bfc22db1c6.png alt=SVM loading=lazy></a></p><p>The above image describes a SVM classifier. The Red line is the actual classifier and the dotted lines show the boundary. The points that lie on the boundary actually decide the Margins. They support the classifier margins, so they are called <strong>Support Vectors</strong>.</p><p>The distance between the classifier and the nearest points is called <strong>Marginal Distance</strong>.</p><p>There can be several classifiers possible but we choose the one with the maximum marginal distance. So, the marginal distance and the support vectors help to choose the best classifier.</p><p><a href=https://scikit-learn.org/stable/modules/svm.html target=_blank rel=noopener>Official Documentation from Sklearn</a></p><p><a href=https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47 target=_blank rel=noopener>Guide to SVM</a></p><h2 id=30_reinforcement-learning>30_Reinforcement Learning</h2><p>“Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward.”</p><p>To play a game, we need to make multiple choices and predictions during the course of the game to achieve success, so they can be called a multiple decision processes. This is where we need a type of algorithm called reinforcement learning algorithms. The class of algorithm is based on decision-making chains which let such algorithms to support multiple decision processes.</p><p>The reinforcement algorithm can be used to reach a goal state from a starting state making decisions accordingly.</p><p>The reinforcement learning involves an agent which learns on its own. If it makes a correct or good move that takes it towards the goal, it is positively rewarded, else not. This way the agent learns.</p><p><a href=/images/81c32258aeef3d5b394b534d2e205b8b.png target=view_window><img class="img-fluid mx-auto d-block rounded shadow-sm data-zoomable" src=/images/81c32258aeef3d5b394b534d2e205b8b.png alt=reinforced loading=lazy></a></p><p>The above image shows reinforcement learning setup.</p><p><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#:~:text=Reinforcement%20learning%20%28RL%29%20is%20an,supervised%20learning%20and%20unsupervised%20learning." target=_blank rel=noopener>WIKI</a></p><h1 id=5-text-mining>5 Text Mining</h1><h2 id=1-corpus>1 Corpus</h2><h2 id=2-named-entity-recognition>2 Named Entity Recognition</h2><h2 id=3-text-analysis>3 Text Analysis</h2><h2 id=4-uima>4 UIMA</h2><h2 id=5-term-document-matrix>5 Term Document matrix</h2><h2 id=6-term-frequency-and-weight>6 Term frequency and Weight</h2><h2 id=7-support-vector-machines-svm>7 Support Vector Machines (SVM)</h2><h2 id=8-association-rules>8 Association rules</h2><h2 id=9-market-based-analysis>9 Market based analysis</h2><h2 id=10-feature-extraction>10 Feature extraction</h2><h2 id=11-using-mahout>11 Using mahout</h2><h2 id=12-using-weka>12 Using Weka</h2><h2 id=13-using-nltk>13 Using NLTK</h2><h2 id=14-classify-text>14 Classify text</h2><h2 id=15-vocabulary-mapping>15 Vocabulary mapping</h2></article></div><div class="card card-body mb-3 shadow-sm border-0 markdown"><blockquote class=mb-0>本站内容采用 CC BY-NC-SA 4.0 许可，请注明出处；商业转载请联系作者授权。</blockquote></div><div class="card card-body shadow-sm border-0"><script async crossorigin=anonymous src=https://giscus.app/client.js data-repo=villsi/villsi.github.io data-repo-id=R_kgDOLG5awg data-category=Announcements data-category-id=DIC_kwDOLG5aws4CcvEE data-mapping=title data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy></script></div></div></div><div class="col-xl-2 col-lg-0 col-md-0 px-0 d-none d-xl-block"><aside class="sticky-top fix-sidebar-top"><section class="d-none d-sm-block"><div class="card shadow-sm border-0 mb-3"><div class="card-header bg-white d-flex justify-content-center border-light-subtle"><span class="text-uppercase fw-medium">This Page</span></div><div class="card-body small"><nav id=TableOfContents><ul><li><a href=#1-what-is-ml->1 What is ML ?</a><ul><li><a href=#definition>Definition</a></li><li><a href=#utilisation-examples>Utilisation examples</a></li></ul></li><li><a href=#2-numerical-var>2 Numerical var</a></li><li><a href=#3-categorical-var>3 Categorical var</a></li><li><a href=#4-supervised-learning>4 Supervised learning</a></li><li><a href=#5-unsupervised-learning>5 Unsupervised learning</a></li><li><a href=#6-concepts-inputs-and-attributes>6 Concepts, inputs and attributes</a></li><li><a href=#7-training-and-test-data>7 Training and test data</a><ul><li><a href=#sklearnmodel_selectiontrain_test_split-is-used-for-splitting-the-data>sklearn.model_selection.train_test_split is used for splitting the data.</a></li></ul></li><li><a href=#8-classifiers>8 Classifiers</a></li><li><a href=#9-prediction>9 Prediction</a></li><li><a href=#10-lift>10 Lift</a></li><li><a href=#11-overfitting>11 Overfitting</a></li><li><a href=#12-bias--variance>12 Bias & variance</a></li><li><a href=#13-tree-and-classification>13 Tree and classification</a></li><li><a href=#14-classification-rate>14 Classification rate</a></li><li><a href=#15-decision-tree>15 Decision tree</a></li><li><a href=#16-boosting>16 Boosting</a><ul><li></li></ul></li><li><a href=#17-naïves-bayes-classifiers>17 Naïves Bayes classifiers</a><ul><li></li></ul></li><li><a href=#18-k-nearest-neighbor>18 K-Nearest neighbor</a></li><li><a href=#19-logistic-regression>19 Logistic regression</a></li><li><a href=#20-ranking>20 Ranking</a></li><li><a href=#21-linear-regression>21 Linear regression</a></li><li><a href=#22-perceptron>22 Perceptron</a></li><li><a href=#23-hierarchical-clustering>23 Hierarchical clustering</a></li><li><a href=#24-k-means-clustering>24 K-means clustering</a></li><li><a href=#25-neural-networks>25 Neural networks</a></li><li><a href=#26-sentiment-analysis>26 Sentiment analysis</a></li><li><a href=#27-collaborative-filtering>27 Collaborative filtering</a></li><li><a href=#28-tagging>28 Tagging</a></li><li><a href=#29-support-vector-machine>29 Support Vector Machine</a></li><li><a href=#30_reinforcement-learning>30_Reinforcement Learning</a></li></ul><ul><li><a href=#1-corpus>1 Corpus</a></li><li><a href=#2-named-entity-recognition>2 Named Entity Recognition</a></li><li><a href=#3-text-analysis>3 Text Analysis</a></li><li><a href=#4-uima>4 UIMA</a></li><li><a href=#5-term-document-matrix>5 Term Document matrix</a></li><li><a href=#6-term-frequency-and-weight>6 Term frequency and Weight</a></li><li><a href=#7-support-vector-machines-svm>7 Support Vector Machines (SVM)</a></li><li><a href=#8-association-rules>8 Association rules</a></li><li><a href=#9-market-based-analysis>9 Market based analysis</a></li><li><a href=#10-feature-extraction>10 Feature extraction</a></li><li><a href=#11-using-mahout>11 Using mahout</a></li><li><a href=#12-using-weka>12 Using Weka</a></li><li><a href=#13-using-nltk>13 Using NLTK</a></li><li><a href=#14-classify-text>14 Classify text</a></li><li><a href=#15-vocabulary-mapping>15 Vocabulary mapping</a></li></ul></nav></div></div><script src=https://cdn.jsdelivr.net/npm/gumshoejs@5.1.2/dist/gumshoe.min.js></script><script>var header=document.querySelector("#my-header"),spy=new Gumshoe("#TableOfContents a",{reflow:!0,nested:!0,nestedClass:"active",offset:function(){return header.getBoundingClientRect().height}})</script></section><section class="d-none d-sm-block"></section></aside></div></div></div></main><footer class="footer pt-3"><nav class="navbar navbar-expand-lg navbar-dark bg-dark"><div class="container fw-light small"><div class=navbar-text>Copyright &copy; 2018 - 2024
<a class="text-reset text-decoration-none" href=https://villsi.net target=_blank rel=noopener>上海红茶馆
</a>|
<a class="text-reset text-decoration-none" href=https://creativecommons.org/licenses/by-nc-sa/4.0/ target=_blank rel=noopener>CC BY-NC-SA 4.0</a></div><div class="navbar-text ms-auto"><a class="text-reset text-decoration-none" href=https://github.com/villsi/ target=_blank rel=noopener>Power by Cloudflare Page</a></div></div></nav><button button type=button class="btn btn-dark btn-sm scrollupBtn" title=返回顶部>
<i class="bi bi-chevron-up"></i></button></footer></body></html>