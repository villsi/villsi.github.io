<!doctype html><html lang=zh><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.121.2"><link rel=preconnect href=https://cdn.jsdelivr.net><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css2?display=swap&family=Inter:wght@300;400;500&family=Noto+Sans+SC:wght@300;400;500&family=Noto+Sans+JP:wght@300;400;500&family=Fira+Code:wght@300;400;500"><title>EfficientDNNs | 上海红茶馆</title>
<meta name=author content="villsi"><meta name=description content="A collection of recent methods on DNN compression and acceleration. This repo is more about pruning (with lottery ticket hypothesis or LTH as a sub-topic), KD, and quantization."><meta name=keywords content="Deep Learning,DNN"><link rel=icon href=https://img.villsi.net/2023/12/efabaf2039214b5692c50465a865f8c1.webp sizes=any><link rel=icon sizes=192x192 href=https://img.villsi.net/2023/12/efabaf2039214b5692c50465a865f8c1.webp><link rel=icon sizes=512x512 href=https://img.villsi.net/2023/12/efabaf2039214b5692c50465a865f8c1.webp><link rel=apple-touch-icon sizes=180x180 href=https://img.villsi.net/2023/12/efabaf2039214b5692c50465a865f8c1.webp><meta property="og:title" content="EfficientDNNs | 上海红茶馆"><meta name=twitter:title content="EfficientDNNs | 上海红茶馆"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.villsi.net/post/2023/efficientdnns/"><meta property="og:description" content="A collection of recent methods on DNN compression and acceleration. This repo is more about pruning (with lottery ticket hypothesis or LTH as a sub-topic), KD, and quantization."><meta name=twitter:description content="A collection of recent methods on DNN compression and acceleration. This repo is more about pruning (with lottery ticket hypothesis or LTH as a sub-topic), KD, and quantization."><meta property="og:image" content="https://img.villsi.net/2023/12/fd029546dac00eb0a7db2781444cb6c2.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://img.villsi.net/2023/12/fd029546dac00eb0a7db2781444cb6c2.jpg"><meta property="article:published_time" content="2023-08-03T23:00:00+08:00"><meta property="article:modified_time" content="2023-11-29T11:33:50+08:00"><link rel=alternate type=application/atom+xml href=https://blog.villsi.net/index.xml><link rel=stylesheet href=https://blog.villsi.net/assets/main.min.css><script defer src=https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/twemoji@14.0.2/dist/twemoji.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/quicklink@2.3.0/dist/quicklink.umd.js></script><script defer src=https://blog.villsi.net/assets/main.min.js></script></head><body data-theme=auto data-section=single><header class=header><nav class="navbar container"><div class=navbar__brand><a class=navbar__item href=https://blog.villsi.net><h1>上海红茶馆</h1></a></div><div class=navbar__menu><div class=navbar__start><a class="navbar__item navbar__item--active" href=https://blog.villsi.net/post/>文章</a><a class=navbar__item href=https://blog.villsi.net/code/>笔记</a><a class=navbar__item href=https://blog.villsi.net/friends/>友链</a><a class=navbar__item href=https://blog.villsi.net/playlist/>番剧</a></div><div class=navbar__end><a class=navbar__item href=https://blog.villsi.net/index.xml title=RSS target=_blank rel=noopener><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" width="1.1em" height="1.1em"><path d="M303.74 463.21c-8.35-154.6-132.18-278.59-286.95-286.95A16 16 0 000 192.25v48.07a16 16 0 0014.89 16c111.83 7.28 201.47 96.7 208.77 208.77a16 16 0 0016 14.89h48.07a16 16 0 0016-16.79zM16.5 32A16 16 0 000 48v48.08a16 16 0 0015.45 16c191.18 7.84 344.63 161.32 352.47 352.47a16 16 0 0016 15.45H432a16 16 0 0016-16.5C439.6 229.68 251.46 40.45 16.5 32z" style="opacity:.4"/><path d="M0 416a64 64 0 1164 64A64 64 0 010 416z"/></svg></a><div class="navbar__item navbar__toc" id=toc-btn title=目录><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" width="1.1em" height="1.1em"><path d="M16 288h416a16 16 0 0016-16v-32a16 16 0 00-16-16H16A16 16 0 000 240v32a16 16 0 0016 16z" style="opacity:.4"/><path d="M432 384H16A16 16 0 000 4e2v32a16 16 0 0016 16h416a16 16 0 0016-16v-32a16 16 0 00-16-16zm0-320H16A16 16 0 000 80v32a16 16 0 0016 16h416a16 16 0 0016-16V80a16 16 0 00-16-16z"/></svg></div></div></nav></header><main class=main><div class=container><aside class=sidebar><div class=sidebar__inner><div class=sticky><div class="card info"><div class=info__avatar><div class=fiximg><div class=fiximg__container style=padding-bottom:100%><img loading=lazy src=https://img.villsi.net/2023/12/fd029546dac00eb0a7db2781444cb6c2.jpg alt=头像></div></div></div><div class=info__meta><span class=info__metaName>villsi</span></div><div class=info__counter><div class=info__counterItem><span class=info__counterData>307</span>
<span class=info__counterName>千字</span></div><div class=info__counterItem><span class=info__counterData>35</span>
<span class=info__counterName>文章</span></div></div></div><div class="card links"><a class=links__item href=https://github.com/villsi target=_blank rel=noopener title=GitHub><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" width="1.1em" height="1.1em"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a class=links__item href=https://twitter.com/villsi target=_blank rel=noopener title=Twitter><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" width="1.1em" height="1.1em"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></a><a class=links__item href=https://steamcommunity.com/id/villsi/ target=_blank rel=noopener title=Steam><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" width="1.1em" height="1.1em"><path d="M496 256c0 137-111.2 248-248.4 248-113.8.0-209.6-76.3-239-180.4l95.2 39.3c6.4 32.1 34.9 56.4 68.9 56.4 39.2.0 71.9-32.4 70.2-73.5l84.5-60.2c52.1 1.3 95.8-40.9 95.8-93.5.0-51.6-42-93.5-93.7-93.5s-93.7 42-93.7 93.5v1.2L176.6 279c-15.5-.9-30.7 3.4-43.5 12.1L0 236.1C10.2 108.4 117.1 8 247.6 8 384.8 8 496 119 496 256zM155.7 384.3l-30.5-12.6a52.79 52.79.0 0027.2 25.8c26.9 11.2 57.8-1.6 69-28.4 5.4-13 5.5-27.3.1-40.3S206 305.6 193 300.2c-12.9-5.4-26.7-5.2-38.9-.6l31.5 13c19.8 8.2 29.2 30.9 20.9 50.7-8.3 19.9-31 29.2-50.8 21zm173.8-129.9c-34.4.0-62.4-28-62.4-62.3s28-62.3 62.4-62.3 62.4 28 62.4 62.3-27.9 62.3-62.4 62.3zm.1-15.6c25.9.0 46.9-21 46.9-46.8.0-25.9-21-46.8-46.9-46.8s-46.9 21-46.9 46.8c.1 25.8 21.1 46.8 46.9 46.8z"/></svg></a></div></div></div></aside><div class=content><div class="content__inner post__article"><div class=card><div class=post__title><h2>EfficientDNNs</h2></div><div class=post__meta><span class=post__metaDate><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" width="1.1em" height="1.1em"><path d="M0 192v272a48 48 0 0048 48h352a48 48 0 0048-48V192zm192 176a16 16 0 01-16 16H80a16 16 0 01-16-16v-96a16 16 0 0116-16h96a16 16 0 0116 16zm112-240h32a16 16 0 0016-16V16A16 16 0 00336 0h-32a16 16 0 00-16 16v96a16 16 0 0016 16zm-192 0h32a16 16 0 0016-16V16A16 16 0 00144 0h-32A16 16 0 0096 16v96a16 16 0 0016 16z" style="opacity:.4"/><path d="M448 112v80H0v-80a48 48 0 0148-48h48v48a16 16 0 0016 16h32a16 16 0 0016-16V64h128v48a16 16 0 0016 16h32a16 16 0 0016-16V64h48a48 48 0 0148 48z"/></svg><time>2023-08-03</time></span><span class=post__metaWords>
<svg class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" width="1.1em" height="1.1em"><path d="M384 128H272a16 16 0 01-16-16V0H24A23.94 23.94.0 000 23.88V488a23.94 23.94.0 0023.88 24H360a23.94 23.94.0 0024-23.88V128zm-96 244a12 12 0 01-12 12H108a12 12 0 01-12-12v-8a12 12 0 0112-12h168a12 12 0 0112 12zm0-64a12 12 0 01-12 12H108a12 12 0 01-12-12v-8a12 12 0 0112-12h168a12 12 0 0112 12zm0-64a12 12 0 01-12 12H108a12 12 0 01-12-12v-8a12 12 0 0112-12h168a12 12 0 0112 12z" style="opacity:.4"/><path d="M377 105 279.1 7a24 24 0 00-17-7H256v112a16 16 0 0016 16h112v-6.1a23.9 23.9.0 00-7-16.9zM276 352H108a12 12 0 00-12 12v8a12 12 0 0012 12h168a12 12 0 0012-12v-8a12 12 0 00-12-12zm0-64H108a12 12 0 00-12 12v8a12 12 0 0012 12h168a12 12 0 0012-12v-8a12 12 0 00-12-12zm0-64H108a12 12 0 00-12 12v8a12 12 0 0012 12h168a12 12 0 0012-12v-8a12 12 0 00-12-12z"/></svg>6420 字</span>
<span class=post__metaMinutes><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" width="1.1em" height="1.1em"><path d="M384 128H272a16 16 0 01-16-16V0H24A23.94 23.94.0 000 23.88V488a23.94 23.94.0 0023.88 24H360a23.94 23.94.0 0024-23.88V128zm-96 244a12 12 0 01-12 12H108a12 12 0 01-12-12v-8a12 12 0 0112-12h168a12 12 0 0112 12zm0-64a12 12 0 01-12 12H108a12 12 0 01-12-12v-8a12 12 0 0112-12h168a12 12 0 0112 12zm0-64a12 12 0 01-12 12H108a12 12 0 01-12-12v-8a12 12 0 0112-12h168a12 12 0 0112 12z" style="opacity:.4"/><path d="M377 105 279.1 7a24 24 0 00-17-7H256v112a16 16 0 0016 16h112v-6.1a23.9 23.9.0 00-7-16.9zM276 352H108a12 12 0 00-12 12v8a12 12 0 0012 12h168a12 12 0 0012-12v-8a12 12 0 00-12-12zm0-64H108a12 12 0 00-12 12v8a12 12 0 0012 12h168a12 12 0 0012-12v-8a12 12 0 00-12-12zm0-64H108a12 12 0 00-12 12v8a12 12 0 0012 12h168a12 12 0 0012-12v-8a12 12 0 00-12-12z"/></svg>31 分钟</span></div><div class=post__content><article class=markdown><p>A collection of recent methods on DNN compression and acceleration. There are mainly 5 kinds of methods for efficient DNNs:</p><ul><li>neural architecture re-design or search (NAS)<ul><li>maintain accuracy, less cost (e.g., #Params, #FLOPs, etc.): MobileNet, ShuffleNet etc.</li><li>maintain cost, more accuracy: Inception, ResNeXt, Xception etc.</li></ul></li><li>pruning (including structured and unstructured)</li><li>quantization</li><li>matrix/low-rank decomposition</li><li>knowledge distillation (KD)</li></ul><p>Note, this repo is more about pruning (with lottery ticket hypothesis or LTH as a sub-topic), KD, and quantization. For other topics like NAS, see more comprehensive collections (## Related Repos and Websites) at the end of this file. Welcome to send a pull request if you&rsquo;d like to add any pertinent papers.</p><p>Other repos:</p><ul><li>LTH (lottery ticket hypothesis) and its broader version, <em>pruning at initialization (PaI)</em>, now is at the frontier of network pruning. We single out the PaI papers to <a class=link href=https://github.com/MingSun-Tse/Awesome-Pruning-at-Initialization target=_blank rel=noopener>this repo</a>. Welcome to check it out!</li><li><a class=link href=https://github.com/MingSun-Tse/Awesome-Efficient-ViT target=_blank rel=noopener>Awesome-Efficient-ViT</a> for a curated list of efficient vision transformers.</li></ul><blockquote><p>About abbreviation: In the list below, <code>o</code> for oral, <code>s</code> for spotlight, <code>b</code> for best paper, <code>w</code> for workshop.</p></blockquote><h2 id=surveys>Surveys</h2><ul><li>1993-TNN-<a class=link href="https://ieeexplore.ieee.org/abstract/document/248452?casa_token=eJan5NO1DxwAAAAA:chz9BYf22tIO4RHt6nC_x4nbTeTslXiLMrvQElnrXZGbg9fn4c-Alonhq8UYWhT86gXFGO2_" target=_blank rel=noopener>Pruning Algorithms &ndash; A survey</a></li><li>2017-Proceedings of the IEEE-<a class=link href=https://ieeexplore.ieee.org/document/8114708 target=_blank rel=noopener>Efficient Processing of Deep Neural Networks: A Tutorial and Survey</a> [<a class=link href="https://www.morganclaypool.com/doi/pdfplus/10.2200/S01004ED1V01Y202004CAC050?casa_token=rnnqUJmipDEAAAAA:fOs90gKOCbMDqjZlGdc25dCi3H4L0gT1tkEqhNL1eNBpV8h36cvQet9WK0qVRqs9M6irYxbH" target=_blank rel=noopener>2020 Book: Efficient Processing of Deep Neural Networks</a>]</li><li>2017.12-<a class=link href=https://arxiv.org/abs/1712.08934 target=_blank rel=noopener>A survey of FPGA-based neural network accelerator</a></li><li>2018-FITEE-<a class=link href=https://link.springer.com/article/10.1631/FITEE.1700789 target=_blank rel=noopener>Recent Advances in Efficient Computation of Deep Convolutional Neural Networks</a></li><li>2018-IEEE Signal Processing Magazine-<a class=link href=https://ieeexplore.ieee.org/abstract/document/8253600 target=_blank rel=noopener>Model compression and acceleration for deep neural networks: The principles, progress, and challenges</a>. <a class=link href=https://arxiv.org/abs/1710.09282 target=_blank rel=noopener>Arxiv extension</a></li><li>2018.8-<a class=link href=https://arxiv.org/abs/1808.04752 target=_blank rel=noopener>A Survey on Methods and Theories of Quantized Neural Networks</a></li><li>2019-JMLR-<a class=link href=http://www.jmlr.org/papers/volume20/18-598/18-598.pdf target=_blank rel=noopener>Neural Architecture Search: A Survey</a></li><li>2020-MLSys-<a class=link href=https://arxiv.org/abs/2003.03033 target=_blank rel=noopener>What is the state of neural network pruning</a></li><li>2019.02-<a class=link href=https://arxiv.org/pdf/1902.09574.pdf target=_blank rel=noopener>The State of Sparsity in Deep Neural Networks</a></li><li>2021-TPAMI-<a class=link href=https://arxiv.org/abs/2004.05937 target=_blank rel=noopener>Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks</a></li><li>2021-IJCV-<a class=link href=https://arxiv.org/abs/2006.05525 target=_blank rel=noopener>Knowledge Distillation: A Survey</a></li><li>2020-Proceedings of the IEEE-<a class=link href=https://ieeexplore.ieee.org/abstract/document/9043731 target=_blank rel=noopener>Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey</a></li><li>2020-Pattern Recognition-<a class=link href="https://www.sciencedirect.com/science/article/pii/S0031320320300856?casa_token=Foe2l0h1AXUAAAAA:z7DaP-QSVCNApUpTsrftp3f2SBfcNj2AH_B0cbzPH4r8BR-cGSns16p1-CQtY7vXuexlPd_Y" target=_blank rel=noopener>Binary neural networks: A survey</a></li><li>2021-TPDS-<a class=link href=https://arxiv.org/abs/2002.03794 target=_blank rel=noopener>The Deep Learning Compiler: A Comprehensive Survey</a></li><li>2021-JMLR-<a class=link href=https://arxiv.org/abs/2102.00554 target=_blank rel=noopener>Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks</a></li><li>2022-IJCAI-<a class=link href=https://arxiv.org/abs/2103.06460 target=_blank rel=noopener>Recent Advances on Neural Network Pruning at Initialization</a></li><li>2021.6-<a class=link href=https://arxiv.org/abs/2106.08962 target=_blank rel=noopener>Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better</a></li></ul><h2 id=papers-pruning-and-quantization>Papers [Pruning and Quantization]</h2><p><strong>1980s,1990s</strong></p><ul><li>1988-NIPS-<a class=link href=https://proceedings.neurips.cc/paper/1988/file/9fc3d7152ba9336a670e36d0ed79bc43-Paper.pdf target=_blank rel=noopener>A back-propagation algorithm with optimal use of hidden units</a></li><li>1988-NIPS-<a class=link href=https://papers.nips.cc/paper/1988/file/07e1cd7dca89a1678042477183b7ac3f-Paper.pdf target=_blank rel=noopener>Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment</a></li><li>1988-NIPS-<a class=link href=https://papers.nips.cc/paper/1988/file/1d7f7abc18fcb43975065399b0d1e48e-Paper.pdf target=_blank rel=noopener>What Size Net Gives Valid Generalization?</a></li><li>1989-NIPS-<a class=link href=https://proceedings.neurips.cc/paper/1989/hash/85d8ce590ad8981ca2c8286f79f59954-Abstract.html target=_blank rel=noopener>Dynamic Behavior of Constained Back-Propagation Networks</a></li><li>1988-NIPS-<a class=link href=https://papers.nips.cc/paper/1988/file/1c9ac0159c94d8d0cbedc973445af2da-Paper.pdf target=_blank rel=noopener>Comparing Biases for Minimal Network Construction with Back-Propagation</a></li><li>1989-NIPS-<a class=link href=https://papers.nips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf target=_blank rel=noopener>Optimal Brain Damage</a></li><li>1990-NN-<a class=link href=https://ieeexplore.ieee.org/abstract/document/80236 target=_blank rel=noopener>A simple procedure for pruning back-propagation trained neural networks</a></li><li>1993-ICNN-<a class=link href="https://ieeexplore.ieee.org/abstract/document/298572?casa_token=8a8fUVuadHEAAAAA:tgRbetEERx1Bdh6RCa27mok9SAPNc8Y33qy2ScdTNOCs_ajHlaUv4_nnvDNJp3jZbb13uouD" target=_blank rel=noopener>Optimal Brain Surgeon and general network pruning</a></li></ul><p><strong>2000s</strong></p><ul><li>2001-JMLR-<a class=link href=https://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf target=_blank rel=noopener>Sparse Bayesian learning and the relevance vector machine</a></li><li>2007-Book-<a class=link href target=_blank rel=noopener>The minimum description length principle</a></li></ul><p><strong>2011</strong></p><ul><li>2011-JMLR-<a class=link href=http://www.jmlr.org/papers/v12/huang11b.html target=_blank rel=noopener>Learning with Structured Sparsity</a></li><li>2011-NIPSw-<a class=link href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.308.2766" target=_blank rel=noopener>Improving the speed of neural networks on CPUs</a></li></ul><p><strong>2013</strong></p><ul><li>2013-NIPS-<a class=link href=http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning target=_blank rel=noopener>Predicting Parameters in Deep Learning</a></li><li>2013.08-<a class=link href=https://arxiv.org/abs/1308.3432 target=_blank rel=noopener>Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation</a></li></ul><p><strong>2014</strong></p><ul><li>2014-BMVC-<a class=link href=https://arxiv.org/abs/1405.3866 target=_blank rel=noopener>Speeding up convolutional neural networks with low rank expansions</a></li><li>2014-INTERSPEECH-<a class=link href=https://www.isca-speech.org/archive/interspeech_2014/i14_1058.html target=_blank rel=noopener>1-Bit Stochastic Gradient Descent and its Application to Data-Parallel Distributed Training of Speech DNNs</a></li><li>2014-NIPS-<a class=link href=http://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation target=_blank rel=noopener>Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation</a></li><li>2014-NIPS-<a class=link href=http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep target=_blank rel=noopener>Do deep neural nets really need to be deep</a></li><li>2014.12-<a class=link href=https://arxiv.org/abs/1412.1442 target=_blank rel=noopener>Memory bounded deep convolutional networks</a></li></ul><p><strong>2015</strong></p><ul><li>2015-ICLR-<a class=link href=https://arxiv.org/abs/1412.6553 target=_blank rel=noopener>Speeding-up convolutional neural networks using fine-tuned cp-decomposition</a></li><li>2015-ICML-<a class=link href=http://proceedings.mlr.press/v37/chenc15.pdf target=_blank rel=noopener>Compressing neural networks with the hashing trick</a></li><li>2015-INTERSPEECH-<a class=link href=https://www.isca-speech.org/archive/interspeech_2015/papers/i15_3590.pdf target=_blank rel=noopener>A Diversity-Penalizing Ensemble Training Method for Deep Learning</a></li><li>2015-BMVC-<a class=link href=https://arxiv.org/abs/1507.06149 target=_blank rel=noopener>Data-free parameter pruning for deep neural networks</a></li><li>2015-BMVC-<a class=link href=http://www.bmva.org/bmvc/2015/papers/paper023/paper023.pdf target=_blank rel=noopener>Learning the structure of deep architectures using l1 regularization</a></li><li>2015-NIPS-<a class=link href=http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network target=_blank rel=noopener>Learning both Weights and Connections for Efficient Neural Network</a></li><li>2015-NIPS-<a class=link href=http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-b target=_blank rel=noopener>Binaryconnect: Training deep neural networks with binary weights during propagations</a></li><li>2015-NIPS-<a class=link href=http://papers.nips.cc/paper/5869-structured-transforms-for-small-footprint-deep-learning target=_blank rel=noopener>Structured Transforms for Small-Footprint Deep Learning</a></li><li>2015-NIPS-<a class=link href=http://papers.nips.cc/paper/5787-tensorizing-neural-networks target=_blank rel=noopener>Tensorizing Neural Networks</a></li><li>2015-NIPSw-<a class=link href=http://homepages.inf.ed.ac.uk/s1459647/papers/distilling_generative_models.pdf target=_blank rel=noopener>Distilling Intractable Generative Models</a></li><li>2015-NIPSw-<a class=link href=https://arxiv.org/abs/1511.03575 target=_blank rel=noopener>Federated Optimization:Distributed Optimization Beyond the Datacenter</a></li><li>2015-CVPR-<a class=link href=https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Zhang_Efficient_and_Accurate_2015_CVPR_paper.html target=_blank rel=noopener>Efficient and Accurate Approximations of Nonlinear Convolutional Networks</a> [2016 TPAMI version: <a class=link href=https://ieeexplore.ieee.org/abstract/document/7332968 target=_blank rel=noopener>Accelerating Very Deep Convolutional Networks for Classification and Detection</a>]</li><li>2015-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2015/html/Liu_Sparse_Convolutional_Neural_2015_CVPR_paper.html target=_blank rel=noopener>Sparse Convolutional Neural Networks</a></li><li>2015-ICCV-<a class=link href=https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Cheng_An_Exploration_of_ICCV_2015_paper.html target=_blank rel=noopener>An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections</a></li><li>2015.12-<a class=link href=https://arxiv.org/abs/1512.09194 target=_blank rel=noopener>Exploiting Local Structures with the Kronecker Layer in Convolutional Networks</a></li></ul><p><strong>2016</strong></p><ul><li>2016-ICLR-<a class=link href=https://arxiv.org/abs/1510.00149 target=_blank rel=noopener>Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a> [Best paper!]</li><li>2016-ICLR-<a class=link href=https://arxiv.org/abs/1511.06422 target=_blank rel=noopener>All you need is a good init</a> [<a class=link href=https://github.com/ducha-aiki/LSUVinit target=_blank rel=noopener>Code</a>]</li><li>2016-ICLR-<a class=link href=https://arxiv.org/abs/1511.06856 target=_blank rel=noopener>Data-dependent Initializations of Convolutional Neural Networks</a> [<a class=link href=https://github.com/philkr/magic_init target=_blank rel=noopener>Code</a>]</li><li>2016-ICLR-<a class=link href=https://arxiv.org/abs/1511.06067 target=_blank rel=noopener>Convolutional neural networks with low-rank regularization</a> [<a class=link href=https://github.com/chengtaipu/lowrankcnn target=_blank rel=noopener>Code</a>]</li><li>2016-ICLR-<a class=link href=https://pdfs.semanticscholar.org/3f08/1a7d2dbdcd10d71d0340721e4857a73ed7ee.pdf target=_blank rel=noopener>Diversity networks</a></li><li>2016-ICLR-<a class=link href=https://arxiv.org/abs/1510.03009 target=_blank rel=noopener>Neural networks with few multiplications</a></li><li>2016-ICLR-<a class=link href=https://arxiv.org/abs/1511.06530 target=_blank rel=noopener>Compression of deep convolutional neural networks for fast and low power mobile applications</a></li><li>2016-ICLRw-<a class=link href=https://arxiv.org/abs/1602.05931 target=_blank rel=noopener>Randomout: Using a convolutional gradient norm to win the filter lottery</a></li><li>2016-CVPR-<a class=link href=https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Lavin_Fast_Algorithms_for_CVPR_2016_paper.html target=_blank rel=noopener>Fast algorithms for convolutional neural networks</a></li><li>2016-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2016/html/Lebedev_Fast_ConvNets_Using_CVPR_2016_paper.html target=_blank rel=noopener>Fast ConvNets Using Group-wise Brain Damage</a></li><li>2016-BMVC-<a class=link href=https://arxiv.org/abs/1511.05497 target=_blank rel=noopener>Learning neural network architectures using backpropagation</a></li><li>2016-ECCV-<a class=link href=http://users.umiacs.umd.edu/~hzhou/paper/zhou_ECCV2016.pdf target=_blank rel=noopener>Less is more: Towards compact cnns</a></li><li>2016-EMNLP-<a class=link href=https://arxiv.org/abs/1606.07947 target=_blank rel=noopener>Sequence-Level Knowledge Distillation</a></li><li>2016-NIPS-<a class=link href=https://proceedings.neurips.cc/paper/2016/hash/41bfd20a38bb1b0bec75acf0845530a7-Abstract.html target=_blank rel=noopener>Learning Structured Sparsity in Deep Neural Networks</a> [<a class=link href=https://github.com/wenwei202/caffe target=_blank rel=noopener>Caffe Code</a>]</li><li>2016-NIPS-<a class=link href=http://papers.nips.cc/paper/6165-dynamic-network-surgery-for-efficient-dnns target=_blank rel=noopener>Dynamic Network Surgery for Efficient DNNs</a> [<a class=link href=https://github.com/yiwenguo/Dynamic-Network-Surgery target=_blank rel=noopener>Caffe Code</a>]</li><li>2016-NIPS-<a class=link href=http://papers.nips.cc/paper/6372-learning-the-number-of-neurons-in-deep-networks target=_blank rel=noopener>Learning the Number of Neurons in Deep Neural Networks</a></li><li>2016-NIPS-<a class=link href=http://papers.nips.cc/paper/6220-memory-efficient-backpropagation-through-time target=_blank rel=noopener>Memory-Efficient Backpropagation Through Time</a></li><li>2016-NIPS-<a class=link href=http://papers.nips.cc/paper/6463-perforatedcnns-acceleration-through-elimination-of-redundant-convolutions target=_blank rel=noopener>PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions</a></li><li>2016-NIPS-<a class=link href=http://papers.nips.cc/paper/6512-lightrnn-memory-and-computation-efficient-recurrent-neural-networks target=_blank rel=noopener>LightRNN: Memory and Computation-Efficient Recurrent Neural Networks</a></li><li>2016-NIPS-<a class=link href=https://papers.nips.cc/paper/6390-cnnpack-packing-convolutional-neural-networks-in-the-frequency-domain.pdf target=_blank rel=noopener>CNNpack: packing convolutional neural networks in the frequency domain</a></li><li>2016-ISCA-<a class=link href=https://people.csail.mit.edu/emer/papers/2016.06.isca.eyeriss_architecture.pdf target=_blank rel=noopener>Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks</a></li><li>2016-ICASSP-<a class=link href=https://arxiv.org/abs/1604.02594 target=_blank rel=noopener>Learning compact recurrent neural networks</a></li><li>2016-CoNLL-<a class=link href=https://arxiv.org/abs/1606.09274 target=_blank rel=noopener>Compression of Neural Machine Translation Models via Pruning</a></li><li>2016.03-<a class=link href=https://arxiv.org/abs/1603.08983 target=_blank rel=noopener>Adaptive Computation Time for Recurrent Neural Networks</a></li><li>2016.06-<a class=link href=https://arxiv.org/abs/1606.02407 target=_blank rel=noopener>Structured Convolution Matrices for Energy-efficient Deep learning</a></li><li>2016.06-<a class=link href=https://arxiv.org/abs/1606.01981 target=_blank rel=noopener>Deep neural networks are robust to weight binarization and other non-linear distortions</a></li><li>2016.06-<a class=link href=https://arxiv.org/abs/1609.09106 target=_blank rel=noopener>Hypernetworks</a></li><li>2016.07-IHT-<a class=link href=https://arxiv.org/abs/1607.05423 target=_blank rel=noopener>Training skinny deep neural networks with iterative hard thresholding methods</a></li><li>2016.08-<a class=link href=https://arxiv.org/abs/1608.06902 target=_blank rel=noopener>Recurrent Neural Networks With Limited Numerical Precision</a></li><li>2016.10-<a class=link href=https://arxiv.org/abs/1610.09650 target=_blank rel=noopener>Deep model compression: Distilling knowledge from noisy teachers</a></li><li>2016.10-<a class=link href=https://arxiv.org/abs/1610.02527 target=_blank rel=noopener>Federated Optimization: Distributed Machine Learning for On-Device Intelligence</a></li><li>2016.11-<a class=link href=https://arxiv.org/abs/1611.01590 target=_blank rel=noopener>Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks</a></li></ul><p><strong>2017</strong></p><ul><li>2017-ICLR-<a class=link href="https://openreview.net/forum?id=rJqFGTslg" target=_blank rel=noopener>Pruning Filters for Efficient ConvNets</a> [<a class=link href=https://github.com/Eric-mingjie/rethinking-network-pruning/tree/master/imagenet/l1-norm-pruning target=_blank rel=noopener>PyTorch Reimpl. #1</a>] [<a class=link href=https://github.com/MingSun-Tse/Regularization-Pruning target=_blank rel=noopener>PyTorch Reimpl. #2</a>]</li><li>2017-ICLR-<a class=link href="https://openreview.net/forum?id=SJGCiw5gl&amp;noteId=SJGCiw5gl" target=_blank rel=noopener>Pruning Convolutional Neural Networks for Resource Efficient Inference</a></li><li>2017-ICLR-<a class=link href=https://arxiv.org/abs/1702.03044 target=_blank rel=noopener>Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights</a> [<a class=link href=https://github.com/Mxbonn/INQ-pytorch target=_blank rel=noopener>Code</a>]</li><li>2017-ICLR-<a class=link href=https://arxiv.org/abs/1603.05691 target=_blank rel=noopener>Do Deep Convolutional Nets Really Need to be Deep and Convolutional?</a></li><li>2017-ICLR-<a class=link href=https://arxiv.org/abs/1607.04381 target=_blank rel=noopener>DSD: Dense-Sparse-Dense Training for Deep Neural Networks</a></li><li>2017-ICLR-<a class=link href=https://arxiv.org/abs/1608.01409 target=_blank rel=noopener>Faster CNNs with Direct Sparse Convolutions and Guided Pruning</a></li><li>2017-ICLR-<a class=link href="https://openreview.net/forum?id=rJ8uNptgl" target=_blank rel=noopener>Towards the Limit of Network Quantization</a></li><li>2017-ICLR-<a class=link href="https://openreview.net/forum?id=S1oWlN9ll&amp;noteId=S1oWlN9ll" target=_blank rel=noopener>Loss-aware Binarization of Deep Networks</a></li><li>2017-ICLR-<a class=link href="https://openreview.net/forum?id=S1_pAu9xl&amp;noteId=S1_pAu9xl" target=_blank rel=noopener>Trained Ternary Quantization</a> [<a class=link href=https://github.com/czhu95/ternarynet target=_blank rel=noopener>Code</a>]</li><li>2017-ICLR-<a class=link href="https://openreview.net/forum?id=BylSPv9gx&amp;noteId=BylSPv9gx" target=_blank rel=noopener>Exploring Sparsity in Recurrent Neural Networks</a></li><li>2017-ICLR-<a class=link href="https://openreview.net/forum?id=HJGwcKclx" target=_blank rel=noopener>Soft Weight-Sharing for Neural Network Compression</a> [<a class=link href=https://www.reddit.com/r/MachineLearning/comments/5u7h3l/r_compressing_nn_with_shannons_blessing/ target=_blank rel=noopener>Reddit discussion</a>] [<a class=link href=https://github.com/KarenUllrich/Tutorial-SoftWeightSharingForNNCompression target=_blank rel=noopener>Code</a>]</li><li>2017-ICLR-<a class=link href="https://openreview.net/forum?id=S1LVSrcge&amp;noteId=S1LVSrcge" target=_blank rel=noopener>Variable Computation in Recurrent Neural Networks</a></li><li>2017-ICLR-<a class=link href="https://openreview.net/forum?id=Hku9NK5lx" target=_blank rel=noopener>Training Compressed Fully-Connected Networks with a Density-Diversity Penalty</a></li><li>2017-ICML-<a class=link href=https://arxiv.org/abs/1703.00144 target=_blank rel=noopener>Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank</a></li><li>2017-ICML-<a class=link href=http://proceedings.mlr.press/v70/budden17a.html target=_blank rel=noopener>Deep Tensor Convolution on Multicores</a></li><li>2017-ICML-<a class=link href=http://proceedings.mlr.press/v70/neil17a.html target=_blank rel=noopener>Delta Networks for Optimized Recurrent Network Computation</a></li><li>2017-ICML-<a class=link href=http://proceedings.mlr.press/v70/wang17m.html target=_blank rel=noopener>Beyond Filters: Compact Feature Map for Portable Deep Model</a></li><li>2017-ICML-<a class=link href=http://proceedings.mlr.press/v70/yoon17a.html target=_blank rel=noopener>Combined Group and Exclusive Sparsity for Deep Neural Networks</a></li><li>2017-ICML-<a class=link href=http://proceedings.mlr.press/v70/cho17a.html target=_blank rel=noopener>MEC: Memory-efficient Convolution for Deep Neural Network</a></li><li>2017-ICML-<a class=link href=http://proceedings.mlr.press/v70/mcgill17a.html target=_blank rel=noopener>Deciding How to Decide: Dynamic Routing in Artificial Neural Networks</a></li><li>2017-ICML-<a class=link href=http://proceedings.mlr.press/v70/zhang17e.html target=_blank rel=noopener>ZipML: Training Models with End-to-End Low Precision: The Cans, the Cannots, and a Little Bit of Deep Learning</a></li><li>2017-ICML-<a class=link href=http://proceedings.mlr.press/v70/sakr17a.html target=_blank rel=noopener>Analytical Guarantees on Numerical Precision of Deep Neural Networks</a></li><li>2017-ICML-<a class=link href=http://proceedings.mlr.press/v70/bolukbasi17a.html target=_blank rel=noopener>Adaptive Neural Networks for Efficient Inference</a></li><li>2017-ICML-<a class=link href=http://proceedings.mlr.press/v70/kim17b.html target=_blank rel=noopener>SplitNet: Learning to Semantically Split Deep Networks for Parameter Reduction and Model Parallelization</a></li><li>2017-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Learning_Deep_CNN_CVPR_2017_paper.html target=_blank rel=noopener>Learning deep CNN denoiser prior for image restoration</a></li><li>2017-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2017/html/Ioannou_Deep_Roots_Improving_CVPR_2017_paper.html target=_blank rel=noopener>Deep roots: Improving cnn efficiency with hierarchical filter groups</a></li><li>2017-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2017/html/Dong_More_Is_Less_CVPR_2017_paper.html target=_blank rel=noopener>More is less: A more complicated network with less inference complexity</a> [<a class=link href=https://github.com/D-X-Y/DXY-Projects/tree/master/LCCL target=_blank rel=noopener>PyTorch Code</a>]</li><li>2017-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_All_You_Need_CVPR_2017_paper.html target=_blank rel=noopener>All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation</a></li><li>2017-CVPR-ResNeXt-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html target=_blank rel=noopener>Aggregated Residual Transformations for Deep Neural Networks</a></li><li>2017-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2017/html/Chollet_Xception_Deep_Learning_CVPR_2017_paper.html target=_blank rel=noopener>Xception: Deep learning with depthwise separable convolutions</a></li><li>2017-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2017/html/Yang_Designing_Energy-Efficient_Convolutional_CVPR_2017_paper.html target=_blank rel=noopener>Designing Energy-Efficient CNN using Energy-aware Pruning</a></li><li>2017-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2017/html/Figurnov_Spatially_Adaptive_Computation_CVPR_2017_paper.html target=_blank rel=noopener>Spatially Adaptive Computation Time for Residual Networks</a></li><li>2017-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2017/html/Guo_Network_Sketching_Exploiting_CVPR_2017_paper.html target=_blank rel=noopener>Network Sketching: Exploiting Binary Structure in Deep CNNs</a></li><li>2017-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2017/html/Wu_A_Compact_DNN_CVPR_2017_paper.html target=_blank rel=noopener>A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification and Domain Adaptation</a></li><li>2017-ICCV-<a class=link href=http://openaccess.thecvf.com/content_iccv_2017/html/He_Channel_Pruning_for_ICCV_2017_paper.html target=_blank rel=noopener>Channel pruning for accelerating very deep neural networks</a> [<a class=link href=https://github.com/yihui-he/channel-pruning target=_blank rel=noopener>Caffe Code</a>]</li><li>2017-ICCV-<a class=link href=http://openaccess.thecvf.com/content_iccv_2017/html/Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.html target=_blank rel=noopener>Learning efficient convolutional networks through network slimming</a> [<a class=link href=https://github.com/liuzhuang13/slimming/ target=_blank rel=noopener>PyTorch Code</a>]</li><li>2017-ICCV-<a class=link href=http://openaccess.thecvf.com/content_iccv_2017/html/Luo_ThiNet_A_Filter_ICCV_2017_paper.html target=_blank rel=noopener>ThiNet: A filter level pruning method for deep neural network compression</a> [<a class=link href=http://lamda.nju.edu.cn/luojh/project/ThiNet_ICCV17/ThiNet_ICCV17.html target=_blank rel=noopener>Project</a>] [<a class=link href=https://github.com/Roll920/ThiNet_Code target=_blank rel=noopener>Caffe Code</a>] [<a class=link href=https://ieeexplore.ieee.org/document/8416559 target=_blank rel=noopener>2018 TPAMI version</a>]</li><li>2017-ICCV-<a class=link href=http://openaccess.thecvf.com/content_iccv_2017/html/Zhang_Interleaved_Group_Convolutions_ICCV_2017_paper.html target=_blank rel=noopener>Interleaved group convolutions</a></li><li>2017-ICCV-<a class=link href=http://openaccess.thecvf.com/content_iccv_2017/html/Wen_Coordinating_Filters_for_ICCV_2017_paper.html target=_blank rel=noopener>Coordinating Filters for Faster Deep Neural Networks</a> [<a class=link href=https://github.com/wenwei202/caffe target=_blank rel=noopener>Caffe Code</a>]</li><li>2017-ICCV-<a class=link href=http://openaccess.thecvf.com/content_iccv_2017/html/Li_Performance_Guaranteed_Network_ICCV_2017_paper.html target=_blank rel=noopener>Performance Guaranteed Network Acceleration via High-Order Residual Quantization</a></li><li>2017-NIPS-<a class=link href=http://papers.nips.cc/paper/6910-net-trim-convex-pruning-of-deep-neural-networks-with-performance-guarantee target=_blank rel=noopener>Net-trim: Convex pruning of deep neural networks with performance guarantee</a> [<a class=link href=https://github.com/DNNToolBox/Net-Trim target=_blank rel=noopener>Code</a>] (Journal version: <a class=link href=https://epubs.siam.org/doi/abs/10.1137/19M1246468 target=_blank rel=noopener>2020-SIAM-Fast Convex Pruning of Deep Neural Networks</a>)</li><li>2017-NIPS-<a class=link href=http://papers.nips.cc/paper/6813-runtime-neural-pruning target=_blank rel=noopener>Runtime neural pruning</a></li><li>2017-NIPS-<a class=link href=http://papers.nips.cc/paper/7071-learning-to-prune-deep-neural-networks-via-layer-wise-optimal-brain-surgeon target=_blank rel=noopener>Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon</a> [<a class=link href=https://github.com/csyhhu/L-OBS target=_blank rel=noopener>Code</a>]</li><li>2017-NIPS-<a class=link href=http://papers.nips.cc/paper/7029-federated-multi-task-learning target=_blank rel=noopener>Federated Multi-Task Learning</a></li><li>2017-NIPS-<a class=link href=http://papers.nips.cc/paper/6638-towards-accurate-binary-convolutional-neural-network target=_blank rel=noopener>Towards Accurate Binary Convolutional Neural Network</a></li><li>2017-NIPS-<a class=link href=http://papers.nips.cc/paper/6714-soft-to-hard-vector-quantization-for-end-to-end-learning-compressible-representations target=_blank rel=noopener>Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations</a></li><li>2017-NIPS-<a class=link href=http://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning target=_blank rel=noopener>TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning</a></li><li>2017-NIPS-<a class=link href=http://papers.nips.cc/paper/6771-flexpoint-an-adaptive-numerical-format-for-efficient-training-of-deep-neural-networks target=_blank rel=noopener>Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks</a></li><li>2017-NIPS-<a class=link href=http://papers.nips.cc/paper/7163-training-quantized-nets-a-deeper-understanding target=_blank rel=noopener>Training Quantized Nets: A Deeper Understanding</a></li><li>2017-NIPS-<a class=link href=http://papers.nips.cc/paper/6816-the-reversible-residual-network-backpropagation-without-storing-activations target=_blank rel=noopener>The Reversible Residual Network: Backpropagation Without Storing Activations</a> [<a class=link href=https://github.com/renmengye/revnet-public target=_blank rel=noopener>Code</a>]</li><li>2017-NIPS-<a class=link href=http://papers.nips.cc/paper/6687-compression-aware-training-of-deep-networks target=_blank rel=noopener>Compression-aware Training of Deep Networks</a></li><li>2017-FPGA-<a class=link href=https://pdfs.semanticscholar.org/99d2/07c18ba48e41560f3081ea1b7c6fde98c1ce.pdf target=_blank rel=noopener>ESE: efficient speech recognition engine with compressed LSTM on FPGA</a> [Best paper!]</li><li>2017-AISTATS-<a class=link href=https://arxiv.org/abs/1602.05629 target=_blank rel=noopener>Communication-Efficient Learning of Deep Networks from Decentralized Data</a></li><li>2017-ICASSP-<a class=link href=https://arxiv.org/abs/1610.00324 target=_blank rel=noopener>Accelerating Deep Convolutional Networks using low-precision and sparsity</a></li><li>2017-NNs-<a class=link href=https://www.sciencedirect.com/science/article/pii/S0893608017300928 target=_blank rel=noopener>Nonredundant sparse feature extraction using autoencoders with receptive fields clustering</a></li><li>2017.02-<a class=link href=https://arxiv.org/abs/1702.06257 target=_blank rel=noopener>The Power of Sparsity in Convolutional Neural Networks</a></li><li>2017.07-<a class=link href=https://arxiv.org/abs/1707.01155 target=_blank rel=noopener>Stochastic, Distributed and Federated Optimization for Machine Learning</a></li><li>2017.05-<a class=link href=https://arxiv.org/abs/1705.07356 target=_blank rel=noopener>Structural Compression of Convolutional Neural Networks Based on Greedy Filter Pruning</a></li><li>2017.07-<a class=link href=https://arxiv.org/abs/1707.09870 target=_blank rel=noopener>Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM</a></li><li>2017.11-<a class=link href=https://openai.com/blog/block-sparse-gpu-kernels/ target=_blank rel=noopener>GPU Kernels for Block-Sparse Weights</a> [<a class=link href=https://github.com/openai/blocksparse target=_blank rel=noopener>Code</a>] (OpenAI)</li><li>2017.11-<a class=link href=https://arxiv.org/abs/1711.02782 target=_blank rel=noopener>Block-sparse recurrent neural networks</a></li></ul><p><strong>2018</strong></p><ul><li>2018-AAAI-<a class=link href=https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16450/16263 target=_blank rel=noopener>Auto-balanced Filter Pruning for Efficient Convolutional Neural Networks</a></li><li>2018-AAAI-<a class=link href=https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16479/16742 target=_blank rel=noopener>Deep Neural Network Compression with Single and Multiple Level Quantization</a></li><li>2018-AAAI-<a class=link href=https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16291 target=_blank rel=noopener>Dynamic Deep Neural Networks_Optimizing Accuracy-Efficiency Trade-offs by Selective Execution</a></li><li>2018-ICLRo-<a class=link href="https://openreview.net/forum?id=HJGXzmspb" target=_blank rel=noopener>Training and Inference with Integers in Deep Neural Networks</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=HJ94fqApW" target=_blank rel=noopener>Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=B1hcZZ-AW" target=_blank rel=noopener>N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=S1XolQbRW" target=_blank rel=noopener>Model compression via distillation and quantization</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=HkXWCMbRW" target=_blank rel=noopener>Towards Image Understanding from Deep Compression Without Decoding</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=SkhQHMW0W" target=_blank rel=noopener>Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=H135uzZ0-" target=_blank rel=noopener>Mixed Precision Training of Convolutional Neural Networks using Integer Operations</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=r1gs9JgRZ" target=_blank rel=noopener>Mixed Precision Training</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=B1ae1lZRb" target=_blank rel=noopener>Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=BkrSv0lA-" target=_blank rel=noopener>Loss-aware Weight Quantization of Deep Networks</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=S19dR9x0b" target=_blank rel=noopener>Alternating Multi-bit Quantization for Recurrent Neural Networks</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=SyOK1Sg0W" target=_blank rel=noopener>Adaptive Quantization of Neural Networks</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=ry-TW-WAb" target=_blank rel=noopener>Variational Network Quantization</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=Sk6fD5yCb&amp;noteId=Sk6fD5yCb" target=_blank rel=noopener>Espresso: Efficient Forward Propagation for Binary Deep Neural Networks</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=rypT3fb0b&amp;noteId=rkwxPE67M" target=_blank rel=noopener>Learning to share: Simultaneous parameter tying and sparsification in deep learning</a></li><li>2018-ICLR-<a class=link href=https://arxiv.org/abs/1712.01312 target=_blank rel=noopener>Learning Sparse Neural Networks through L0 Regularization</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=B1ZvaaeAZ&amp;noteId=B1ZvaaeAZ" target=_blank rel=noopener>WRPN: Wide Reduced-Precision Networks</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=BJ_wN01C-&amp;noteId=BJ_wN01C-" target=_blank rel=noopener>Deep rewiring: Training very sparse deep networks</a></li><li>2018-ICLR-<a class=link href=https://arxiv.org/pdf/1802.06367.pdf target=_blank rel=noopener>Efficient sparse-winograd convolutional neural networks</a> [<a class=link href=https://github.com/xingyul/Sparse-Winograd-CNN target=_blank rel=noopener>Code</a>]</li><li>2018-ICLR-<a class=link href=https://arxiv.org/pdf/1709.05027 target=_blank rel=noopener>Learning Intrinsic Sparse Structures within Long Short-term Memory</a></li><li>2018-ICLR-<a class=link href=https://arxiv.org/abs/1703.09844 target=_blank rel=noopener>Multi-scale dense networks for resource efficient image classification</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=BJRZzFlRb&amp;noteId=BJRZzFlRb" target=_blank rel=noopener>Compressing Word Embedding via Deep Compositional Code Learning</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=BySRH6CpW" target=_blank rel=noopener>Learning Discrete Weights Using the Local Reparameterization Trick</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=rytNfI1AZ&amp;noteId=rytNfI1AZ" target=_blank rel=noopener>Training wide residual networks for deployment using a single bit for each weight</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=B1IDRdeCW&amp;noteId=B1IDRdeCW" target=_blank rel=noopener>The High-Dimensional Geometry of Binary Neural Networks</a></li><li>2018-ICLRw-<a class=link href="https://openreview.net/forum?id=Sy1iIDkPM" target=_blank rel=noopener>To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression</a> (Similar topic: <a class=link href="https://openreview.net/forum?id=r1lbgwFj5m" target=_blank rel=noopener>2018-NIPSw-nip in the bud</a>, <a class=link href="https://openreview.net/forum?id=r1eLk2mKiX" target=_blank rel=noopener>2018-NIPSw-rethink</a>)</li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_Context-Aware_Deep_Feature_CVPR_2018_paper.pdf target=_blank rel=noopener>Context-Aware Deep Feature Compression for High-Speed Visual Tracking</a></li><li>2018-CVPR-<a class=link href=https://arxiv.org/pdf/1711.05908.pdf target=_blank rel=noopener>NISP: Pruning Networks using Neuron Importance Score Propagation</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Huang_CondenseNet_An_Efficient_CVPR_2018_paper.html target=_blank rel=noopener>Condensenet: An efficient densenet using learned group convolutions</a> [<a class=link href=https://github.com/ShichenLiu/CondenseNet target=_blank rel=noopener>Code</a>]</li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Shift_A_Zero_CVPR_2018_paper.html target=_blank rel=noopener>Shift: A zero flop, zero parameter alternative to spatial convolutions</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Explicit_Loss-Error-Aware_Quantization_CVPR_2018_paper.html target=_blank rel=noopener>Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Xie_Interleaved_Structured_Sparse_CVPR_2018_paper.html target=_blank rel=noopener>Interleaved structured sparse convolutional neural networks</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhuang_Towards_Effective_Low-Bitwidth_CVPR_2018_paper.pdf target=_blank rel=noopener>Towards Effective Low-bitwidth Convolutional Neural Networks</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Tung_CLIP-Q_Deep_Network_CVPR_2018_paper.html target=_blank rel=noopener>CLIP-Q: Deep Network Compression Learning by In-Parallel Pruning-Quantization</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_BlockDrop_Dynamic_Inference_CVPR_2018_paper.html target=_blank rel=noopener>Blockdrop: Dynamic inference paths in residual networks</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Kim_NestedNet_Learning_Nested_CVPR_2018_paper.html target=_blank rel=noopener>Nestednet: Learning nested sparse structures in deep neural networks</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Kuen_Stochastic_Downsampling_for_CVPR_2018_paper.html target=_blank rel=noopener>Stochastic downsampling for cost-adjustable inference and improved regularization in convolutional networks</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Wide_Compression_Tensor_CVPR_2018_paper.html target=_blank rel=noopener>Wide Compression: Tensor Ring Nets</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Ye_Learning_Compact_Recurrent_CVPR_2018_paper.html target=_blank rel=noopener>Learning Compact Recurrent Neural Networks With Block-Term Tensor Decomposition</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Veniat_Learning_TimeMemory-Efficient_Deep_CVPR_2018_paper.html target=_blank rel=noopener>Learning Time/Memory-Efficient Deep Architectures With Budgeted Super Networks</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Mullapudi_HydraNets_Specialized_Dynamic_CVPR_2018_paper.html target=_blank rel=noopener>HydraNets: Specialized Dynamic Architectures for Efficient Inference</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Faraone_SYQ_Learning_Symmetric_CVPR_2018_paper.html target=_blank rel=noopener>SYQ: Learning Symmetric Quantization for Efficient Deep Neural Networks</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Zhuang_Towards_Effective_Low-Bitwidth_CVPR_2018_paper.html target=_blank rel=noopener>Towards Effective Low-Bitwidth Convolutional Neural Networks</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Two-Step_Quantization_for_CVPR_2018_paper.html target=_blank rel=noopener>Two-Step Quantization for Low-Bit Neural Networks</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Jacob_Quantization_and_Training_CVPR_2018_paper.html target=_blank rel=noopener>Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/papers/Carreira-Perpinan_Learning-Compression_Algorithms_for_CVPR_2018_paper.pdf target=_blank rel=noopener>&ldquo;Learning-Compression&rdquo; Algorithms for Neural Net Pruning</a></li><li>2018-CVPR-<a class=link href=https://arxiv.org/pdf/1711.05769v2.pdf target=_blank rel=noopener>PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning</a> [<a class=link href=https://github.com/arunmallya/packnet target=_blank rel=noopener>Code</a>]</li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Gordon_MorphNet_Fast__CVPR_2018_paper.html target=_blank rel=noopener>MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks</a> [<a class=link href=https://github.com/google-research/morph-net target=_blank rel=noopener>Code</a>]</li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.html target=_blank rel=noopener>ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a></li><li>2018-CVPRw-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018_workshops/w33/html/Gholami_SqueezeNext_Hardware-Aware_Neural_CVPR_2018_paper.html target=_blank rel=noopener>Squeezenext: Hardware-aware neural network design</a></li><li>2018-IJCAI-<a class=link href=https://www.ijcai.org/proceedings/2018/0318.pdf target=_blank rel=noopener>Efficient DNN Neuron Pruning by Minimizing Layer-wise Nonlinear Reconstruction Error</a></li><li>2018-IJCAI-<a class=link href=https://arxiv.org/abs/1808.06866 target=_blank rel=noopener>Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks</a> [<a class=link href=https://github.com/he-y/soft-filter-pruning target=_blank rel=noopener>PyTorch Code</a>]</li><li>2018-IJCAI-<a class=link href=https://www.ijcai.org/proceedings/2018/0445.pdf target=_blank rel=noopener>Where to Prune: Using LSTM to Guide End-to-end Pruning</a></li><li>2018-IJCAI-<a class=link href=https://www.ijcai.org/proceedings/2018/0336.pdf target=_blank rel=noopener>Accelerating Convolutional Networks via Global & Dynamic Filter Pruning</a></li><li>2018-IJCAI-<a class=link href=http://staff.ustc.edu.cn/~chaoqian/ijcai18-olmp.pdf target=_blank rel=noopener>Optimization based Layer-wise Magnitude-based Pruning for DNN Compression</a></li><li>2018-IJCAI-<a class=link href=https://www.ijcai.org/proceedings/2018/0384.pdf target=_blank rel=noopener>Progressive Blockwise Knowledge Distillation for Neural Network Acceleration</a></li><li>2018-IJCAI-<a class=link href=https://www.ijcai.org/proceedings/2018/0292.pdf target=_blank rel=noopener>Complementary Binary Quantization for Joint Multiple Indexing</a></li><li>2018-ICML-<a class=link href=http://proceedings.mlr.press/v80/dai18d.html target=_blank rel=noopener>Compressing Neural Networks using the Variational Information Bottleneck</a></li><li>2018-ICML-<a class=link href=http://proceedings.mlr.press/v80/qiu18a.html target=_blank rel=noopener>DCFNet: Deep Neural Network with Decomposed Convolutional Filters</a></li><li>2018-ICML-<a class=link href=http://proceedings.mlr.press/v80/wu18h.html target=_blank rel=noopener>Deep k-Means Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions</a></li><li>2018-ICML-<a class=link href=http://proceedings.mlr.press/v80/wu18d.html target=_blank rel=noopener>Error Compensated Quantized SGD and its Applications to Large-scale Distributed Optimization</a></li><li>2018-ICML-<a class=link href=http://proceedings.mlr.press/v80/zhang18d.html target=_blank rel=noopener>High Performance Zero-Memory Overhead Direct Convolutions</a></li><li>2018-ICML-<a class=link href=http://proceedings.mlr.press/v80/jose18a.html target=_blank rel=noopener>Kronecker Recurrent Units</a></li><li>2018-ICML-<a class=link href=http://proceedings.mlr.press/v80/reagan18a.html target=_blank rel=noopener>Weightless: Lossy weight encoding for deep neural network compression</a></li><li>2018-ICML-<a class=link href=http://proceedings.mlr.press/v80/tschannen18a.html target=_blank rel=noopener>StrassenNets: Deep learning with a multiplication budget</a></li><li>2018-ICML-<a class=link href=http://proceedings.mlr.press/v80/oymak18a.html target=_blank rel=noopener>Learning Compact Neural Networks with Regularization</a></li><li>2018-ICML-<a class=link href=http://proceedings.mlr.press/v80/jin18d.html target=_blank rel=noopener>WSNet: Compact and Efficient Networks Through Weight Sampling</a></li><li>2018-ICML-<a class=link href=https://arxiv.org/abs/1711.09280 target=_blank rel=noopener>Gradually Updated Neural Networks for Large-Scale Image Recognition</a> [<a class=link href=https://github.com/joe-siyuan-qiao/GUNN target=_blank rel=noopener>Code</a>]</li><li>2018-ICML-<a class=link href=https://arxiv.org/abs/1802.06509 target=_blank rel=noopener>On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization</a></li><li>2018-ICML-<a class=link href=http://proceedings.mlr.press/v80/bender18a.html target=_blank rel=noopener>Understanding and simplifying one-shot architecture search</a></li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/papers/Tianyun_Zhang_A_Systematic_DNN_ECCV_2018_paper.pdf target=_blank rel=noopener>A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers</a> [<a class=link href=https://github.com/KaiqiZhang/admm-pruning target=_blank rel=noopener>Code</a>]</li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/papers/Abhimanyu_Dubey_Coreset-Based_Convolutional_Neural_ECCV_2018_paper.pdf target=_blank rel=noopener>Coreset-Based Neural Network Compression</a></li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/papers/Zehao_Huang_Data-Driven_Sparse_Structure_ECCV_2018_paper.pdf target=_blank rel=noopener>Data-Driven Sparse Structure Selection for Deep Neural Networks</a> [<a class=link href=https://github.com/TuSimple/sparse-structure-selection target=_blank rel=noopener>MXNet Code</a>]</li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Qinghao_Hu_Training_Binary_Weight_ECCV_2018_paper.html target=_blank rel=noopener>Training Binary Weight Networks via Semi-Binary Decomposition</a></li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Xiangyu_He_Learning_Compression_from_ECCV_2018_paper.html target=_blank rel=noopener>Learning Compression from Limited Unlabeled Data</a></li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Changan_Chen_Constraints_Matter_in_ECCV_2018_paper.html target=_blank rel=noopener>Constraint-Aware Deep Neural Network Compression</a></li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Ligeng_Zhu_Sparsely_Aggregated_Convolutional_ECCV_2018_paper.html target=_blank rel=noopener>Sparsely Aggregated Convolutional Networks</a></li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper.html target=_blank rel=noopener>Deep Expander Networks: Efficient Deep Networks from Graph Theory</a> [<a class=link href=https://github.com/DrImpossible/Deep-Expander-Networks target=_blank rel=noopener>Code</a>]</li><li>2018-ECCV-<a class=link href=https://arxiv.org/abs/1801.05895 target=_blank rel=noopener>SparseNet-Sparsely Aggregated Convolutional Networks</a> [<a class=link href=https://github.com/Lyken17/SparseNet target=_blank rel=noopener>Code</a>]</li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Konda_Reddy_Mopuri_Ask_Acquire_and_ECCV_2018_paper.html target=_blank rel=noopener>Ask, acquire, and attack: Data-free uap generation using class impressions</a></li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Tien-Ju_Yang_NetAdapt_Platform-Aware_Neural_ECCV_2018_paper.html target=_blank rel=noopener>Netadapt: Platform-aware neural network adaptation for mobile applications</a></li><li>2018-ECCV-<a class=link href=https://link.springer.com/content/pdf/10.1007%2F978-3-030-01237-3_14.pdf target=_blank rel=noopener>Clustering Convolutional Kernels to Compress Deep Neural Networks</a></li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/zechun_liu_Bi-Real_Net_Enhancing_ECCV_2018_paper.html target=_blank rel=noopener>Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm</a></li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Bo_Peng_Extreme_Network_Compression_ECCV_2018_paper.html target=_blank rel=noopener>Extreme Network Compression via Filter Group Approximation</a></li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Andreas_Veit_Convolutional_Networks_with_ECCV_2018_paper.html target=_blank rel=noopener>Convolutional Networks with Adaptive Inference Graphs</a></li><li>2018-ECCV-<a class=link href=https://arxiv.org/abs/1711.09485 target=_blank rel=noopener>SkipNet: Learning Dynamic Routing in Convolutional Networks</a> [<a class=link href=https://github.com/ucbdrive/skipnet target=_blank rel=noopener>Code</a>]</li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Eunhyeok_Park_Value-aware_Quantization_for_ECCV_2018_paper.html target=_blank rel=noopener>Value-aware Quantization for Training and Inference of Neural Networks</a></li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper.html target=_blank rel=noopener>LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks</a></li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Yihui_He_AMC_Automated_Model_ECCV_2018_paper.html target=_blank rel=noopener>AMC: AutoML for Model Compression and Acceleration on Mobile Devices</a></li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper.html target=_blank rel=noopener>Piggyback: Adapting a single network to multiple tasks by learning to mask weights</a></li><li>2018-BMVCo-<a class=link href=http://bmvc2018.org/contents/papers/0870.pdf target=_blank rel=noopener>Structured Probabilistic Pruning for Convolutional Neural Network Acceleration</a></li><li>2018-BMVC-<a class=link href=http://bmvc2018.org/contents/papers/0291.pdf target=_blank rel=noopener>Efficient Progressive Neural Architecture Search</a></li><li>2018-BMVC-<a class=link href=https://arxiv.org/abs/1806.00178 target=_blank rel=noopener>Igcv3: Interleaved lowrank group convolutions for efficient deep neural networks</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7367-discrimination-aware-channel-pruning-for-deep-neural-networks.pdf target=_blank rel=noopener>Discrimination-aware Channel Pruning for Deep Neural Networks</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7382-frequency-domain-dynamic-pruning-for-convolutional-neural-networks.pdf target=_blank rel=noopener>Frequency-Domain Dynamic Pruning for Convolutional Neural Networks</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7766-channelnets-compact-and-efficient-convolutional-neural-networks-via-channel-wise-convolutions.pdf target=_blank rel=noopener>ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/8271-dropblock-a-regularization-method-for-convolutional-networks target=_blank rel=noopener>DropBlock: A regularization method for convolutional networks</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7835-constructing-fast-network-through-deconstruction-of-convolution target=_blank rel=noopener>Constructing fast network through deconstruction of convolution</a></li><li>2018-NIPS-<a class=link href=https://papers.nips.cc/paper/7433-learning-versatile-filters-for-efficient-convolutional-neural-networks target=_blank rel=noopener>Learning Versatile Filters for Efficient Convolutional Neural Networks</a> [<a class=link href=https://github.com/NoahEC/Versatile-Filters target=_blank rel=noopener>Code</a>]</li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7553-moonshine-distilling-with-cheap-convolutions target=_blank rel=noopener>Moonshine: Distilling with cheap convolutions</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7341-hitnet-hybrid-ternary-recurrent-neural-network target=_blank rel=noopener>HitNet: hybrid ternary recurrent neural network</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/8116-fastgrnn-a-fast-accurate-stable-and-tiny-kilobyte-sized-gated-recurrent-neural-network target=_blank rel=noopener>FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7327-training-dnns-with-hybrid-block-floating-point target=_blank rel=noopener>Training DNNs with Hybrid Block Floating Point</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/8117-reversible-recurrent-neural-networks target=_blank rel=noopener>Reversible Recurrent Neural Networks</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/8218-synaptic-strength-for-convolutional-neural-network target=_blank rel=noopener>Synaptic Strength For Convolutional Neural Network</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7644-learning-sparse-neural-networks-via-sensitivity-driven-regularization target=_blank rel=noopener>Learning sparse neural networks via sensitivity-driven regularization</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7841-multi-task-zipping-via-layer-wise-neuron-sharing target=_blank rel=noopener>Multi-Task Zipping via Layer-wise Neuron Sharing</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7519-a-linear-speedup-analysis-of-distributed-deep-learning-with-sparse-and-quantized-communication target=_blank rel=noopener>A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7405-gradient-sparsification-for-communication-efficient-distributed-optimization target=_blank rel=noopener>Gradient Sparsification for Communication-Efficient Distributed Optimization</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7759-gradiveq-vector-quantization-for-bandwidth-efficient-gradient-aggregation-in-distributed-cnn-training target=_blank rel=noopener>GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/8191-atomo-communication-efficient-learning-via-atomic-sparsification target=_blank rel=noopener>ATOMO: Communication-efficient Learning via Atomic Sparsification</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7485-norm-matters-efficient-and-accurate-normalization-schemes-in-deep-networks target=_blank rel=noopener>Norm matters: efficient and accurate normalization schemes in deep networks</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7697-sparsified-sgd-with-memory target=_blank rel=noopener>Sparsified SGD with memory</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7466-pelee-a-real-time-object-detection-system-on-mobile-devices target=_blank rel=noopener>Pelee: A Real-Time Object Detection System on Mobile Devices</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7761-scalable-methods-for-8-bit-training-of-neural-networks target=_blank rel=noopener>Scalable methods for 8-bit training of neural networks</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7666-tetris-tile-matching-the-tremendous-irregular-sparsity target=_blank rel=noopener>TETRIS: TilE-matching the TRemendous Irregular Sparsity</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7994-training-deep-neural-networks-with-8-bit-floating-point-numbers target=_blank rel=noopener>Training deep neural networks with 8-bit floating point numbers</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/8292-multiple-instance-learning-for-efficient-sequential-data-classification-on-resource-constrained-devices target=_blank rel=noopener>Multiple instance learning for efficient sequential data classification on resource-constrained devices</a></li><li>2018-NIPS-<a class=link href=https://papers.nips.cc/paper/2018/hash/4c5bde74a8f110656874902f07378009-Abstract.html target=_blank rel=noopener>Sparse dnns with improved adversarial robustness</a></li><li>2018-NIPSw-<a class=link href="https://openreview.net/forum?id=r1lbgwFj5m" target=_blank rel=noopener>Pruning neural networks: is it time to nip it in the bud?</a></li><li>2018-NIPSw-<a class=link href="https://openreview.net/forum?id=r1eLk2mKiX" target=_blank rel=noopener>Rethinking the Value of Network Pruning</a> [<a class=link href="https://openreview.net/forum?id=rJlnB3C5Ym" target=_blank rel=noopener>2019 ICLR version</a>] [<a class=link href=https://github.com/Eric-mingjie/rethinking-network-pruning target=_blank rel=noopener>PyTorch Code</a>]</li><li>2018-NIPSw-<a class=link href="https://openreview.net/forum?id=S1e_xM7_iQ" target=_blank rel=noopener>Structured Pruning for Efficient ConvNets via Incremental Regularization</a> [<a class=link href=https://arxiv.org/abs/1804.09461 target=_blank rel=noopener>2019 IJCNN version</a>] [<a class=link href=https://github.com/MingSun-Tse/Caffe_IncReg target=_blank rel=noopener>Caffe Code</a>]</li><li>2018-NIPSw-<a class=link href="https://openreview.net/forum?id=B1eHgu-Fim" target=_blank rel=noopener>Adaptive Mixture of Low-Rank Factorizations for Compact Neural Modeling</a></li><li>2018-NIPSw-<a class=link href=https://arxiv.org/abs/1905.13678 target=_blank rel=noopener>Learning Sparse Networks Using Targeted Dropout</a> [<a class=link href="https://openreview.net/forum?id=HkghWScuoQ" target=_blank rel=noopener>OpenReview</a>] [<a class=link href=https://github.com/for-ai/TD target=_blank rel=noopener>Code</a>]</li><li>2018-WACV-<a class=link href=https://arxiv.org/abs/1801.10447 target=_blank rel=noopener>Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks</a></li><li>2018.05-<a class=link href=https://arxiv.org/abs/1805.08303 target=_blank rel=noopener>Compression of Deep Convolutional Neural Networks under Joint Sparsity Constraints</a></li><li>2018.05-<a class=link href=https://arxiv.org/abs/1805.08941 target=_blank rel=noopener>AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference</a></li><li>2018.10-<a class=link href=https://arxiv.org/abs/1810.04622 target=_blank rel=noopener>A Closer Look at Structured Pruning for Neural Network Compression</a> [<a class=link href=https://github.com/BayesWatch/pytorch-prunes target=_blank rel=noopener>Code</a>]</li><li>2018.11-<a class=link href=https://arxiv.org/abs/1811.12019 target=_blank rel=noopener>Second-order Optimization Method for Large Mini-batch: Training ResNet-50 on ImageNet in 35 Epochs</a></li><li>2018.11-<a class=link href=https://arxiv.org/abs/1811.07083 target=_blank rel=noopener>PydMobileNet: Improved Version of MobileNets with Pyramid Depthwise Separable Convolution</a></li></ul><p><strong>2019</strong></p><ul><li>2019-MLSys-<a class=link href=https://arxiv.org/pdf/1902.01046.pdf target=_blank rel=noopener>Towards Federated Learning at Scale: System Design</a></li><li>2019-MLsys-<a class=link href=https://arxiv.org/abs/1810.00208 target=_blank rel=noopener>To compress or not to compress: Understanding the Interactions between Adversarial Attacks and Neural Network Compression</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=H1gMCsAqY7" target=_blank rel=noopener>Slimmable Neural Networks</a> [<a class=link href=https://github.com/JiahuiYu/slimmable_networks target=_blank rel=noopener>Code</a>]</li><li>2019-ICLR-<a class=link href=https://arxiv.org/abs/1904.08444 target=_blank rel=noopener>Defensive Quantization: When Efficiency Meets Robustness</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=r1f0YiCctm" target=_blank rel=noopener>Minimal Random Code Learning: Getting Bits Back from Compressed Model Parameters</a> [<a class=link href=https://github.com/cambridge-mlg/miracle target=_blank rel=noopener>Code</a>]</li><li>2019-ICLR-<a class=link href=https://arxiv.org/abs/1812.00332 target=_blank rel=noopener>ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</a> [<a class=link href=https://github.com/MIT-HAN-LAB/ProxylessNAS target=_blank rel=noopener>Code</a>]</li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=B1VZqjAcYX" target=_blank rel=noopener>SNIP: Single-shot Network Pruning based on Connection Sensitivity</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=BJgqqsAct7" target=_blank rel=noopener>Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=BJxh2j0qYm" target=_blank rel=noopener>Dynamic Channel Pruning: Feature Boosting and Suppression</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=BylBr3C9K7" target=_blank rel=noopener>Energy-Constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=H1gTEj09FX" target=_blank rel=noopener>RotDCF: Decomposition of Convolutional Filters for Rotation-Equivariant Deep Networks</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=H1goBoR9F7" target=_blank rel=noopener>Dynamic Sparse Graph for Efficient Deep Learning</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=HJMHpjC9Ym" target=_blank rel=noopener>Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=HJfwJ2A5KX" target=_blank rel=noopener>Data-Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=HkNGYjR9FX" target=_blank rel=noopener>Learning Recurrent Binary/Ternary Weights</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=HkfYOoCcYX" target=_blank rel=noopener>Double Viterbi: Weight Encoding for High Compression Ratio and Fast On-Chip Reconstruction for Deep Neural Network</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=HkxjYoCqKX" target=_blank rel=noopener>Relaxed Quantization for Discretized Neural Networks</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=S1zz2i0cY7" target=_blank rel=noopener>Integer Networks for Data Compression with Latent-Variable Models</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=r1f0YiCctm" target=_blank rel=noopener>Minimal Random Code Learning: Getting Bits Back from Compressed Model Parameters</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=ryM_IoAqYX" target=_blank rel=noopener>Analysis of Quantized Models</a></li><li>2019-ICLR-<a class=link href=https://arxiv.org/abs/1806.09055 target=_blank rel=noopener>DARTS: Differentiable Architecture Search</a> [<a class=link href=https://github.com/quark0/darts target=_blank rel=noopener>Code</a>]</li><li>2019-ICLR-<a class=link href=https://arxiv.org/abs/1810.05749 target=_blank rel=noopener>Graph HyperNetworks for Neural Architecture Search</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/forum?id=S1xLN3C9YX" target=_blank rel=noopener>Learnable Embedding Space for Efficient Neural Architecture Compression</a> [<a class=link href=https://github.com/Friedrich1006/ESNAC target=_blank rel=noopener>Code</a>]</li><li>2019-ICLR-<a class=link href=https://arxiv.org/abs/1804.09081 target=_blank rel=noopener>Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution</a></li><li>2019-ICLR-<a class=link href="https://openreview.net/pdf?id=rylqooRqK7" target=_blank rel=noopener>SNAS: stochastic neural architecture search</a></li><li>2019-AAAIo-<a class=link href=https://arxiv.org/abs/1812.06611 target=_blank rel=noopener>A layer decomposition-recomposition framework for neuron pruning towards accurate lightweight networks</a></li><li>2019-AAAI-<a class=link href=https://arxiv.org/abs/1811.00206 target=_blank rel=noopener>Balanced Sparsity for Efficient DNN Inference on GPU</a> [<a class=link href=https://github.com/Howal/balanced-sparsity target=_blank rel=noopener>Code</a>]</li><li>2019-AAAI-<a class=link href=https://arxiv.org/abs/1902.11268 target=_blank rel=noopener>CircConv: A Structured Convolution with Low Complexity</a></li><li>2019-AAAI-<a class=link href=https://arxiv.org/pdf/1802.01548.pdf target=_blank rel=noopener>Regularized Evolution for Image Classifier Architecture Search</a></li><li>2019-AAAI-<a class=link href=https://www.aaai.org/ojs/index.php/AAAI/article/view/4475 target=_blank rel=noopener>Universal Approximation Property and Equivalence of Stochastic Computing-Based Neural Networks and Binary Neural Networks</a></li><li>2019-WACV-<a class=link href=https://arxiv.org/abs/1812.08374 target=_blank rel=noopener>DAC: Data-free Automatic Acceleration of Convolutional Networks</a></li><li>2019-ASPLOS-<a class=link href=https://arxiv.org/abs/1811.04770 target=_blank rel=noopener>Packing Sparse Convolutional Neural Networks for Efficient Systolic Array Implementations: Column Combining Under Joint Optimization</a></li><li>2019-CVPRo-<a class=link href=https://arxiv.org/pdf/1811.08886.pdf target=_blank rel=noopener>HAQ: hardware-aware automated quantization</a></li><li>2019-CVPRo-<a class=link href=https://arxiv.org/abs/1811.00250 target=_blank rel=noopener>Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration</a> [<a class=link href=https://github.com/he-y/filter-pruning-geometric-median target=_blank rel=noopener>Code</a>]</li><li>2019-CVPR-<a class=link href=https://arxiv.org/abs/1903.05285 target=_blank rel=noopener>All You Need is a Few Shifts: Designing Efficient Convolutional Neural Networks for Image Classification</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/html/Molchanov_Importance_Estimation_for_Neural_Network_Pruning_CVPR_2019_paper.html target=_blank rel=noopener>Importance Estimation for Neural Network Pruning</a> [<a class=link href=https://github.com/NVlabs/Taylor_pruning target=_blank rel=noopener>Code</a>]</li><li>2019-CVPR-<a class=link href=https://arxiv.org/abs/1903.04120 target=_blank rel=noopener>HetConv Heterogeneous Kernel-Based Convolutions for Deep CNNs</a></li><li>2019-CVPR-<a class=link href=https://arxiv.org/abs/1904.00346 target=_blank rel=noopener>Fully Learnable Group Convolution for Acceleration of Deep Neural Networks</a></li><li>2019-CVPR-<a class=link href=https://arxiv.org/abs/1903.09291 target=_blank rel=noopener>Towards Optimal Structured CNN Pruning via Generative Adversarial Learning</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/papers/Dai_ChamNet_Towards_Efficient_Network_Design_Through_Platform-Aware_Model_Adaptation_CVPR_2019_paper.pdf target=_blank rel=noopener>ChamNet: Towards Efficient Network Design through Platform-Aware Model Adaptation</a></li><li>2019-CVPR-<a class=link href=https://arxiv.org/pdf/1903.03777.pdf target=_blank rel=noopener>Partial Order Pruning: for Best Speed/Accuracy Trade-off in Neural Architecture Search</a> [<a class=link href=https://github.com/lixincn2015/Partial-Order-Pruning target=_blank rel=noopener>Code</a>]</li><li>2019-CVPR-<a class=link href=https://arxiv.org/pdf/1901.02985.pdf target=_blank rel=noopener>Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation</a> [<a class=link href=https://github.com/tensorflow/models/tree/master/research/deeplab target=_blank rel=noopener>Code</a>]</li><li>2019-CVPR-<a class=link href=https://arxiv.org/abs/1807.11626 target=_blank rel=noopener>MnasNet: Platform-Aware Neural Architecture Search for Mobile</a> [<a class=link href=https://github.com/AnjieZheng/MnasNet-PyTorch target=_blank rel=noopener>Code</a>]</li><li>2019-CVPR-<a class=link href=https://arxiv.org/pdf/1903.06496.pdf target=_blank rel=noopener>MFAS: Multimodal Fusion Architecture Search</a></li><li>2019-CVPR-<a class=link href=https://arxiv.org/pdf/1805.10726.pdf target=_blank rel=noopener>A Neurobiological Evaluation Metric for Neural Network Model Search</a></li><li>2019-CVPR-<a class=link href=https://arxiv.org/abs/1810.10804 target=_blank rel=noopener>Fast Neural Architecture Search of Compact Semantic Segmentation Models via Auxiliary Cells</a></li><li>2019-CVPR-<a class=link href=https://arxiv.org/abs/1811.12781 target=_blank rel=noopener>Efficient Neural Network Compression</a> [<a class=link href=https://github.com/Hyeji-Kim/ENC target=_blank rel=noopener>Code</a>]</li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/html/Kossaifi_T-Net_Parametrizing_Fully_Convolutional_Nets_With_a_Single_High-Order_Tensor_CVPR_2019_paper.html target=_blank rel=noopener>T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order Tensor</a></li><li>2019-CVPR-<a class=link href=https://arxiv.org/abs/1904.03837 target=_blank rel=noopener>Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure</a> [<a class=link href=https://github.com/ShawnDing1994/Centripetal-SGD target=_blank rel=noopener>Code</a>]</li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPRW_2019/html/SAIAD/Frickenstein_DSC_Dense-Sparse_Convolution_for_Vectorized_Inference_of_Convolutional_Neural_Networks_CVPRW_2019_paper.html target=_blank rel=noopener>DSC: Dense-Sparse Convolution for Vectorized Inference of Convolutional Neural Networks</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPRW_2019/html/EVW/Gao_DupNet_Towards_Very_Tiny_Quantized_CNN_With_Improved_Accuracy_for_CVPRW_2019_paper.html target=_blank rel=noopener>DupNet: Towards Very Tiny Quantized CNN With Improved Accuracy for Face Detection</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_ECC_Platform-Independent_Energy-Constrained_Deep_Neural_Network_Compression_via_a_Bilinear_CVPR_2019_paper.html target=_blank rel=noopener>ECC: Platform-Independent Energy-Constrained Deep Neural Network Compression via a Bilinear Regression Model</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Variational_Convolutional_Neural_Network_Pruning_CVPR_2019_paper.html target=_blank rel=noopener>Variational Convolutional Neural Network Pruning</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/html/Georgiadis_Accelerating_Convolutional_Neural_Networks_via_Activation_Map_Compression_CVPR_2019_paper.html target=_blank rel=noopener>Accelerating Convolutional Neural Networks via Activation Map Compression</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Compressing_Convolutional_Neural_Networks_via_Factorized_Convolutional_Filters_CVPR_2019_paper.html target=_blank rel=noopener>Compressing Convolutional Neural Networks via Factorized Convolutional Filters</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Deep_Virtual_Networks_for_Memory_Efficient_Inference_of_Multiple_Tasks_CVPR_2019_paper.html target=_blank rel=noopener>Deep Virtual Networks for Memory Efficient Inference of Multiple Tasks</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Exploiting_Kernel_Sparsity_and_Entropy_for_Interpretable_CNN_Compression_CVPR_2019_paper.html target=_blank rel=noopener>Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/html/Lin_MBS_Macroblock_Scaling_for_CNN_Model_Reduction_CVPR_2019_paper.html target=_blank rel=noopener>MBS: Macroblock Scaling for CNN Model Reduction</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/html/Mehta_On_Implicit_Filter_Level_Sparsity_in_Convolutional_Neural_Networks_CVPR_2019_paper.html target=_blank rel=noopener>On Implicit Filter Level Sparsity in Convolutional Neural Networks</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/html/Lemaire_Structured_Pruning_of_Neural_Networks_With_Budget-Aware_Regularization_CVPR_2019_paper.html target=_blank rel=noopener>Structured Pruning of Neural Networks With Budget-Aware Regularization</a></li><li>2019-CVPRo-<a class=link href=https://arxiv.org/abs/1812.00481 target=_blank rel=noopener>Neural Rejuvenation: Improving Deep Network Training by Enhancing Computational Resource Utilization</a> [<a class=link href=https://github.com/joe-siyuan-qiao/NeuralRejuvenation-CVPR19 target=_blank rel=noopener>Code</a>]</li><li>2019-ICML-<a class=link href=https://arxiv.org/abs/1905.04748 target=_blank rel=noopener>Approximated Oracle Filter Pruning for Destructive CNN Width Optimization</a> [<a class=link href=https://github.com/ShawnDing1994/AOFP target=_blank rel=noopener>Code</a>]</li><li>2019-ICML-<a class=link href=https://arxiv.org/abs/1905.05934 target=_blank rel=noopener>EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis</a> [<a class=link href=https://github.com/alecwangcq/EigenDamage-Pytorch target=_blank rel=noopener>PyTorch Code</a>]</li><li>2019-ICML-<a class=link href=https://arxiv.org/abs/1905.08114 target=_blank rel=noopener>Zero-Shot Knowledge Distillation in Deep Networks</a> [<a class=link href=https://github.com/vcl-iisc/ZSKD target=_blank rel=noopener>Code</a>]</li><li>2019-ICML-<a class=link href=http://proceedings.mlr.press/v97/yang19c.html target=_blank rel=noopener>LegoNet: Efficient Convolutional Neural Networks with Lego Filters</a> [<a class=link href=https://github.com/zhaohui-yang/LegoNet_pytorch target=_blank rel=noopener>Code</a>]</li><li>2019-ICML-<a class=link href=https://arxiv.org/abs/1905.11946 target=_blank rel=noopener>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a> [<a class=link href=https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet target=_blank rel=noopener>Code</a>]</li><li>2019-ICML-<a class=link href=http://proceedings.mlr.press/v97/peng19c.html target=_blank rel=noopener>Collaborative Channel Pruning for Deep Networks</a></li><li>2019-ICML-<a class=link href=http://proceedings.mlr.press/v97/jeong19c.html target=_blank rel=noopener>Training CNNs with Selective Allocation of Channels</a></li><li>2019-ICML-<a class=link href=https://arxiv.org/abs/1902.09635 target=_blank rel=noopener>NAS-Bench-101: Towards Reproducible Neural Architecture Search</a> [<a class=link href=https://github.com/google-research/nasbench target=_blank rel=noopener>Code</a>]</li><li>2019-ICML-<a class=link href=https://arxiv.org/abs/1903.05895 target=_blank rel=noopener>Learning fast algorithms for linear transforms using butterfly factorizations</a></li><li>2019-ICMLw-<a class=link href=https://arxiv.org/abs/1904.09872 target=_blank rel=noopener>Towards Learning of Filter-Level Heterogeneous Compression of Convolutional Neural Networks</a> [<a class=link href=https://github.com/yochaiz/Slimmable target=_blank rel=noopener>Code</a>] (AutoML workshop)</li><li>2019-IJCAI-<a class=link href=https://arxiv.org/abs/1905.04446 target=_blank rel=noopener>Play and Prune: Adaptive Filter Pruning for Deep Model Compression</a></li><li>2019-BigComp-<a class=link href=https://ieeexplore.ieee.org/abstract/document/8679132 target=_blank rel=noopener>Towards Robust Compressed Convolutional Neural Networks</a></li><li>2019-ICCV-<a class=link href=https://arxiv.org/abs/1811.08883 target=_blank rel=noopener>Rethinking ImageNet Pre-training</a></li><li>2019-ICCV-<a class=link href=https://arxiv.org/abs/1903.05134 target=_blank rel=noopener>Universally Slimmable Networks and Improved Training Techniques</a></li><li>2019-ICCV-<a class=link href=https://arxiv.org/abs/1903.10258 target=_blank rel=noopener>MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning</a> [<a class=link href=https://github.com/liuzechun/MetaPruning target=_blank rel=noopener>Code</a>]</li><li>2019-ICCV-<a class=link href=https://arxiv.org/abs/1904.12760 target=_blank rel=noopener>Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation</a> [<a class=link href=https://github.com/chenxin061/pdarts target=_blank rel=noopener>Code</a>]</li><li>2019-ICCV-<a class=link href=https://arxiv.org/abs/1906.04721 target=_blank rel=noopener>Data-Free Quantization through Weight Equalization and Bias Correction</a></li><li>2019-ICCV-<a class=link href=https://arxiv.org/abs/1908.03930 target=_blank rel=noopener>ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks</a></li><li>2019-ICCV-<a class=link href=https://openaccess.thecvf.com/content_ICCV_2019/papers/Ye_Adversarial_Robustness_vs._Model_Compression_or_Both_ICCV_2019_paper.pdf target=_blank rel=noopener>Adversarial Robustness vs. Model Compression, or Both?</a> [<a class=link href=https://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM target=_blank rel=noopener>PyTorch Code</a>]</li><li>2019-NIPS-<a class=link href=https://arxiv.org/abs/1909.12778 target=_blank rel=noopener>Global Sparse Momentum SGD for Pruning Very Deep Neural Networks</a></li><li>2019-NIPS-<a class=link href=http://papers.nips.cc/paper/8410-model-compression-with-adversarial-robustness-a-unified-optimization-framework target=_blank rel=noopener>Model Compression with Adversarial Robustness: A Unified Optimization Framework</a></li><li>2019-NIPS-<a class=link href="https://nips.cc/Conferences/2019/Schedule?showEvent=14303" target=_blank rel=noopener>AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters</a></li><li>2019-NIPS-<a class=link href="https://nips.cc/Conferences/2019/Schedule?showEvent=13598" target=_blank rel=noopener>Double Quantization for Communication-Efficient Distributed Optimization</a></li><li>2019-NIPS-<a class=link href="https://nips.cc/Conferences/2019/Schedule?showEvent=13686" target=_blank rel=noopener>Focused Quantization for Sparse CNNs</a></li><li>2019-NIPS-<a class=link href=http://papers.nips.cc/paper/8757-e2-train-training-state-of-the-art-cnns-with-over-80-less-energy target=_blank rel=noopener>E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings</a></li><li>2019-NIPS-<a class=link href=https://papers.nips.cc/paper/8647-metaquant-learning-to-quantize-by-learning-to-penetrate-non-differentiable-quantization target=_blank rel=noopener>MetaQuant: Learning to Quantize by Learning to Penetrate Non-differentiable Quantization</a></li><li>2019-NIPS-<a class=link href=https://papers.nips.cc/paper/9268-random-projections-with-asymmetric-quantization target=_blank rel=noopener>Random Projections with Asymmetric Quantization</a></li><li>2019-NIPS-<a class=link href=https://arxiv.org/abs/1905.09717 target=_blank rel=noopener>Network Pruning via Transformable Architecture Search</a> [<a class=link href=https://github.com/D-X-Y/TAS target=_blank rel=noopener>Code</a>]</li><li>2019-NIPS-<a class=link href=http://papers.nips.cc/paper/8382-point-voxel-cnn-for-efficient-3d-deep-learning target=_blank rel=noopener>Point-Voxel CNN for Efficient 3D Deep Learning</a> [<a class=link href=https://github.com/mit-han-lab/pvcnn target=_blank rel=noopener>Code</a>]</li><li>2019-NIPS-<a class=link href=https://arxiv.org/abs/1909.08174 target=_blank rel=noopener>Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks</a> [<a class=link href=https://github.com/youzhonghui/gate-decorator-pruning target=_blank rel=noopener>PyTorch Code</a>]</li><li>2019-NIPS-<a class=link href=https://papers.nips.cc/paper/8926-a-mean-field-theory-of-quantized-deep-networks-the-quantization-depth-trade-off target=_blank rel=noopener>A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off</a></li><li>2019-NIPS-<a class=link href=https://papers.nips.cc/paper/9610-qsparse-local-sgd-distributed-sgd-with-quantization-sparsification-and-local-computations target=_blank rel=noopener>Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations</a></li><li>2019-NIPS-<a class=link href=https://papers.nips.cc/paper/9008-post-training-4-bit-quantization-of-convolutional-networks-for-rapid-deployment target=_blank rel=noopener>Post training 4-bit quantization of convolutional networks for rapid-deployment</a></li><li>2019-PR-<a class=link href=https://www.sciencedirect.com/science/article/abs/pii/S0031320319300640 target=_blank rel=noopener>Filter-in-Filter: Improve CNNs in a Low-cost Way by Sharing Parameters among the Sub-filters of a Filter</a></li><li>2019-PRL-<a class=link href=https://www.sciencedirect.com/science/article/abs/pii/S0167865519301096 target=_blank rel=noopener>BDNN: Binary Convolution Neural Networks for Fast Object Detection</a></li><li>2019-TNNLS-<a class=link href=https://arxiv.org/abs/1901.07827 target=_blank rel=noopener>Towards Compact ConvNets via Structure-Sparsity Regularized Filter Pruning</a> [<a class=link href=https://github.com/ShaohuiLin/SSR target=_blank rel=noopener>Code</a>]</li><li>2019.03-<a class=link href=https://arxiv.org/abs/1903.11728 target=_blank rel=noopener>Network Slimming by Slimmable Networks: Towards One-Shot Architecture Search for Channel Numbers</a> [<a class=link href=https://github.com/JiahuiYu/slimmable_networks target=_blank rel=noopener>Code</a>]</li><li>2019.03-<a class=link href=https://arxiv.org/abs/1904.00420 target=_blank rel=noopener>Single Path One-Shot Neural Architecture Search with Uniform Sampling</a></li><li>2019.04-<a class=link href=https://arxiv.org/abs/1904.02422 target=_blank rel=noopener>Resource Efficient 3D Convolutional Neural Networks</a></li><li>2019.04-<a class=link href=https://arxiv.org/abs/1904.03961 target=_blank rel=noopener>Meta Filter Pruning to Accelerate Deep Convolutional Neural Networks</a></li><li>2019.04-<a class=link href=https://arxiv.org/abs/1904.05100 target=_blank rel=noopener>Knowledge Squeezed Adversarial Network Compression</a></li><li>2019.05-<a class=link href=https://arxiv.org/abs/1905.06435 target=_blank rel=noopener>Dynamic Neural Network Channel Execution for Efficient Training</a></li><li>2019.06-<a class=link href=https://arxiv.org/abs/1906.02909 target=_blank rel=noopener>AutoGrow: Automatic Layer Growing in Deep Convolutional Networks</a></li><li>2019.06-<a class=link href=https://arxiv.org/abs/1906.04509 target=_blank rel=noopener>BasisConv: A method for compressed representation and learning in CNNs</a></li><li>2019.06-<a class=link href=https://arxiv.org/abs/1906.04113 target=_blank rel=noopener>BlockSwap: Fisher-guided Block Substitution for Network Compression</a></li><li>2019.06-<a class=link href=https://arxiv.org/abs/1906.00859 target=_blank rel=noopener>Separable Layers Enable Structured Efficient Linear Substitutions</a> [<a class=link href=https://github.com/BayesWatch/deficient-efficient target=_blank rel=noopener>Code</a>]</li><li>2019.06-<a class=link href=https://arxiv.org/abs/1906.02256 target=_blank rel=noopener>Butterfly Transform: An Efficient FFT Based Neural Architecture Design</a></li><li>2019.06-<a class=link href=https://arxiv.org/abs/1906.04675 target=_blank rel=noopener>A Taxonomy of Channel Pruning Signals in CNNs</a></li><li>2019.08-<a class=link href=https://arxiv.org/abs/1908.04355 target=_blank rel=noopener>Adversarial Neural Pruning with Latent Vulnerability Suppression</a></li><li>2019.09-<a class=link href=https://arxiv.org/abs/1909.13063 target=_blank rel=noopener>Training convolutional neural networks with cheap convolutions and online distillation</a></li><li>2019.09-<a class=link href=https://arxiv.org/abs/1909.12579 target=_blank rel=noopener>Pruning from Scratch</a></li><li>2019.11-<a class=link href="https://openreview.net/forum?id=Syejj0NYvr" target=_blank rel=noopener>Adversarial Interpolation Training: A Simple Approach for Improving Model Robustness</a></li><li>2019.11-<a class=link href=https://arxiv.org/abs/1911.02497 target=_blank rel=noopener>A Programmable Approach to Model Compression</a> [<a class=link href=https://github.com/NVlabs/condensa target=_blank rel=noopener>Code</a>]</li></ul><p><strong>2020</strong></p><ul><li>2020-AAAI-<a class=link href=https://arxiv.org/abs/1909.05073 target=_blank rel=noopener>Pconv: The missing but desirable sparsity in dnn weight pruning for real-time execution on mobile devices</a></li><li>2020-AAAI-<a class=link href=https://arxiv.org/abs/2003.06757 target=_blank rel=noopener>Channel Pruning Guided by Classification Loss and Feature Importance</a></li><li>2020-AAAI-<a class=link href="https://arxiv.org/abs/1909.12579?context=cs.CV" target=_blank rel=noopener>Pruning from Scratch</a></li><li>2020-AAAI-<a class=link href=https://aaai.org/Papers/AAAI/2020GB/AAAI-YangL.9289.pdf target=_blank rel=noopener>Harmonious Coexistence of Structured Weight Pruning and Ternarization for Deep Neural Networks</a></li><li>2020-AAAI-<a class=link href=https://arxiv.org/abs/1907.03141 target=_blank rel=noopener>AutoCompress: An Automatic DNN Structured Pruning Framework for Ultra-High Compression Rates</a></li><li>2020-AAAI-<a class=link href=https://arxiv.org/abs/1911.08020 target=_blank rel=noopener>DARB: A Density-Adaptive Regular-Block Pruning for Deep Neural Networks</a></li><li>2020-AAAI-<a class=link href=https://arxiv.org/abs/1911.11170 target=_blank rel=noopener>Real-Time Object Tracking via Meta-Learning: Efficient Model Adaptation and One-Shot Channel Pruning</a></li><li>2020-AAAI-<a class=link href=https://aaai.org/ojs/index.php/AAAI/article/view/6098 target=_blank rel=noopener>Dynamic Network Pruning with Interpretable Layerwise Channel Selection</a></li><li>2020-AAAI-<a class=link href=https://www.aaai.org/Papers/AAAI/2020GB/AAAI-TangY.1279.pdf target=_blank rel=noopener>Reborn Filters: Pruning Convolutional Neural Networks with Limited Data</a></li><li>2020-AAAI-<a class=link href=https://aaai.org/ojs/index.php/AAAI/article/view/5927 target=_blank rel=noopener>Layerwise Sparse Coding for Pruned Deep Neural Networks with Extreme Compression Ratio</a></li><li>2020-AAAI-<a class=link href=https://www.aaai.org/Papers/AAAI/2020GB/AAAI-WangP.1440.pdf target=_blank rel=noopener>Sparsity-inducing Binarized Neural Networks</a></li><li>2020-AAAI-<a class=link href=https://www.aaai.org/Papers/AAAI/2020GB/AAAI-LobachevaE.8844.pdf target=_blank rel=noopener>Structured Sparsification of Gated Recurrent Neural Networks</a></li><li>2020-AAAI-<a class=link href=https://www.aaai.org/Papers/AAAI/2020GB/AAAI-LiP.697.pdf target=_blank rel=noopener>Hierarchical Knowledge Squeezed Adversarial Network Compression</a></li><li>2020-AAAI-<a class=link href=https://arxiv.org/abs/2001.05314 target=_blank rel=noopener>Embedding Compression with Isotropic Iterative Quantization</a></li><li>2020-ICLR-<a class=link href="https://openreview.net/forum?id=S1gSj0NKvB" target=_blank rel=noopener>Comparing Rewinding and Fine-tuning in Neural Network Pruning</a> [<a class=link href=https://github.com/lottery-ticket/rewinding-iclr20-public target=_blank rel=noopener>Code</a>]</li><li>2020-ICLR-<a class=link href="https://openreview.net/forum?id=ryl3ygHYDB" target=_blank rel=noopener>Lookahead: A Far-sighted Alternative of Magnitude-based Pruning</a> [<a class=link href=https://github.com/alinlab/lookahead_pruning target=_blank rel=noopener>Code</a>]</li><li>2020-ICLR-<a class=link href="https://openreview.net/pdf?id=SJem8lSFwB" target=_blank rel=noopener>Dynamic Model Pruning with Feedback</a></li><li>2020-ICLR-<a class=link href="https://openreview.net/forum?id=BJxkOlSYDH" target=_blank rel=noopener>Provable Filter Pruning for Efficient Neural Networks</a></li><li>2020-ICLR-<a class=link href="https://openreview.net/forum?id=H1gmHaEKwB" target=_blank rel=noopener>Data-Independent Neural Pruning via Coresets</a></li><li>2020-ICLR-<a class=link href="https://openreview.net/forum?id=S1xtORNFwH" target=_blank rel=noopener>FSNet: Compression of Deep Convolutional Neural Networks by Filter Summary</a></li><li>2020-ICLR-<a class=link href="https://openreview.net/forum?id=HJgCF0VFwr" target=_blank rel=noopener>Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks</a></li><li>2020-ICLR-<a class=link href="https://openreview.net/forum?id=HyxjOyrKvr" target=_blank rel=noopener>Neural Epitome Search for Architecture-Agnostic Network Compression</a></li><li>2020-ICLR-<a class=link href="https://openreview.net/forum?id=r1e9GCNKvH" target=_blank rel=noopener>One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation</a></li><li>2020-ICLR-<a class=link href="https://openreview.net/forum?id=rylBK34FDS" target=_blank rel=noopener>DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures</a> [<a class=link href=https://github.com/yanghr/DeepHoyer target=_blank rel=noopener>Code</a>]</li><li>2020-ICLR-<a class=link href="https://openreview.net/forum?id=SJlbGJrtDB" target=_blank rel=noopener>Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers</a></li><li>2020-ICLR-<a class=link href="https://openreview.net/forum?id=HkgxW0EYDS" target=_blank rel=noopener>Scalable Model Compression by Entropy Penalized Reparameterization</a></li><li>2020-ICLR-<a class=link href=https://arxiv.org/abs/1906.06307 target=_blank rel=noopener>A Signal Propagation Perspective for Pruning Neural Networks at Initialization</a></li><li>2020-CVPR-<a class=link href=https://arxiv.org/abs/1911.11907 target=_blank rel=noopener>GhostNet: More Features from Cheap Operations</a> [<a class=link href=https://github.com/huawei-noah/ghostnet target=_blank rel=noopener>Code</a>]</li><li>2020-CVPR-<a class=link href=https://arxiv.org/pdf/2001.05868.pdf target=_blank rel=noopener>Filter Grafting for Deep Neural Networks</a></li><li>2020-CVPR-<a class=link href=http://graduatestudent.ucmerced.edu/yidelbayev/papers/cvpr20/cvpr20a.pdf target=_blank rel=noopener>Low-rank Compression of Neural Nets: Learning the Rank of Each Layer</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Kwon_Structured_Compression_by_Weight_Encryption_for_Unstructured_Pruning_and_Quantization_CVPR_2020_paper.pdf target=_blank rel=noopener>Structured Compression by Weight Encryption for Unstructured Pruning and Quantization</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Learning_Filter_Pruning_Criteria_for_Deep_Convolutional_Neural_Networks_Acceleration_CVPR_2020_paper.pdf target=_blank rel=noopener>Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_APQ_Joint_Search_for_Network_Architecture_Pruning_and_Quantization_Policy_CVPR_2020_paper.pdf target=_blank rel=noopener>APQ: Joint Search for Network Architecture, Pruning and Quantization Policy</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Group_Sparsity_The_Hinge_Between_Filter_Pruning_and_Decomposition_for_CVPR_2020_paper.pdf target=_blank rel=noopener>Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression</a> [<a class=link href=https://github.com/ofsoundof/group_sparsity target=_blank rel=noopener>Code</a>]</li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_Neural_Network_Pruning_With_Residual-Connections_and_Limited-Data_CVPR_2020_paper.pdf target=_blank rel=noopener>Neural Network Pruning With Residual-Connections and Limited-Data</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Multi-Dimensional_Pruning_A_Unified_Framework_for_Model_Compression_CVPR_2020_paper.pdf target=_blank rel=noopener>Multi-Dimensional Pruning: A Unified Framework for Model Compression</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_Discrete_Model_Compression_With_Resource_Constraint_for_Deep_Neural_Networks_CVPR_2020_paper.pdf target=_blank rel=noopener>Discrete Model Compression With Resource Constraint for Deep Neural Networks</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Automatic_Neural_Network_Compression_by_Sparsity-Quantization_Joint_Learning_A_Constrained_CVPR_2020_paper.pdf target=_blank rel=noopener>Automatic Neural Network Compression by Sparsity-Quantization Joint Learning: A Constrained Optimization-Based Approach</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Idelbayev_Low-Rank_Compression_of_Neural_Nets_Learning_the_Rank_of_Each_CVPR_2020_paper.pdf target=_blank rel=noopener>Low-Rank Compression of Neural Nets: Learning the Rank of Each Layer</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Haroush_The_Knowledge_Within_Methods_for_Data-Free_Model_Compression_CVPR_2020_paper.pdf target=_blank rel=noopener>The Knowledge Within: Methods for Data-Free Model Compression</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_GAN_Compression_Efficient_Architectures_for_Interactive_Conditional_GANs_CVPR_2020_paper.pdf target=_blank rel=noopener>GAN Compression: Efficient Architectures for Interactive Conditional GANs</a> [<a class=link href=https://github.com/mit-han-lab/gan-compression target=_blank rel=noopener>Code</a>]</li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Few_Sample_Knowledge_Distillation_for_Efficient_Network_Compression_CVPR_2020_paper.pdf target=_blank rel=noopener>Few Sample Knowledge Distillation for Efficient Network Compression</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/html/Elsen_Fast_Sparse_ConvNets_CVPR_2020_paper.html target=_blank rel=noopener>Fast sparse convnets</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Eban_Structured_Multi-Hashing_for_Model_Compression_CVPR_2020_paper.pdf target=_blank rel=noopener>Structured Multi-Hashing for Model Compression</a></li><li>2020-CVPRo-<a class=link href=https://arxiv.org/abs/1912.13200 target=_blank rel=noopener>AdderNet: Do We Really Need Multiplications in Deep Learning?</a> [<a class=link href=https://github.com/huawei-noah/AdderNet target=_blank rel=noopener>Code</a>]</li><li>2020-CVPRo-<a class=link href=https://arxiv.org/abs/1904.12368 target=_blank rel=noopener>Towards Efficient Model Compression via Learned Global Ranking</a> [<a class=link href=https://github.com/cmu-enyac/LeGR target=_blank rel=noopener>Code</a>]</li><li>2020-CVPRo-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_HRank_Filter_Pruning_Using_High-Rank_Feature_Map_CVPR_2020_paper.pdf target=_blank rel=noopener>HRank: Filter Pruning Using High-Rank Feature Map</a> [<a class=link href=https://github.com/lmbxmu/HRank target=_blank rel=noopener>Code</a>]</li><li>2020-CVPRo-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_DaST_Data-Free_Substitute_Training_for_Adversarial_Attacks_CVPR_2020_paper.html target=_blank rel=noopener>DaST: Data-free Substitute Training for Adversarial Attacks</a> [<a class=link href=https://github.com/zhoumingyi/DaST target=_blank rel=noopener>Code</a>]</li><li>2020-ICML-<a class=link href=https://arxiv.org/abs/2005.07133 target=_blank rel=noopener>PENNI: Pruned Kernel Sharing for Efficient CNN Inference</a> [<a class=link href=https://github.com/timlee0212/PENNI target=_blank rel=noopener>Code</a>]</li><li>2020-ICML-<a class=link href=https://arxiv.org/abs/2007.03938 target=_blank rel=noopener>Operation-Aware Soft Channel Pruning using Differentiable Masks</a></li><li>2020-ICML-<a class=link href=https://proceedings.icml.cc/static/paper_files/icml/2020/2026-Paper.pdf target=_blank rel=noopener>DropNet: Reducing Neural Network Complexity via Iterative Pruning</a></li><li>2020-ICML-<a class=link href=https://arxiv.org/abs/2003.01794 target=_blank rel=noopener>Network Pruning by Greedy Subnetwork Selection</a></li><li>2020-ICML-<a class=link href=https://arxiv.org/abs/2006.08198 target=_blank rel=noopener>AutoGAN-Distiller: Searching to Compress Generative Adversarial Networks</a></li><li>2020-ICML-<a class=link href=https://arxiv.org/abs/2002.03231 target=_blank rel=noopener>Soft Threshold Weight Reparameterization for Learnable Sparsity</a> [<a class=link href=https://github.com/RAIVNLab/STR target=_blank rel=noopener>PyTorch Code</a>]</li><li>2020-ICML-<a class=link href=http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf target=_blank rel=noopener>Activation sparsity: Inducing and exploiting activation sparsity for fast inference on deep neural networks</a></li><li>2020-EMNLP-<a class=link href=https://arxiv.org/abs/1910.04732 target=_blank rel=noopener>Structured Pruning of Large Language Models</a> [<a class=link href=https://github.com/asappresearch/flop target=_blank rel=noopener>Code</a>]</li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/46a4378f835dc8040c8057beb6a2da52-Abstract.html target=_blank rel=noopener>Pruning neural networks without any data by iteratively conserving synaptic flow</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/703957b6dd9e3a7980e040bee50ded65-Abstract.html target=_blank rel=noopener>Neuron-level Structured Pruning using Polarization Regularizer</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/7bcdf75ad237b8e02e301f4091fb6bc8-Abstract.html target=_blank rel=noopener>SCOP: Scientific Control for Reliable Neural Network Pruning</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/a09e75c5c86a7bf6582d2b4d75aad615-Abstract.html target=_blank rel=noopener>Directional Pruning of Deep Neural Networks</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/a914ecef9c12ffdb9bede64bb703d877-Abstract.html target=_blank rel=noopener>Storage Efficient and Dynamic Flexible Runtime Channel Pruning via Deep Reinforcement Learning</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/ccb1d45fb76f7c5a0bf619f979c6cf36-Abstract.html target=_blank rel=noopener>Pruning Filter in Filter</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/e3a72c791a69f87b05ea7742e04430ed-Abstract.html target=_blank rel=noopener>HYDRA: Pruning Adversarially Robust Neural Networks</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/eae15aabaa768ae4a5993a8a4f4fa6e4-Abstract.html target=_blank rel=noopener>Movement Pruning: Adaptive Sparsity by Fine-Tuning</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/eae27d77ca20db309e056e3d2dcd7d69-Abstract.html target=_blank rel=noopener>Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/eb1e78328c46506b46a4ac4a1e378b91-Abstract.html target=_blank rel=noopener>Position-based Scaled Gradient for Model Quantization and Pruning</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/ef2ee09ea9551de88bc11fd7eeea93b0-Abstract.html target=_blank rel=noopener>The Generalization-Stability Tradeoff In Neural Network Pruning</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/0e230b1a582d76526b7ad7fc62ae937d-Abstract.html target=_blank rel=noopener>FleXOR: Trainable Fractional Quantization</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/20b5e1cf8694af7a3c1ba4a87f073021-Abstract.html target=_blank rel=noopener>Adaptive Gradient Quantization for Data-Parallel SGD</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/3948ead63a9f2944218de038d8934305-Abstract.html target=_blank rel=noopener>Robust Quantization: One Model to Rule Them All</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/d77c703536718b95308130ff2e5cf9ee-Abstract.html target=_blank rel=noopener>HAWQ-V2: Hessian Aware trace-Weighted Quantization of Neural Networks</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/1385974ed5904a438616ff7bdb3f7439-Abstract.html target=_blank rel=noopener>Efficient Exact Verification of Binarized Neural Networks</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/13b919438259814cd5be8cb45877d577-Abstract.html target=_blank rel=noopener>Ultra-Low Precision 4-bit Training of Deep Neural Networks</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/96fca94df72984fc97ee5095410d4dec-Abstract.html target=_blank rel=noopener>Path Sample-Analytic Gradient Estimators for Stochastic Binary Networks</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/2fd5d41ec6cfab47e32164d5624269b1-Abstract.html target=_blank rel=noopener>Fast fourier convolution</a></li></ul><p><strong>2021</strong></p><ul><li>2021-WACV-<a class=link href=https://openaccess.thecvf.com/content/WACV2021/papers/He_CAP_Context-Aware_Pruning_for_Semantic_Segmentation_WACV_2021_paper.pdf target=_blank rel=noopener>CAP: Context-Aware Pruning for Semantic Segmentation</a> [<a class=link href=https://github.com/erichhhhho/CAP-Context-Aware-Pruning-for-Semantic-Segmentation target=_blank rel=noopener>Code</a>]</li><li>2021-AAAI-<a class=link href=https://arxiv.org/abs/1911.09450 target=_blank rel=noopener>Few Shot Network Compression via Cross Distillation</a></li><li>2021-AAAI-<a class=link href target=_blank rel=noopener>Conditional Channel Pruning for Automated Model Compression</a> [<a class=link href=https://github.com/liuyixin-louis/CAMC-hanlab target=_blank rel=noopener>Code</a>]</li><li>2021-ICLR-<a class=link href="https://openreview.net/forum?id=o966_Is_nPA" target=_blank rel=noopener>Neural Pruning via Growing Regularization</a> [<a class=link href=https://github.com/MingSun-Tse/Regularization-Pruning target=_blank rel=noopener>PyTorch Code</a>]</li><li>2021-ICLR-<a class=link href="https://openreview.net/forum?id=Cb54AMqHQFP" target=_blank rel=noopener>Network Pruning That Matters: A Case Study on Retraining Variants</a></li><li>2021-ICLR-<a class=link href="https://openreview.net/forum?id=xCxXwTzx4L1" target=_blank rel=noopener>ChipNet: Budget-Aware Pruning with Heaviside Continuous Approximations</a></li><li>2021-ICLR-<a class=link href="https://openreview.net/forum?id=rumv7QmLUue" target=_blank rel=noopener>A Gradient Flow Framework For Analyzing Network Pruning</a> (Spotlight)</li><li>2021-CVPR-<a class=link href=https://arxiv.org/abs/2105.11228 target=_blank rel=noopener>Towards Compact CNNs via Collaborative Compression</a></li><li>2021-CVPR-<a class=link href=https://arxiv.org/abs/2103.05861 target=_blank rel=noopener>Manifold Regularized Dynamic Network Pruning</a></li><li>2021-CVPR-<a class=link href=https://arxiv.org/abs/2103.07156 target=_blank rel=noopener>Learnable Companding Quantization for Accurate Low-bit Neural Networks</a></li><li>2021-CVPR-<a class=link href=https://arxiv.org/abs/2103.01049 target=_blank rel=noopener>Diversifying Sample Generation for Accurate Data-Free Quantization</a></li><li>2021-CVPR-<a class=link href=https://arxiv.org/abs/2103.15263 target=_blank rel=noopener>Zero-shot Adversarial Quantization</a> [Oral] [<a class=link href=https://github.com/FLHonker/ZAQ-code target=_blank rel=noopener>Code</a>]</li><li>2021-CVPR-<a class=link href=https://arxiv.org/abs/2104.00903 target=_blank rel=noopener>Network Quantization with Element-wise Gradient Scaling</a> [<a class=link href=https://cvlab.yonsei.ac.kr/projects/EWGS/ target=_blank rel=noopener>Project</a>]</li><li>2021-ICML-<a class=link href=http://proceedings.mlr.press/v139/liu21ab.html target=_blank rel=noopener>Group Fisher Pruning for Practical Network Compression</a> [<a class=link href=https://github.com/jshilong/FisherPruning target=_blank rel=noopener>Code</a>]</li><li>2021-ICML-<a class=link href=http://proceedings.mlr.press/v139/wang21e.html target=_blank rel=noopener>Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework</a></li><li>2021-ICML-<a class=link href=https://arxiv.org/abs/2105.10065 target=_blank rel=noopener>A Probabilistic Approach to Neural Network Pruning</a></li><li>2021-ICML-<a class=link href=http://proceedings.mlr.press/v139/rosenfeld21a.html target=_blank rel=noopener>On the Predictability of Pruning Across Scales</a></li><li>2021-ICML-<a class=link href=http://proceedings.mlr.press/v139/verma21b.html target=_blank rel=noopener>Sparsifying Networks via Subdifferential Inclusion</a></li><li>2021-ICML-<a class=link href=https://arxiv.org/abs/2101.09048 target=_blank rel=noopener>Selfish Sparse RNN Training</a> [<a class=link href=https://github.com/Shiweiliuiiiiiii/Selfish-RNN target=_blank rel=noopener>Code</a>]</li><li>2021-ICML-<a class=link href=https://arxiv.org/abs/2102.02887 target=_blank rel=noopener>Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training</a> [<a class=link href=https://github.com/Shiweiliuiiiiiii/In-Time-Over-Parameterization target=_blank rel=noopener>Code</a>]</li><li>2021-ICML-<a class=link href=http://proceedings.mlr.press/v139/ozdenizci21a.html target=_blank rel=noopener>Training Adversarially Robust Sparse Networks via Bayesian Connectivity Sampling</a></li><li>2021-ICML-<a class=link href=https://arxiv.org/abs/2104.14129 target=_blank rel=noopener>ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training</a></li><li>2021-ICML-<a class=link href=https://arxiv.org/abs/2105.04857 target=_blank rel=noopener>Leveraging Sparse Linear Layers for Debuggable Deep Networks</a></li><li>2021-ICML-<a class=link href=http://proceedings.mlr.press/v139/patil21a.html target=_blank rel=noopener>PHEW: Constructing Sparse Networks that Learn Fast and Generalize Well without Training Data</a></li><li>2021-ICML-<a class=link href=https://arxiv.org/abs/2103.16716 target=_blank rel=noopener>BASE Layers: Simplifying Training of Large, Sparse Models</a> [<a class=link href=https://github.com/pytorch/fairseq/ target=_blank rel=noopener>Code</a>]</li><li>2021-ICML-<a class=link href=https://arxiv.org/abs/2102.07655 target=_blank rel=noopener>Dense for the Price of Sparse: Improved Performance of Sparsely Initialized Networks via a Subspace Offset</a></li><li>2021-ICML-<a class=link href=https://arxiv.org/abs/2101.01321 target=_blank rel=noopener>I-BERT: Integer-only BERT Quantization</a></li><li>2021-ICML-<a class=link href=https://arxiv.org/abs/2105.01420 target=_blank rel=noopener>Training Quantized Neural Networks to Global Optimality via Semidefinite Programming</a></li><li>2021-ICML-<a class=link href=https://arxiv.org/abs/2106.02295 target=_blank rel=noopener>Differentiable Dynamic Quantization with Mixed Precision and Adaptive Resolution</a></li><li>2021-ICML-<a class=link href=https://arxiv.org/abs/2102.07214 target=_blank rel=noopener>Communication-Efficient Distributed Optimization with Quantized Preconditioners</a></li><li>2021-NIPS-<a class=link href=https://papers.nips.cc/paper/2021/file/15de21c670ae7c3f6f3f1f37029303c9-Paper.pdf target=_blank rel=noopener>Aligned Structured Sparsity Learning for Efficient Image Super-Resolution</a> [<a class=link href=https://github.com/MingSun-Tse/ASSL target=_blank rel=noopener>Code</a>] (Spotlight!)</li><li>2021-NIPS-<a class=link href=https://arxiv.org/abs/2110.15343 target=_blank rel=noopener>Scatterbrain: Unifying Sparse and Low-rank Attention</a> [<a class=link href=https://github.com/HazyResearch/scatterbrain target=_blank rel=noopener>Code</a>]</li><li>2021-NIPS-<a class=link href=https://proceedings.neurips.cc/paper/2021/file/a376033f78e144f494bfc743c0be3330-Paper.pdf target=_blank rel=noopener>Only Train Once: A One-Shot Neural Network Training And Pruning Framework</a> [<a class=link href=https://github.com/tianyic/only_train_once target=_blank rel=noopener>Code</a>]</li><li>2021.5-<a class=link href=https://arxiv.org/abs/2105.05916 target=_blank rel=noopener>Dynamical Isometry: The Missing Ingredient for Neural Network Pruning</a></li></ul><h4 id=2022>2022</h4><ul><li>2022-ICLR-<a class=link href=https://arxiv.org/abs/2112.00029 target=_blank rel=noopener>Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=11nMVZK0WYM" target=_blank rel=noopener>Pruning has a disparate impact on model accuracy</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=btpIaJiRx6z" target=_blank rel=noopener>Pruning Neural Networks via Coresets and Convex Geometry: Towards No Assumptions</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=ksVGCOlOEba" target=_blank rel=noopener>Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=K2QGzyLwpYG" target=_blank rel=noopener>Data-Efficient Structured Pruning via Submodular Optimization</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=0GRBKLBjJE" target=_blank rel=noopener>A Fast Post-Training Pruning Framework for Transformers</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=w5DacXWzQ-Q" target=_blank rel=noopener>SAViT: Structure-Aware Vision Transformer Pruning via Collaborative Optimization</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=5hgYi4r5MDp" target=_blank rel=noopener>Recall Distortion in Neural Network Pruning and the Undecayed Pruning Algorithm</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=cUOR-_VsavA" target=_blank rel=noopener>Structural Pruning via Latency-Saliency Knapsack</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=KieCChVB6mN" target=_blank rel=noopener>Sparse Probabilistic Circuits via Pruning and Growing</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=mTXQIpXPDbh" target=_blank rel=noopener>Back Razor: Memory-Efficient Transfer Learning by Self-Sparsified Backpropogation</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=oQIJsMlyaW_" target=_blank rel=noopener>SInGE: Sparsity via Integrated Gradients Estimation of Neuron Relevance</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=HuiLIB6EaOk" target=_blank rel=noopener>VTC-LFC: Vision Transformer Compression with Low-Frequency Components</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=UQJoGBNRX4" target=_blank rel=noopener>Weighted Mutual Learning with Diversity-Driven Model Compression</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=wfel7CjOYk" target=_blank rel=noopener>Resource-Adaptive Federated Learning with All-In-One Neural Composition</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=XUvSYc6TqDF" target=_blank rel=noopener>Controlled Sparsity via Constrained Optimization or: How I Learned to Stop Tuning Penalties and Love Constraints</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=l2CVt1ySC2Q" target=_blank rel=noopener>On Measuring Excess Capacity in Neural Networks</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=2OpRgzLhoPQ" target=_blank rel=noopener>Prune and distill: similar reformatting of image information along rat visual cortex and deep neural networks</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=EZQnauHn-77" target=_blank rel=noopener>Deep Compression of Pre-trained Transformer Models</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=HZ20IYYAwah" target=_blank rel=noopener>Sparsity in Continuous-Depth Neural Networks</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=u4KagP_FjB" target=_blank rel=noopener>Spartan: Differentiable Sparsity via Regularized Transportation</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=0Z0xltoU1q" target=_blank rel=noopener>Accelerated Projected Gradient Algorithms for Sparsity Constrained Optimization Problems</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=kK200QKfvjB" target=_blank rel=noopener>Feature Learning in L2-regularized DNNs: Attraction/Repulsion and Sparsity</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=tbdk6XLYmZj" target=_blank rel=noopener>Learning Best Combination for Efficient N:M Sparsity</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=Q5kXC6hCr1" target=_blank rel=noopener>Accelerating Sparse Convolution with Column Vector-Wise Sparsity</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=68EuccCtO5i" target=_blank rel=noopener>Differentially Private Model Compression</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=w_jvWzNXd6n" target=_blank rel=noopener>Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=UmaiVbwN1v" target=_blank rel=noopener>A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=88_wNI6ZBDZ" target=_blank rel=noopener>Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation Approach</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=dZEZu7zxJBF" target=_blank rel=noopener>Learning sparse features can lead to overfitting in neural networks</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=edgCBcwZxgd" target=_blank rel=noopener>Deep Architecture Connectivity Matters for Its Convergence: A Fine-Grained Analysis</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=NXHXoYMLIG" target=_blank rel=noopener>EfficientFormer: Vision Transformers at MobileNet Speed</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=INzRLBAA4JX" target=_blank rel=noopener>Revisiting Sparse Convolutional Model for Visual Recognition</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=AUz5Oig77OS" target=_blank rel=noopener>Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=AODVskSug8" target=_blank rel=noopener>A Theoretical View on Sparsely Activated Networks</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=ZxOO5jfqSYw" target=_blank rel=noopener>Dynamic Sparse Network for Time Series Classification: Learning What to “See”</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=QqWqFLbllZh" target=_blank rel=noopener>Spatial Pruned Sparse Convolution for Efficient 3D Object Detection</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=oOte_397Q4P" target=_blank rel=noopener>Sparse Structure Search for Delta Tuning</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=n0dD3d54Wgf" target=_blank rel=noopener>Beyond L1: Faster and Better Sparse Models with skglm</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=mWaYC6CZf5" target=_blank rel=noopener>On the Representation Collapse of Sparse Mixture of Experts</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=cFOhdl1cyU-" target=_blank rel=noopener>M³ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=zGvRdBW06F5" target=_blank rel=noopener>On-Device Training Under 256KB Memory</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=rBCvMG-JsPd" target=_blank rel=noopener>Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</a></li></ul><h4 id=2023>2023</h4><ul><li>2023-ICLR-<a class=link href=https://arxiv.org/abs/2207.12534 target=_blank rel=noopener>Trainability Preserving Neural Pruning</a> [<a class=link href=https://github.com/MingSun-Tse/TPP target=_blank rel=noopener>Code</a>]</li><li>2023-ICLR-<a class=link href="https://openreview.net/forum?id=-5EWhW_4qWP" target=_blank rel=noopener>NTK-SAP: Improving neural network pruning by aligning training dynamics</a> [<a class=link href=https://github.com/YiteWang/NTK-SAP target=_blank rel=noopener>Code</a>]</li><li>2023-CVPR-<a class=link href=https://arxiv.org/abs/2301.12900 target=_blank rel=noopener>DepGraph: Towards Any Structural Pruning</a>[<a class=link href=https://github.com/VainF/Torch-Pruning target=_blank rel=noopener>code</a>]</li><li>2023-CVPR-<a class=link href=https://arxiv.org/abs/2303.13097 target=_blank rel=noopener>CP3: Channel Pruning Plug-in for Point-based Networks</a></li></ul><hr><h3 id=papers-actual-acceleration-via-sparsity>Papers [Actual Acceleration via Sparsity]</h3><ul><li>2018-ICML-<a class=link href=https://arxiv.org/abs/1802.08435 target=_blank rel=noopener>Efficient Neural Audio Synthesis</a></li><li>2018-NIPS-<a class=link href=https://papers.nips.cc/paper/2018/hash/89885ff2c83a10305ee08bd507c1049c-Abstract.html target=_blank rel=noopener>Tetris: Tile-matching the tremendous irregular sparsity</a></li><li>2021.4-<a class=link href=https://arxiv.org/abs/2104.08378 target=_blank rel=noopener>Accelerating Sparse Deep Neural Networks</a> (White paper from NVIDIA)</li><li>2021-ICLR-<a class=link href="https://openreview.net/forum?id=K9bw7vqp_s" target=_blank rel=noopener>Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch</a> [<a class=link href=https://github.com/NM-sparsity/NM-sparsity target=_blank rel=noopener>Code</a>]</li><li>2021-NIPS-<a class=link href=https://proceedings.neurips.cc/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html target=_blank rel=noopener>Channel Permutations for N: M Sparsity</a> [<a class=link href=https://github.com/NVIDIA/apex/tree/master/apex/contrib/sparsity target=_blank rel=noopener>Code: NVIDIA ASP</a>]</li><li>2021-NIPS-<a class=link href="https://openreview.net/forum?id=vRWZsBLKqA" target=_blank rel=noopener>Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N:M Transposable Masks</a></li><li>2021-ICLR-<a class=link href=https://arxiv.org/abs/2102.04010 target=_blank rel=noopener>Learning N:M fine-grained structured sparse neural networks from scratch</a> [<a class=link href=https://github.com/NM-sparsity/NM-sparsity target=_blank rel=noopener>Code</a>] [<a class=link href=https://iclr.cc/media/iclr-2021/Slides/3174.pdf target=_blank rel=noopener>Slides</a>]</li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=ZJe-XahpyBf" target=_blank rel=noopener>UDC: Unified DNAS for Compressible TinyML Models for Neural Processing Units</a></li></ul><hr><h3 id=papers-lottery-ticket-hypothesis-lth>Papers [Lottery Ticket Hypothesis (LTH)]</h3><p>For LTH and other <em>Pruning at Initialization</em> papers, please refer to <a class=link href=https://github.com/MingSun-Tse/Awesome-Pruning-at-Initialization target=_blank rel=noopener>Awesome-Pruning-at-Initialization</a>.</p><hr><h3 id=papers-bayesian-compression>Papers [Bayesian Compression]</h3><ul><li>1995-Neural Computation-<a class=link href=https://www.researchgate.net/profile/Peter_Williams19/publication/2719575_Bayesian_Regularisation_and_Pruning_using_a_Laplace_Prior/links/58fde123aca2728fa70f6aab/Bayesian-Regularisation-and-Pruning-using-a-Laplace-Prior.pdf target=_blank rel=noopener>Bayesian Regularisation and Pruning using a Laplace Prior</a></li><li>1997-Neural Networks-<a class=link href="https://www.sciencedirect.com/science/article/pii/S0893608097000270?casa_token=sLb4dFBnyH8AAAAA:a9WwAAoYl5CgLepZGXjZ5DKQ4YBEjINgGd7Jl2bPHqrbhIWZHso-uC_gpL-85JmdxG7g8x71" target=_blank rel=noopener>Regularization with a Pruning Prior</a></li><li>2015-NIPS-<a class=link href=http://papers.nips.cc/paper/5965-bayesian-dark-knowledge.pdf target=_blank rel=noopener>Bayesian dark knowledge</a></li><li>2017-NIPS-<a class=link href=http://papers.nips.cc/paper/6921-bayesian-compression-for-deep-learning target=_blank rel=noopener>Bayesian Compression for Deep Learning</a> [<a class=link href=https://github.com/KarenUllrich/Tutorial_BayesianCompressionForDL target=_blank rel=noopener>Code</a>]</li><li>2017-ICML-<a class=link href=https://arxiv.org/pdf/1701.05369.pdf target=_blank rel=noopener>Variational dropout sparsifies deep neural networks</a></li><li>2017-NIPSo-<a class=link href=http://papers.nips.cc/paper/7254-structured-bayesian-pruning-via-log-normal-multiplicative-noise target=_blank rel=noopener>Structured Bayesian Pruning via Log-Normal Multiplicative Noise</a></li><li>2017-ICMLw-<a class=link href=https://arxiv.org/abs/1708.00077 target=_blank rel=noopener>Bayesian Sparsification of Recurrent Neural Networks</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/3f13cf4ddf6fc50c0d39a1d5aeb57dd8-Abstract.html target=_blank rel=noopener>Bayesian Bits: Unifying Quantization and Pruning</a></li></ul><h2 id=papers-knowledge-distillation-kd>Papers [Knowledge Distillation (KD)]</h2><p><strong>Before 2014</strong></p><ul><li>1996-<a class=link href=ftp://ftp.stat.berkeley.edu/pub/users/breiman/BAtrees.ps target=_blank rel=noopener>Born again trees</a> (proposed compressing neural networks and multipletree predictors by approximating them with a single tree)</li><li>2006-SIGKDD-<a class=link href="https://dl.acm.org/citation.cfm?id=1150464" target=_blank rel=noopener>Model compression</a></li><li>2010-ML-<a class=link href=https://link.springer.com/content/pdf/10.1007%2Fs10994-009-5152-4.pdf target=_blank rel=noopener>A theory of learning from different domains</a></li></ul><p><strong>2014</strong></p><ul><li>2014-NIPS-<a class=link href=https://arxiv.org/abs/1312.6184 target=_blank rel=noopener>Do deep nets really need to be deep?</a></li><li>2014-NIPSw-<a class=link href=https://arxiv.org/pdf/1503.02531.pdf target=_blank rel=noopener>Distilling the Knowledge in a Neural Network</a> [<a class=link href=https://github.com/peterliht/knowledge-distillation-pytorch target=_blank rel=noopener>Code</a>]</li></ul><p><strong>2016</strong></p><ul><li>2016-ICLR-<a class=link href=https://arxiv.org/abs/1511.05641 target=_blank rel=noopener>Net2net: Accelerating learning via knowledge transfer</a></li><li>2016-ECCV-<a class=link href=https://www.researchgate.net/publication/308277663_Accelerating_Convolutional_Neural_Networks_with_Dominant_Convolutional_Kernel_and_Knowledge_Pre-regression target=_blank rel=noopener>Accelerating convolutional neural networks with dominant convolutional kernel and knowledge pre-regression</a></li></ul><p><strong>2017</strong></p><ul><li>2017-ICLR-<a class=link href=http://arxiv.org/abs/1612.03928 target=_blank rel=noopener>Paying more attention to attention: Improving the performance of convolutional neural networksvia attention transfer</a></li><li>2017-ICLR-<a class=link href=https://arxiv.org/pdf/1603.05691.pdf target=_blank rel=noopener>Do deep convolutional nets really need to be deep and convolutional?</a></li><li>2017-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf target=_blank rel=noopener>A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</a></li><li>2017-BMVC-<a class=link href=https://arxiv.org/abs/1604.00433 target=_blank rel=noopener>Adapting models to signal degradation using distillation</a></li><li>2017-NIPS-<a class=link href=http://papers.nips.cc/paper/7015-sobolev-training-for-neural-networks.pdf target=_blank rel=noopener>Sobolev training for neural networks</a></li><li>2017-NIPS-<a class=link href=http://papers.nips.cc/paper/6676-learning-efficient-object-detection-models-with-knowledge-distillation target=_blank rel=noopener>Learning efficient object detection models with knowledge distillation</a></li><li>2017-NIPSw-<a class=link href=https://arxiv.org/abs/1710.07535 target=_blank rel=noopener>Data-Free Knowledge Distillation for Deep Neural Networks</a> [<a class=link href=https://github.com/iRapha/replayed_distillation target=_blank rel=noopener>Code</a>]</li><li>2017.07-<a class=link href=https://arxiv.org/pdf/1707.01219.pdf target=_blank rel=noopener>Like What You Like: Knowledge Distill via Neuron Selectivity Transfer</a></li><li>2017.10-<a class=link href=https://arxiv.org/abs/1710.09505 target=_blank rel=noopener>Knowledge Projection for Deep Neural Networks</a></li><li>2017.11-<a class=link href=https://arxiv.org/abs/1711.09784 target=_blank rel=noopener>Distilling a Neural Network Into a Soft Decision Tree</a></li><li>2017.12-<a class=link href=https://arxiv.org/abs/1712.04440 target=_blank rel=noopener>Data Distillation: Towards Omni-Supervised Learning</a></li></ul><p><strong>2018</strong></p><ul><li>2018-AAAI-<a class=link href=https://arxiv.org/abs/1707.01220 target=_blank rel=noopener>DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer</a></li><li>2018-AAAI-<a class=link href=https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16291/16575 target=_blank rel=noopener>Dynamic deep neural networks: Optimizing accuracy-efficiency trade-offs by selective execution</a></li><li>2018-AAAI-<a class=link href=https://arxiv.org/abs/1708.04106 target=_blank rel=noopener>Rocket Launching: A Universal and Efficient Framework for Training Well-performing Light Net</a></li><li>2018-AAAI-<a class=link href=https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16509 target=_blank rel=noopener>Adversarial Learning of Portable Student Networks</a></li><li>2018-AAAI-<a class=link href=https://arxiv.org/abs/1805.05551 target=_blank rel=noopener>Knowledge Distillation in Generations: More Tolerant Teachers Educate Better Students</a></li><li>2018-ICLR-<a class=link href="https://openreview.net/forum?id=rkr1UDeC-" target=_blank rel=noopener>Large scale distributed neural network training through online distillation</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.html target=_blank rel=noopener>Deep mutual learning</a></li><li>2018-ICML-<a class=link href=https://arxiv.org/pdf/1805.04770.pdf target=_blank rel=noopener>Born-Again Neural Networks</a></li><li>2018-IJCAI-<a class=link href=https://arxiv.org/abs/1804.10069 target=_blank rel=noopener>Better and Faster: Knowledge Transfer from Multiple Self-supervised Learning Tasks via Graph Distillation for Video Classification</a></li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Nikolaos_Passalis_Learning_Deep_Representations_ECCV_2018_paper.html target=_blank rel=noopener>2018-ECCV-Learning deep representations with probabilistic knowledge transfer</a> [<a class=link href=https://github.com/passalis/probabilistic_kt target=_blank rel=noopener>Code</a>]</li><li>2018-ECCV-<a class=link href=http://openaccess.thecvf.com/content_ECCV_2018/html/Zhengming_Ding_Graph_Adaptive_Knowledge_ECCV_2018_paper.html target=_blank rel=noopener>Graph adaptive knowledge transfer for unsupervised domain adaptation</a></li><li>2018-SIGKDD-<a class=link href=https://www.researchgate.net/profile/Yunhe_Wang3/publication/326502551_Towards_Evolutionary_Compression/links/5b7e9304a6fdcc5f8b5e4fe5/Towards-Evolutionary-Compression.pdf target=_blank rel=noopener>Towards Evolutionary Compression</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7358-kdgan-knowledge-distillation-with-generative-adversarial-networks target=_blank rel=noopener>KDGAN: knowledge distillation with generative adversarial networks</a> [<a class=link href=https://ieeexplore.ieee.org/abstract/document/8845633 target=_blank rel=noopener>2019 TPAMI version</a>]</li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7980-knowledge-distillation-by-on-the-fly-native-ensemble target=_blank rel=noopener>Knowledge Distillation by On-the-Fly Native Ensemble</a></li><li>2018-NIPS-<a class=link href=http://papers.nips.cc/paper/7541-paraphrasing-complex-network-network-compression-via-factor-transfer target=_blank rel=noopener>Paraphrasing Complex Network: Network Compression via Factor Transfer</a></li><li>2018-NIPSw-<a class=link href=http://hushell.github.io/papers/nips18_cl.pdf target=_blank rel=noopener>Variational Mutual Information Distillation for Transfer Learning</a> <a class=link href=https://sites.google.com/view/continual2018/ target=_blank rel=noopener>workshop: continual learning</a></li><li>2018-NIPSw-<a class=link href=https://arxiv.org/pdf/1801.08640.pdf target=_blank rel=noopener>Transparent Model Distillation</a></li><li>2018.03-<a class=link href=https://arxiv.org/abs/1803.04042 target=_blank rel=noopener>Interpreting Deep Classifier by Visual Distillation of Dark Knowledge</a></li><li>2018.11-<a class=link href=https://arxiv.org/abs/1811.10959 target=_blank rel=noopener>Dataset Distillation</a> [<a class=link href=https://github.com/SsnL/dataset-distillation target=_blank rel=noopener>Code</a>]</li><li>2018.12-<a class=link href=https://arxiv.org/abs/1812.06597 target=_blank rel=noopener>Learning Student Networks via Feature Embedding</a></li><li>2018.12-<a class=link href=https://arxiv.org/abs/1812.01839 target=_blank rel=noopener>Few Sample Knowledge Distillation for Efficient Network Compression</a></li></ul><p><strong>2019</strong></p><ul><li>2019-AAAI-<a class=link href=https://arxiv.org/abs/1805.05532 target=_blank rel=noopener>Knowledge Distillation with Adversarial Samples Supporting Decision Boundary</a></li><li>2019-AAAI-<a class=link href=https://arxiv.org/abs/1811.03233 target=_blank rel=noopener>Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons</a> [<a class=link href=https://github.com/bhheo/AB_distillation target=_blank rel=noopener>Code</a>]</li><li>2019-AAAI-<a class=link href=https://arxiv.org/abs/1811.02759 target=_blank rel=noopener>Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks</a> [<a class=link href=https://github.com/cardwing/Codes-for-Steering-Control target=_blank rel=noopener>Code</a>]</li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPRW_2019/html/CEFRL/Liu_Knowledge_Representing_Efficient_Sparse_Representation_of_Prior_Knowledge_for_Knowledge_CVPRW_2019_paper.html target=_blank rel=noopener>Knowledge Representing: Efficient, Sparse Representation of Prior Knowledge for Knowledge Distillation</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Knowledge_Distillation_via_Instance_Relationship_Graph_CVPR_2019_paper.html target=_blank rel=noopener>Knowledge Distillation via Instance Relationship Graph</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/html/Ahn_Variational_Information_Distillation_for_Knowledge_Transfer_CVPR_2019_paper.html target=_blank rel=noopener>Variational Information Distillation for Knowledge Transfer</a></li><li>2019-CVPR-<a class=link href=http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Learning_Metrics_From_Teachers_Compact_Networks_for_Image_Embedding_CVPR_2019_paper.html target=_blank rel=noopener>Learning Metrics from Teachers Compact Networks for Image Embedding</a> [<a class=link href=https://github.com/yulu0724/EmbeddingDistillation target=_blank rel=noopener>Code</a>]</li><li>2019-ICCV-<a class=link href=http://openaccess.thecvf.com/content_ICCV_2019/html/Heo_A_Comprehensive_Overhaul_of_Feature_Distillation_ICCV_2019_paper.html target=_blank rel=noopener>A Comprehensive Overhaul of Feature Distillation</a></li><li>2019-ICCV-<a class=link href=https://arxiv.org/abs/1907.09682 target=_blank rel=noopener>Similarity-Preserving Knowledge Distillation</a></li><li>2019-ICCV-<a class=link href=https://arxiv.org/abs/1904.01802 target=_blank rel=noopener>Correlation Congruence for Knowledge Distillation</a></li><li>2019-ICCV-<a class=link href=https://arxiv.org/abs/1904.01186 target=_blank rel=noopener>Data-Free Learning of Student Networks</a></li><li>2019-ICCV-<a class=link href=https://arxiv.org/abs/1908.00821 target=_blank rel=noopener>Learning Lightweight Lane Detection CNNs by Self Attention Distillation</a> [<a class=link href=https://github.com/cardwing/Codes-for-Lane-Detection target=_blank rel=noopener>Code</a>]</li><li>2019-ICCV-<a class=link href=http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Attention_Bridging_Network_for_Knowledge_Transfer_ICCV_2019_paper.html target=_blank rel=noopener>Attention bridging network for knowledge transfer</a></li><li>2019-NIPS-<a class=link href=https://papers.nips.cc/paper/9151-zero-shot-knowledge-transfer-via-adversarial-belief-matching target=_blank rel=noopener>Zero-shot Knowledge Transfer via Adversarial Belief Matching</a> [<a class=link href=https://github.com/polo5/ZeroShotKnowledgeTransfer target=_blank rel=noopener>Code</a>] (spotlight)</li><li>2019.05-<a class=link href=https://arxiv.org/abs/1905.03465 target=_blank rel=noopener>DistillHash: Unsupervised Deep Hashing by Distilling Data Pairs</a></li></ul><p><strong>2020</strong></p><ul><li>2020-ICLR-<a class=link href=https://arxiv.org/abs/1910.10699 target=_blank rel=noopener>Contrastive Representation Distillation</a> [<a class=link href=https://github.com/HobbitLong/RepDistiller target=_blank rel=noopener>Code</a>]</li><li>2020-AAAI-<a class=link href target=_blank rel=noopener>A Knowledge Transfer Framework for Differentially Private Sparse Learning</a></li><li>2020-AAAI-<a class=link href target=_blank rel=noopener>Uncertainty-aware Multi-shot Knowledge Distillation for Image-based Object Re-identification</a></li><li>2020-AAAI-<a class=link href target=_blank rel=noopener>Improved Knowledge Distillation via Teacher Assistant</a></li><li>2020-AAAI-<a class=link href target=_blank rel=noopener>Knowledge Distillation from Internal Representations</a></li><li>2020-AAAI-<a class=link href target=_blank rel=noopener>Distilling Knowledge from Well-informed Soft Labels for Neural Relation Extraction</a></li><li>2020-AAAI-<a class=link href=https://arxiv.org/abs/1912.00350 target=_blank rel=noopener>Online Knowledge Distillation with Diverse Peers</a></li><li>2020-AAAI-<a class=link href=https://arxiv.org/abs/1904.04449 target=_blank rel=noopener>Ultrafast Video Attention Prediction with Coupled Knowledge Distillation</a></li><li>2020-AAAI-<a class=link href target=_blank rel=noopener>Graph Few-shot Learning via Knowledge Transfer</a></li><li>2020-AAAI-<a class=link href target=_blank rel=noopener>Diversity Transfer Network for Few-Shot Learning</a></li><li>2020-AAAI-<a class=link href=https://arxiv.org/abs/1911.09450 target=_blank rel=noopener>Few Shot Network Compression via Cross Distillation</a></li><li>2020-ICLR-<a class=link href="https://openreview.net/pdf?id=BJeS62EtwH" target=_blank rel=noopener>Knowledge Consistency between Neural Networks and Beyond</a></li><li>2020-ICLR-<a class=link href="https://openreview.net/pdf?id=SkgpBJrtvS" target=_blank rel=noopener>Contrastive Representation Distillation</a> [<a class=link href=http://github.com/HobbitLong/RepDistiller target=_blank rel=noopener>Code</a>]</li><li>2020-ICLR-<a class=link href="https://openreview.net/forum?id=SklkDkSFPB" target=_blank rel=noopener>BlockSwap: Fisher-guided Block Substitution for Network Compression on a Budget</a></li><li>2020-ICLR-<a class=link href="https://openreview.net/pdf?id=BygSP6Vtvr" target=_blank rel=noopener>Ensemble Distribution Distillation</a></li><li>2020-CVPR-<a class=link href=https://arxiv.org/abs/2003.08436 target=_blank rel=noopener>Collaborative Distillation for Ultra-Resolution Universal Style Transfer</a> [<a class=link href=https://github.com/MingSun-Tse/Collaborative-Distillation target=_blank rel=noopener>Code</a>]</li><li>2020-CVPR-<a class=link href=https://arxiv.org/abs/2003.03622 target=_blank rel=noopener>Explaining Knowledge Distillation by Quantifying the Knowledge</a></li><li>2020-CVPR-<a class=link href=https://arxiv.org/abs/1911.04252 target=_blank rel=noopener>Self-training with Noisy Student improves ImageNet classification</a> [<a class=link href=https://github.com/google-research/noisystudent target=_blank rel=noopener>Code</a>]</li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Neural_Networks_Are_More_Productive_Teachers_Than_Human_Raters_Active_CVPR_2020_paper.pdf target=_blank rel=noopener>Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation From a Blackbox Model</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Passalis_Heterogeneous_Knowledge_Distillation_Using_Information_Flow_Modeling_CVPR_2020_paper.pdf target=_blank rel=noopener>Heterogeneous Knowledge Distillation Using Information Flow Modeling</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Creating_Something_From_Nothing_Unsupervised_Knowledge_Distillation_for_Cross-Modal_Hashing_CVPR_2020_paper.pdf target=_blank rel=noopener>Creating Something From Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf target=_blank rel=noopener>Revisiting Knowledge Distillation via Label Smoothing Regularization</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Distilling_Knowledge_From_Graph_Convolutional_Networks_CVPR_2020_paper.pdf target=_blank rel=noopener>Distilling Knowledge From Graph Convolutional Networks</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_MineGAN_Effective_Knowledge_Transfer_From_GANs_to_Target_Domains_With_CVPR_2020_paper.pdf target=_blank rel=noopener>MineGAN: Effective Knowledge Transfer From GANs to Target Domains With Few Images</a> [<a class=link href=https://github.com/yaxingwang/MineGAN target=_blank rel=noopener>Code</a>]</li><li>2020-CVPRo-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.pdf target=_blank rel=noopener>Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion</a> [<a class=link href=https://github.com/NVlabs/DeepInversion target=_blank rel=noopener>Code</a>]</li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Online_Knowledge_Distillation_via_Collaborative_Learning_CVPR_2020_paper.pdf target=_blank rel=noopener>Online Knowledge Distillation via Collaborative Learning</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Ye_Distilling_Cross-Task_Knowledge_via_Relationship_Matching_CVPR_2020_paper.pdf target=_blank rel=noopener>Distilling Cross-Task Knowledge via Relationship Matching</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Ye_Data-Free_Knowledge_Amalgamation_via_Group-Stack_Dual-GAN_CVPR_2020_paper.pdf target=_blank rel=noopener>Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN</a></li><li>2020-CVPR-<a class=link href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Yun_Regularizing_Class-Wise_Predictions_via_Self-Knowledge_Distillation_CVPR_2020_paper.pdf target=_blank rel=noopener>Regularizing Class-Wise Predictions via Self-Knowledge Distillation</a></li><li>2020-ICML-<a class=link href=https://arxiv.org/abs/2002.01775 target=_blank rel=noopener>Feature-map-level Online Adversarial Knowledge Distillation</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/1731592aca5fb4d789c4119c65c10b4b-Abstract.html target=_blank rel=noopener>Self-Distillation as Instance-Specific Label Smoothing</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/18df51b97ccd68128e994804f3eccc87-Abstract.html target=_blank rel=noopener>Ensemble Distillation for Robust Model Fusion in Federated Learning</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/2288f691b58edecadcc9a8691762b4fd-Abstract.html target=_blank rel=noopener>Self-Distillation Amplifies Regularization in Hilbert Space</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html target=_blank rel=noopener>MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/657b96f0592803e25a4f07166fff289a-Abstract.html target=_blank rel=noopener>Residual Distillation: Towards Portable Deep Neural Networks without Shortcuts</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/912d2b1c7b2826caf99687388d2e8f7c-Abstract.html target=_blank rel=noopener>Kernel Based Progressive Distillation for Adder Neural Networks</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/91c77393975889bd08f301c9e13a44b7-Abstract.html target=_blank rel=noopener>Agree to Disagree: Adaptive Ensemble Knowledge Distillation in Gradient Space</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/a96b65a721e561e1e3de768ac819ffbb-Abstract.html target=_blank rel=noopener>Task-Oriented Feature Distillation</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/c3535febaff29fcb7c0d20cbe94391c7-Abstract.html target=_blank rel=noopener>Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/fef6f971605336724b5e6c0c12dc2534-Abstract.html target=_blank rel=noopener>Distributed Distillation for On-Device Learning</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/ef0d3930a7b6c95bd2b32ed45989c61f-Abstract.html target=_blank rel=noopener>Knowledge Distillation in Wide Neural Networks: Risk Bound, Data Efficiency and Imperfect Teacher</a></li><li>2020.12-<a class=link href=https://arxiv.org/abs/2012.02909 target=_blank rel=noopener>Knowledge Distillation Thrives on Data Augmentation</a></li><li>2020.12-<a class=link href=https://arxiv.org/abs/2012.02911 target=_blank rel=noopener>Multi-head Knowledge Distillation for Model Compression</a></li></ul><p><strong>2021</strong></p><ul><li>2021-AAAI-<a class=link href=https://arxiv.org/abs/2012.03236 target=_blank rel=noopener>Cross-Layer Distillation with Semantic Calibration</a> [<a class=link href=https://github.com/DefangChen/SemCKD target=_blank rel=noopener>Code</a>]</li><li>2021-ICLR-<a class=link href="https://openreview.net/forum?id=NTEz-6wysdb" target=_blank rel=noopener>Distilling Knowledge from Reader to Retriever for Question Answering</a></li><li>2021-ICLR-<a class=link href="https://openreview.net/forum?id=uKhGRvM8QNH" target=_blank rel=noopener>Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors</a></li><li>2021-ICLR-<a class=link href="https://openreview.net/forum?id=ZzwDy_wiWv" target=_blank rel=noopener>Knowledge distillation via softmax regression representation learning</a> [<a class=link href=https://github.com/jingyang2017/KD_SRRL target=_blank rel=noopener>Code</a>]</li><li>2021-ICLR-<a class=link href="https://openreview.net/forum?id=m4UCf24r0Y" target=_blank rel=noopener>Knowledge Distillation as Semiparametric Inference</a></li><li>2021-ICLR-<a class=link href="https://openreview.net/forum?id=PObuuGVrGaZ" target=_blank rel=noopener>Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study</a></li><li>2021-ICLR-<a class=link href="https://openreview.net/forum?id=gIHd-5X324" target=_blank rel=noopener>Rethinking Soft Labels for Knowledge Distillation: A Bias–Variance Tradeoff Perspective</a></li><li>2021-CVPR-<a class=link href=https://arxiv.org/abs/2103.08273 target=_blank rel=noopener>Refine Myself by Teaching Myself: Feature Refinement via Self-Knowledge Distillation</a> [<a class=link href=https://github.com/MingiJi/FRSKD target=_blank rel=noopener>PyTorch Code</a>]</li><li>2021-CVPR-<a class=link href=https://arxiv.org/abs/2103.16367 target=_blank rel=noopener>Complementary Relation Contrastive Distillation</a></li><li>2021-CVPR-<a class=link href=https://arxiv.org/abs/2104.09044 target=_blank rel=noopener>Distilling Knowledge via Knowledge Review</a> [<a class=link href=https://github.com/dvlab-research/ReviewKD target=_blank rel=noopener>Code</a>]</li><li>2021-ICML-<a class=link href target=_blank rel=noopener>KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation</a></li><li>2021-ICML-<a class=link href target=_blank rel=noopener>A statistical perspective on distillation</a></li><li>2021-ICML-<a class=link href target=_blank rel=noopener>Training data-efficient image transformers & distillation through attention</a></li><li>2021-ICML-<a class=link href target=_blank rel=noopener>Zero-Shot Knowledge Distillation from a Decision-Based Black-Box Model</a></li><li>2021-ICML-<a class=link href target=_blank rel=noopener>Data-Free Knowledge Distillation for Heterogeneous Federated Learning</a></li><li>2021-ICML-<a class=link href target=_blank rel=noopener>Simultaneous Similarity-based Self-Distillation for Deep Metric Learning</a></li><li>2021-NIPS-<a class=link href=https://papers.nips.cc/paper/2021/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf target=_blank rel=noopener>Slow Learning and Fast Inference: Efficient Graph Similarity Computation via Knowledge Distillation</a> [<a class=link href=https://github.com/canqin001/Efficient_Graph_Similarity_Computation target=_blank rel=noopener>Code</a>]</li></ul><h4 id=2022-1>2022</h4><ul><li>2022-ECCV-<a class=link href=https://arxiv.org/abs/2203.17261 target=_blank rel=noopener>R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis</a> <a class=link href=https://github.com/snap-research/R2L target=_blank rel=noopener>Code</a></li><li>2022-NIPS-<a class=link href="https://openreview.net/forum?id=4d_tnQ_agHI" target=_blank rel=noopener>An Analytical Theory of Curriculum Learning in Teacher-Student Networks</a></li></ul><h2 id=papers-automl-nas-etc>Papers [AutoML (NAS etc.)]</h2><ul><li>2016.11-<a class=link href=https://arxiv.org/abs/1611.01578 target=_blank rel=noopener>Neural architecture search with reinforcement learning</a></li><li>2019-CVPR-<a class=link href=https://github.com/D-X-Y/GDAS/blob/master/data/GDAS.pdf target=_blank rel=noopener>Searching for A Robust Neural Architecture in Four GPU Hours</a> [<a class=link href=https://github.com/D-X-Y/GDAS target=_blank rel=noopener>Code</a>]</li><li>2019-CVPR-<a class=link href=https://arxiv.org/abs/1812.03443 target=_blank rel=noopener>FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search</a></li><li>2019-CVPR-<a class=link href=https://arxiv.org/abs/1808.00193 target=_blank rel=noopener>RENAS: Reinforced Evolutionary Neural Architecture Search</a></li><li>2019-NIPS-<a class=link href=https://papers.nips.cc/paper/9301-meta-architecture-search target=_blank rel=noopener>Meta Architecture Search</a></li><li>2019-NIPS-<a class=link href=https://papers.nips.cc/paper/8743-sparse-sparse-architecture-search-for-cnns-on-resource-constrained-microcontrollers target=_blank rel=noopener>SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/62d75fb2e3075506e8837d8f55021ab1-Abstract.html target=_blank rel=noopener>Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/d072677d210ac4c03ba046120f0802ec-Abstract.html target=_blank rel=noopener>Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural Architecture Search</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/5e1b18c4c6a6d31695acbae3fd70ecc6-Abstract.html target=_blank rel=noopener>Theory-Inspired Path-Regularized Differential Network Architecture Search</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/76cf99d3614e23eabab16fb27e944bf9-Abstract.html target=_blank rel=noopener>ISTA-NAS: Efficient and Consistent Neural Architecture Search by Sparse Coding</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/77305c2f862ad1d353f55bf38e5a5183-Abstract.html target=_blank rel=noopener>Semi-Supervised Neural Architecture Search</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/13d4635deccc230c944e4ff6e03404b5-Abstract.html target=_blank rel=noopener>Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/937936029af671cf479fa893db91cbdd-Abstract.html target=_blank rel=noopener>Does Unsupervised Architecture Representation Learning Help Neural Architecture Search?</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/9a96a2c73c0d477ff2a6da3bf538f4f4-Abstract.html target=_blank rel=noopener>Differentiable Neural Architecture Search in Equivalent Space with Exploration Enhancement</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/c6e81542b125c36346d9167691b8bd09-Abstract.html target=_blank rel=noopener>CLEARER: Multi-Scale Neural Architecture Search for Image Restoration</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/ea4eb49329550caaa1d2044105223721-Abstract.html target=_blank rel=noopener>A Study on Encodings for Neural Architecture Search</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/ec1f764517b7ffb52057af6df18142b7-Abstract.html target=_blank rel=noopener>Auto-Panoptic: Cooperative Multi-Component Architecture Search for Panoptic Segmentation</a></li><li>2020-NIPS-<a class=link href=https://papers.nips.cc/paper/2020/hash/fc146be0b230d7e0a92e66a6114b840d-Abstract.html target=_blank rel=noopener>Hierarchical Neural Architecture Search for Deep Stereo Matching</a></li></ul><p>*</p><h2 id=papers-interpretability>Papers [Interpretability]</h2><ul><li>2010-JMLR-<a class=link href=http://www.jmlr.org/papers/v11/baehrens10a.html target=_blank rel=noopener>How to explain individual classification decisions</a></li><li>2015-PLOS ONE-<a class=link href=http://heatmapping.org/ target=_blank rel=noopener>On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation</a></li><li>2015-CVPR-<a class=link href=https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dosovitskiy_Learning_to_Generate_2015_CVPR_paper.pdf target=_blank rel=noopener>Learning to generate chairs with convolutional neural networks</a></li><li>2015-CVPR-<a class=link href=https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Mahendran_Understanding_Deep_Image_2015_CVPR_paper.html target=_blank rel=noopener>Understanding deep image representations by inverting them</a> [2016 IJCV version: <a class=link href=https://link.springer.com/content/pdf/10.1007%2Fs11263-016-0911-8.pdf target=_blank rel=noopener>Visualizing deep convolutional neural networks using natural pre-images</a>]</li><li>2016-CVPR-<a class=link href=https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Dosovitskiy_Inverting_Visual_Representations_CVPR_2016_paper.html target=_blank rel=noopener>Inverting Visual Representations with Convolutional Networks</a></li><li>2016-KDD-<a class=link href=https://arxiv.org/abs/1602.04938 target=_blank rel=noopener>&ldquo;Why Should I Trust You?&rdquo;: Explaining the Predictions of Any Classifier</a></li><li>2016-ICMLw-<a class=link href=https://arxiv.org/abs/1606.03490 target=_blank rel=noopener>The Mythos of Model Interpretability</a></li><li>2017-NIPSw-<a class=link href=https://arxiv.org/abs/1711.00867 target=_blank rel=noopener>The (Un)reliability of saliency methods</a></li><li>2017-DSP-<a class=link href=https://arxiv.org/abs/1706.07979 target=_blank rel=noopener>Methods for interpreting and understanding deep neural networks</a></li><li>2018-ICML-<a class=link href=https://arxiv.org/abs/1711.11279 target=_blank rel=noopener>Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors</a></li><li>2018-CVPR-<a class=link href=http://openaccess.thecvf.com/content_cvpr_2018/html/Ulyanov_Deep_Image_Prior_CVPR_2018_paper.html target=_blank rel=noopener>Deep Image Prior</a> [<a class=link href=https://dmitryulyanov.github.io/deep_image_prior target=_blank rel=noopener>Code</a>]</li><li>2018-NIPSs-<a class=link href=https://arxiv.org/abs/1810.03292 target=_blank rel=noopener>Sanity Checks for Saliency Maps</a></li><li>2018-NIPSs-<a class=link href=https://arxiv.org/abs/1805.11571 target=_blank rel=noopener>Human-in-the-Loop Interpretability Prior</a></li><li>2018-NIPS-<a class=link href=https://arxiv.org/abs/1805.11783 target=_blank rel=noopener>To Trust Or Not To Trust A Classifier</a> [<a class=link href=https://github.com/google/TrustScore target=_blank rel=noopener>Code</a>]</li><li>2019-AISTATS-<a class=link href=https://arxiv.org/abs/1810.10118 target=_blank rel=noopener>Interpreting Black Box Predictions using Fisher Kernels</a></li><li>2019.05-<a class=link href=https://arxiv.org/pdf/1905.13405.pdf target=_blank rel=noopener>Luck Matters: Understanding Training Dynamics of Deep ReLU Networks</a></li><li>2019.05-<a class=link href=https://arxiv.org/abs/1905.02175 target=_blank rel=noopener>Adversarial Examples Are Not Bugs, They Are Features</a></li><li>2019.06-<a class=link href=https://arxiv.org/abs/1906.03728 target=_blank rel=noopener>The Generalization-Stability Tradeoff in Neural Network Pruning</a></li><li>2019.06-<a class=link href=https://arxiv.org/abs/1906.02773 target=_blank rel=noopener>One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers</a></li><li>2019-Book-<a class=link href=https://christophm.github.io/interpretable-ml-book/index.html target=_blank rel=noopener>Interpretable Machine Learning</a></li></ul><h2 id=workshops>Workshops</h2><ul><li><a class=link href=http://people.csail.mit.edu/beenkim/icml_tutorial.html target=_blank rel=noopener>2017-ICML Tutorial</a>: interpretable machine learning</li><li><a class=link href="https://openreview.net/group?id=ICML.cc/2018/ECA" target=_blank rel=noopener>2018-ICML Workshop</a>: Efficient Credit Assignment in Deep Learning and Reinforcement Learning</li><li>CDNNRIA Workshop (Compact Deep Neural Network Representation with Industrial Applications): <a class=link href="https://openreview.net/group?id=NIPS.cc/2018/Workshop/CDNNRIA" target=_blank rel=noopener>1st-2018-NIPSw</a>, <a class=link href=https://sites.google.com/view/icml2019-on-device-compact-dnn target=_blank rel=noopener>2nd-2019-ICMLw</a></li><li>LLD Workshop (Learning with Limited Data): <a class=link href=https://lld-workshop.github.io/2017/ target=_blank rel=noopener>1st-2017-NIPSw</a>, <a class=link href=https://lld-workshop.github.io/ target=_blank rel=noopener>2nd-2019-ICLRw</a></li><li>WHI (Worshop on Human Interpretability in Machine Learning): <a class=link href=https://sites.google.com/site/2016whi/ target=_blank rel=noopener>1st-2016-ICMLw</a>, <a class=link href=https://sites.google.com/view/whi2017/home target=_blank rel=noopener>2nd-2017-ICMLw</a>, <a class=link href=https://sites.google.com/view/whi2018 target=_blank rel=noopener>3rd-2018-ICMLw</a></li><li><a class=link href=http://learningsys.org/nips18/schedule.html target=_blank rel=noopener>NIPS-18 Workshop on Systems for ML and Open Source Software</a></li><li>MLPCD Workshop (Machine Learning on the Phone and other Consumer Devices): <a class=link href=https://sites.google.com/view/nips-2018-on-device-ml/home target=_blank rel=noopener>2nd-2018-NIPSw</a></li><li><a class=link href=http://bayesiandeeplearning.org/ target=_blank rel=noopener>Workshop on Bayesian Deep Learning</a></li><li><a class=link href=https://sites.google.com/view/cvpr20-nas/program target=_blank rel=noopener>2020 CVPR Workshop on NAS</a></li></ul><h2 id=books--courses>Books & Courses</h2><ul><li><a class=link href=https://efficientml.ai/ target=_blank rel=noopener>TinyML and Efficient Deep Learning</a> @MIT by Prof. Song Han</li></ul><h2 id=lightweight-dnn-enginesapis>Lightweight DNN Engines/APIs</h2><ul><li><a class=link href=https://github.com/Maratyszcza/NNPACK target=_blank rel=noopener>NNPACK</a></li><li>DMLC: <a class=link href=https://github.com/dmlc/tvm target=_blank rel=noopener>Tensor Virtual Machine (TVM): Open Deep Learning Compiler Stack</a></li><li>Tencent: <a class=link href=https://github.com/Tencent/ncnn target=_blank rel=noopener>NCNN</a></li><li>Xiaomi: <a class=link href=https://github.com/XiaoMi/mace target=_blank rel=noopener>MACE</a>, <a class=link href=https://github.com/XiaoMi/mobile-ai-bench target=_blank rel=noopener>Mobile AI Benchmark</a></li><li>Alibaba: <a class=link href=https://github.com/alibaba/MNN target=_blank rel=noopener>MNN</a> <a class=link href="https://yq.aliyun.com/articles/707014?spm=a2c4e.11153940.0.0.696d586bavHos1" target=_blank rel=noopener>blog (in Chinese)</a></li><li>Baidu: <a class=link href=https://github.com/PaddlePaddle/models/tree/v1.4/PaddleSlim target=_blank rel=noopener>Paddle-Slim</a>, <a class=link href=https://github.com/PaddlePaddle/paddle-mobile target=_blank rel=noopener>Paddle-Mobile</a>, <a class=link href=https://github.com/PaddlePaddle/Anakin target=_blank rel=noopener>Anakin</a></li><li>Microsoft: <a class=link href=https://microsoft.github.io/ELL/ target=_blank rel=noopener>ELL</a>, AutoML tool <a class=link href=https://github.com/microsoft/nni target=_blank rel=noopener>NNI</a></li><li>Facebook: <a class=link href=https://caffe2.ai/ target=_blank rel=noopener>Caffe2/PyTorch</a></li><li>Apple: <a class=link href=https://developer.apple.com/documentation/coreml target=_blank rel=noopener>CoreML</a> (iOS 11+)</li><li>Google: <a class=link href=https://developers.google.cn/ml-kit/ target=_blank rel=noopener>ML-Kit</a>, <a class=link href=https://developer.android.google.cn/ndk/guides/neuralnetworks/index.html target=_blank rel=noopener>NNAPI</a> (Android 8.1+), <a class=link href=https://tensorflow.google.cn/lite target=_blank rel=noopener>TF-Lite</a></li><li>Qualcomm: <a class=link href=https://developer.qualcomm.com/software/adreno-gpu-sdk/gpu target=_blank rel=noopener>Snapdragon Neural Processing Engine (SNPE)</a>, <a class=link href=https://developer.qualcomm.com/software/adreno-gpu-sdk/gpu target=_blank rel=noopener>Adreno GPU SDK</a></li><li>Huawei: <a class=link href=https://developer.huawei.com/consumer/cn/hiai target=_blank rel=noopener>HiAI</a></li><li>ARM: <a class=link href=https://github.com/OAID/Tengine/ target=_blank rel=noopener>Tengine</a></li><li>Related: <a class=link href=https://dawn.cs.stanford.edu//benchmark/index.html target=_blank rel=noopener>DAWNBench: An End-to-End Deep Learning Benchmark and Competition</a></li></ul><h2 id=related-repos-and-websites>Related Repos and Websites</h2><ul><li><a class=link href=https://github.com/D-X-Y/Awesome-NAS target=_blank rel=noopener>Awesome-NAS</a></li><li><a class=link href=https://github.com/he-y/Awesome-Pruning target=_blank rel=noopener>Awesome-Pruning</a></li><li><a class=link href=https://github.com/FLHonker/Awesome-Knowledge-Distillation target=_blank rel=noopener>Awesome-Knowledge-Distillation</a></li><li><a class=link href=https://github.com/microsoft/AI-System/tree/main/Lectures target=_blank rel=noopener>MS AI-System open course</a></li><li><a class=link href=https://github.com/BUG1989/caffe-int8-convert-tools target=_blank rel=noopener>caffe-int8-convert-tools</a></li><li><a class=link href=https://github.com/fengbintu/Neural-Networks-on-Silicon target=_blank rel=noopener>Neural-Networks-on-Silicon</a></li><li><a class=link href=https://github.com/ZhishengWang/Embedded-Neural-Network target=_blank rel=noopener>Embedded-Neural-Network</a></li><li><a class=link href=https://github.com/j-marple-dev/model_compression target=_blank rel=noopener>model_compression</a></li><li><a class=link href=https://github.com/666DZY666/model-compression target=_blank rel=noopener>model-compression</a> (in Chinese)</li><li><a class=link href=https://github.com/xiaoyufenfei/Efficient-Segmentation-Networks target=_blank rel=noopener>Efficient-Segmentation-Networks</a></li><li><a class=link href=https://www.automl.org/automl/literature-on-neural-architecture-search/ target=_blank rel=noopener>AutoML NAS Literature</a></li><li><a class=link href=https://paperswithcode.com/task/network-pruning target=_blank rel=noopener>Papers with code</a></li><li><a class=link href=https://paperswithcode.com/sota/image-classification-on-imagenet target=_blank rel=noopener>ImageNet Benckmark</a></li><li><a class=link href=https://paperswithcode.com/sota/self-supervised-image-classification-on target=_blank rel=noopener>Self-supervised ImageNet Benckmark</a></li><li><a class=link href=https://developer.nvidia.com/blog/tag/sparsity/ target=_blank rel=noopener>NVIDIA Blog with Sparsity Tag</a></li></ul></article></div><div class="license markdown"><blockquote>本站内容采用 CC BY-NC-SA 4.0 许可，请注明出处；商业转载请联系作者授权。</blockquote></div></div><div class="card comment-area"><script async crossorigin=anonymous src=https://giscus.app/client.js data-repo=villsi/villsi.github.io data-repo-id=R_kgDOKvi-KA data-category=评论 data-category-id=DIC_kwDOKvi-KM4CbUNT data-mapping=title data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy></script></div></div></div><aside class=sidebar><div class=sidebar__inner><div class="card search"><div class=search__input><input type=text id=search-input></div><div class=search__ctrl id=search-btn title=搜索><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" width="1.1em" height="1.1em"><path d="M208 80a128 128 0 11-90.51 37.49A127.15 127.15.0 01208 80m0-80C93.12.0.0 93.12.0 208s93.12 208 208 208 208-93.12 208-208S322.88.0 208 0z" style="opacity:.4"/><path d="M504.9 476.7 476.6 505a23.9 23.9.0 01-33.9.0L343 405.3a24 24 0 01-7-17V372l36-36h16.3a24 24 0 0117 7l99.7 99.7a24.11 24.11.0 01-.1 34z"/></svg></div></div><section class="card widget category"><div class=widget-title><div><span class=title-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></span><span class=title-text>Categories</span></div></div><hr class=widget-line><div class=category-list><ul><a href=https://blog.villsi.net/categories/blog/>Blog (1)</a></ul><ul><a href=https://blog.villsi.net/categories/uncategorized/>Uncategorized (15)</a></ul><ul><a href=https://blog.villsi.net/categories/%E8%B7%91%E5%9B%A2%E8%AE%B0%E5%BD%95/>跑团记录 (10)</a></ul></div></section><section class="card widget tagcloud"><div class=widget-title><div><span class=title-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg></span><span class=title-text>Tag</span></div></div><hr class=widget-line><div class=tagcloud-tags><a href=https://blog.villsi.net/tags/blog/>Blog
<sup><small>1</small></sup>
</a><a href=https://blog.villsi.net/tags/cloudflare/>Cloudflare
<sup><small>1</small></sup>
</a><a href=https://blog.villsi.net/tags/deep-learning/>Deep Learning
<sup><small>1</small></sup>
</a><a href=https://blog.villsi.net/tags/dnn/>DNN
<sup><small>1</small></sup>
</a><a href=https://blog.villsi.net/tags/game/>Game
<sup><small>1</small></sup>
</a><a href=https://blog.villsi.net/tags/github/>Github
<sup><small>9</small></sup>
</a><a href=https://blog.villsi.net/tags/katex/>Katex
<sup><small>1</small></sup>
</a><a href=https://blog.villsi.net/tags/markdown/>Markdown
<sup><small>3</small></sup>
</a><a href=https://blog.villsi.net/tags/obsidian/>Obsidian
<sup><small>1</small></sup>
</a><a href=https://blog.villsi.net/tags/programming/>Programming
<sup><small>8</small></sup>
</a><a href=https://blog.villsi.net/tags/trpg/>TRPG
<sup><small>10</small></sup>
</a><a href=https://blog.villsi.net/tags/typora/>Typora
<sup><small>1</small></sup>
</a><a href=https://blog.villsi.net/tags/%E5%86%B0%E9%A3%8E%E8%B0%B7/>冰风谷
<sup><small>10</small></sup>
</a><a href=https://blog.villsi.net/tags/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/>经验总结
<sup><small>1</small></sup>
</a><a href=https://blog.villsi.net/tags/%E9%9A%8F%E6%89%8B%E6%91%98%E5%BD%95/>随手摘录
<sup><small>1</small></sup></a></div></section><div class=sticky><div class="card toc markdown" id=toc-overlay><nav id=TableOfContents><ul><li><a href=#surveys>Surveys</a></li><li><a href=#papers-pruning-and-quantization>Papers [Pruning and Quantization]</a><ul><li></li><li><a href=#papers-actual-acceleration-via-sparsity>Papers [Actual Acceleration via Sparsity]</a></li><li><a href=#papers-lottery-ticket-hypothesis-lth>Papers [Lottery Ticket Hypothesis (LTH)]</a></li><li><a href=#papers-bayesian-compression>Papers [Bayesian Compression]</a></li></ul></li><li><a href=#papers-knowledge-distillation-kd>Papers [Knowledge Distillation (KD)]</a><ul><li></li></ul></li><li><a href=#papers-automl-nas-etc>Papers [AutoML (NAS etc.)]</a></li><li><a href=#papers-interpretability>Papers [Interpretability]</a></li><li><a href=#workshops>Workshops</a></li><li><a href=#books--courses>Books & Courses</a></li><li><a href=#lightweight-dnn-enginesapis>Lightweight DNN Engines/APIs</a></li><li><a href=#related-repos-and-websites>Related Repos and Websites</a></li></ul></nav></div></div></div></aside></div></main><footer class=footer><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><div class=container><span class=footer__text>Copyright &copy; 2018-2024 | <a class=link href=https://villsi.net target=_blank rel=noopener>villsi</a></span>
<a href=https://gohugo.io/ target=_blank rel=noopener title=Hugo><svg class="icon" viewBox="0 0 512 134" width="60" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid"><g fill-rule="evenodd"><path d="M113.888 85.371V43.446a15.378 15.378.0 00-7.587-13.258C96.212 24.262 78.39 13.79 67.475 7.376a18.048 18.048.0 00-18.614.2l-34.462 21.25a19.866 19.866.0 00-9.44 16.91v42.926a19.24 19.24.0 009.53 16.61L50.4 126.264a15.637 15.637.0 0015.5.161l39.76-22.145a16.018 16.018.0 008.22-13.991V85.37h.007z" fill="#ff4088"/><path d="M58.381.26h.095l.085.002h.485l.084.003h.01l.085.004h.01l.085.003h.007l.086.003h.01l.083.004h.01l.082.003h.007l.086.004h.01l.086.007h.01l.082.006h.007l.082.007h.01l.086.007h.007l.083.007h.01l.086.007h.01l.086.007h.01l.086.007h.01l.082.006h.01l.083.007h.01l.086.007h.01l.086.01h.01l.086.01h.007l.082.011h.01l.083.01h.01l.086.01h.01l.086.01h.007l.082.011h.01l.086.01h.01l.083.014.01.004.086.013h.01l.082.014h.007l.086.014h.01l.082.013h.01l.083.014h.007l.086.014h.01l.082.014.01.003.083.014.01.003.082.017h.01l.083.014h.01l.082.017h.01l.087.017h.006l.083.017h.01l.082.018h.007l.086.017.01.003.086.017h.007l.082.02.01.004.083.02h.01l.086.021.007.004.082.017.01.003.082.018.007.003.083.02h.01l.082.021.01.004.083.02.007.004.085.02.007.004.083.02.01.004.082.02h.007l.082.024h.01l.086.02.01.004.083.02h.01l.082.025.01.003.083.024.01.004.083.024.01.003.082.024.007.004.082.024.01.003.083.024.01.003.083.024.01.004.082.027.007.004.082.027h.01l.083.028h.01l.083.027.01.003.082.028.01.003.083.028.007.003.082.028.01.003.082.027.007.004.083.027.006.004.08.027.01.004.079.027.01.004.079.03.01.004.082.03.01.004.08.031.006.003.083.028.006.003.083.031.01.004.082.03.01.004.08.03.01.004.079.034.007.004.082.03.01.004.079.034.01.004.083.034.01.004.082.034.007.003.082.035.007.003.08.034.006.004.079.034.007.003.082.035.007.003.079.035.01.003.079.034.01.004.08.034.006.003.079.038.01.004.079.037.01.004.08.037.006.004.079.038.007.003.079.038.01.003.079.038.01.003.079.038.01.003.08.038.006.004.079.04.007.008.079.037.006.004.076.04.01.004.079.038.007.003.079.042.01.003.075.041.01.007.08.041.01.004.079.04.007.004.075.041.01.007.08.041.006.007.076.045.006.007.076.044.01.004.076.044.006.004.076.044.007.007.075.045.01.007.073.044h.006l38.855 22.83.065.037.007.007.065.038.01.003.062.041.007.007.062.038.006.007.062.037.007.004.062.037.007.004.061.038.007.003.062.038.01.003.062.041.007.004.061.041.01.003.062.038.007.004.062.04.007.008.062.04.007.008.058.037.01.004.062.04.007.004.061.041.007.004.059.041.006.003.059.042.007.003.058.041.007.007.058.041.007.004.058.044.01.007.059.041.007.004.058.04.007.004.058.041.007.007.058.041.007.004.058.041.007.007.059.041.006.007.059.044.01.007.055.045.01.007.058.044.01.004.06.044.006.007.058.045.007.007.055.044.01.007.059.048.01.01.11.09.013.01.11.092.014.01.113.093.014.01.113.093.014.01.11.093.013.014.11.092.013.01.107.097.013.01.107.096.013.01.107.1.014.01.102.096.014.01.106.1.014.013.103.1.014.014.103.1.013.013.1.1.014.013.099.103.014.014.1.102.01.014.099.103.014.014.096.103.013.013.096.103.014.014.096.103.014.014.096.106.014.014.092.11.01.013.096.106.014.014.093.11.013.014.093.11.01.013.093.11.014.013.089.11.01.014.09.11.013.013.09.114.01.013.085.113.01.014.086.113.01.014.086.113.01.017.086.117.01.014.086.113.01.017.083.117.01.013.082.117.01.014.083.116.01.017.08.117.01.017.078.12.01.017.076.117.01.014.076.116.01.014.079.12.01.014.076.12.01.017.072.12.01.017.076.12.01.017.072.12.01.017.072.124.01.013.07.124.01.017.068.124.01.017.069.123.01.017.069.124.007.017.068.127.007.017.065.127.007.017.065.127.01.014.062.13.007.017.065.127.007.017.062.127.01.017.059.13.006.018.059.13.007.017.058.13.007.017.058.13.007.018.055.13.007.017.054.13.007.018.052.134.007.017.051.13.007.017.051.134.007.017.052.134.007.017.051.134.007.017.048.134.007.017.044.134.007.017.045.133.007.018.044.133.007.017.045.138.003.017.041.134.007.017.041.137.004.017.04.137.008.017.04.137.008.018.037.137.007.017.038.137.003.017.035.137.007.021.037.14.004.018.034.14.007.018.03.14.008.017.034.14.003.021.031.141.004.017.03.14.004.018.027.14.004.018.027.14.004.017.027.141.003.017.024.144.004.017.024.141.003.02.024.145.004.02.02.144.004.02.02.141v.014l.01.076v.006l.007.072v.01l.01.073v.007l.011.072v.01l.01.072.004.01.01.072v.01l.007.072.003.007.007.072.004.01.007.073v.01l.006.072.004.01.007.072.003.01.007.073v.01l.007.072v.01l.007.072v.01l.007.072v.01l.006.073v.01l.004.072v.01l.003.076v.01l.007.076v.006l.004.072v.01l.003.073v.01l.003.072v.01l.004.072v.01l.003.076v.01l.004.072v.01l.003.073v.007l.004.072v47.873l-.004.079v.1l-.003.078v.01l-.004.076v.007l-.003.079v.01l-.004.076v.006l-.003.076v.01l-.003.076-.004.01-.003.075v.01l-.004.076v.007l-.003.079v.01l-.007.079v.007l-.007.079v.01l-.007.075v.01l-.003.076v.01l-.007.076v.01l-.007.076v.01l-.007.075v.01l-.007.076-.003.007-.007.079-.003.01-.007.076v.01l-.007.075-.004.01-.01.076v.01l-.01.076v.007l-.01.075v.01l-.01.076-.004.01-.01.079-.004.01-.01.076-.004.01-.01.075v.01l-.01.076v.01l-.01.076-.004.01-.01.076-.004.01-.013.075v.007l-.01.076v.006l-.014.076-.004.01-.01.076-.003.01-.01.075v.007l-.014.076v.007l-.014.075v.007l-.014.075v.01l-.014.073-.003.01-.014.072-.003.01-.014.076-.003.01-.017.075v.007l-.018.076v.007l-.013.072v.01l-.017.075v.007l-.014.076-.004.006-.017.072v.007l-.017.072v.01l-.017.073-.003.01-.018.072-.003.01-.017.072-.004.007-.017.072-.003.007-.017.072-.004.01-.02.072-.004.01-.017.073-.003.01-.02.072v.01l-.021.076-.004.01-.02.072v.01l-.021.072-.003.01-.021.073-.004.01-.02.075-.004.007-.02.072-.004.007-.02.072-.004.01-.024.072-.003.01-.02.073-.004.007-.02.072-.004.01-.02.072-.004.007-.024.072-.003.01-.024.072-.004.01-.024.072-.003.01-.024.073-.004.01-.024.072-.003.007-.024.072-.004.01-.024.072-.003.01-.024.072-.003.007-.024.072-.004.01-.024.073-.003.01-.028.068-.003.01-.024.07-.004.01-.024.068-.003.01-.024.073-.003.01-.028.072-.003.01-.028.069-.003.01-.028.069-.003.01-.028.068-.003.01-.027.07-.004.01-.027.068-.004.01-.027.07-.004.006-.03.069-.004.01-.03.072-.004.01-.028.069-.003.01-.03.069-.004.01-.031.068-.004.01-.03.07-.004.006-.03.069-.004.01-.031.069-.003.006-.031.07-.004.006-.03.069-.004.006-.03.069-.004.007-.035.068-.003.007-.034.065-.004.01-.03.07-.004.006-.034.065-.004.01-.03.07-.004.006-.034.069-.004.01-.034.069-.004.006-.034.066-.003.006-.035.069-.003.01-.034.069-.004.01-.034.069-.007.01-.034.065-.004.007-.034.069-.003.01-.035.068-.003.01-.038.066-.007.007-.037.065-.004.007-.037.065-.004.007-.038.065-.003.007-.038.065-.003.007-.038.065-.003.01-.038.065-.004.007-.037.065-.004.01-.037.066-.007.007-.038.065-.007.007-.038.065-.003.007-.038.065-.003.01-.041.065-.004.007-.037.062-.004.01-.038.062-.006.007-.042.065-.003.01-.041.062-.007.007-.041.062-.004.007-.04.065-.008.01-.04.062-.008.007-.04.061-.004.007-.041.065-.004.007-.041.062-.007.007-.044.061-.004.007-.041.062-.003.007-.045.062-.003.006-.042.062-.006.007-.042.062-.006.007-.045.061-.007.007-.041.062-.003.007-.045.062-.007.01-.041.062-.007.006-.044.062-.007.007-.045.062-.007.007-.044.061-.007.007-.045.062-.003.007-.045.058-.007.007-.044.058-.004.007-.048.058-.007.007-.044.059-.004.006-.044.062-.007.007-.048.062-.007.007-.048.058-.007.007-.048.058-.003.007-.048.058-.007.007-.048.058-.007.007-.048.059-.007.006-.048.059-.007.007-.048.058-.003.007-.048.058-.007.007-.051.058-.007.007-.048.058-.007.007-.048.058-.007.007-.048.059-.007.006-.048.059-.007.007-.048.054-.007.007-.051.059-.007.007-.048.058-.007.007-.051.058-.007.007-.048.058-.007.007-.051.055-.007.007-.052.055-.007.006-.051.055-.007.007-.051.055-.007.007-.052.055-.006.007-.052.054-.007.007-.051.055-.007.007-.051.055-.007.007-.055.055-.007.007-.052.054-.006.007-.052.052-.007.007-.055.054-.01.007-.055.055-.007.007-.054.051-.007.007-.055.055-.007.007-.055.051-.007.007-.055.052-.006.007-.055.051-.007.007-.055.051-.007.007-.055.052-.007.007-.055.051-.006.007-.059.051-.007.004-.054.051-.007.007-.059.051-.006.007-.059.052-.007.003-.058.052-.007.006-.058.048-.01.007-.059.048-.006.007-.059.052-.007.007-.058.051-.007.007-.058.051-.007.007-.058.048-.007.007-.062.048-.007.003-.058.048-.007.007-.058.048-.01.007-.059.048-.007.007-.058.048-.007.007-.058.048-.007.007-.062.048-.007.007-.058.044-.007.007-.061.048-.007.007-.062.048-.007.007-.062.048-.006.007-.062.048-.007.003-.062.045-.007.003-.061.045-.007.003-.062.045-.01.003-.062.045-.007.007-.062.044-.006.007-.062.045-.007.006-.062.045-.01.007-.062.044-.007.004-.065.045-.01.003-.065.045-.007.006-.065.045-.007.007-.065.041-.007.007-.065.044-.007.007-.062.045-.01.007-.065.04-.007.004-.065.041-.01.004-.066.041-.007.007-.065.041-.007.007-.065.041-.007.003-.065.042-.01.003-.065.041-.007.007-.065.041-.01.004-.066.04-.01.008-.065.04-.01.008-.069.04-.007.008-.065.037-.01.004-.065.038-.007.006-.069.042-.01.006-.069.038-.007.007-.065.038-.01.003-.069.038-.01.003-.069.038-.006.003-.066.038-.003.004-39.756 22.153h-.004l-.061.038-.01.003-.066.038-.01.007-.065.034-.01.004-.066.034-.007.003-.065.035-.01.003-.065.034-.01.004-.066.034-.01.003-.065.035-.007.003-.065.034-.007.004-.065.03-.007.004-.069.035-.006.006-.066.035-.006.003-.066.031-.006.003-.066.031-.007.004-.065.034-.007.003-.068.031-.007.004-.065.03-.007.004-.069.03-.006.004-.066.031-.006.004-.066.03-.01.007-.068.031-.007.004-.069.027-.007.003-.068.031-.007.007-.069.031-.01.003-.065.028-.007.003-.069.031-.007.004-.068.027-.007.003-.069.028-.006.003-.069.028-.01.003-.069.028-.01.003-.069.028-.01.003-.065.027-.007.004-.069.024-.007.003-.068.028-.01.003-.069.028-.01.003-.069.027-.01.004-.069.024-.01.003-.069.028-.01.003-.069.024-.006.004-.069.024-.01.003-.069.024-.01.003-.069.024-.01.004-.069.024-.006.003-.069.024-.069.024-.01.004-.072.02-.01.004-.069.02-.007.004-.068.024-.007.003-.072.02-.01.004-.069.02-.01.004-.069.02-.01.004-.072.024-.014.004-.137.04-.017.008-.14.04-.018.004-.14.041-.018.004-.14.038-.017.006-.141.038-.017.004-.14.037-.021.004-.141.034-.017.003-.14.035-.021.003-.14.034-.018.004-.14.03-.018.004-.144.031-.02.003-.144.031-.018.004-.144.027-.017.004-.14.024-.018.003-.14.027-.017.004-.144.024-.017.003-.144.024-.018.004-.144.02-.017.004-.144.02-.017.004-.144.02-.017.004-.144.02-.02.004-.145.017-.017.003-.144.018-.017.003-.144.017-.144.014h-.017l-.144.01-.018.004-.144.01h-.017l-.144.01h-.017l-.144.01-.02.004-.145.01h-.017l-.144.007h-.017l-.144.007h-.02l-.144.003h-.018l-.144.004h-.017l-.144.003h-.507l-.144-.003-.144-.004h-.021l-.144-.003h-.003l-.144-.007-.017-.003-.144-.007h-.017l-.145-.007-.017-.004-.144-.006h-.017l-.144-.01-.017-.004-.144-.014-.02-.003-.145-.014-.02-.003-.144-.014-.017-.004-.144-.013h-.018l-.144-.014-.017-.003-.144-.018h-.02l-.144-.017-.021-.003-.144-.02-.017-.004-.144-.02-.017-.004-.144-.02-.018-.004-.144-.02-.017-.004-.14-.024h-.01l-.145-.028-.018-.003-.14-.028-.018-.003-.14-.027-.018-.004-.14-.027-.02-.004-.143-.027-.018-.004-.14-.03-.02-.007-.141-.031-.018-.004-.14-.034-.02-.007-.14-.034-.017-.007-.14-.038-.018-.003-.14-.038-.018-.007-.14-.037-.017-.007-.141-.038-.017-.007-.14-.04-.021-.008-.14-.04-.018-.008-.14-.04-.018-.008-.14-.04-.018-.004-.136-.045-.014-.003-.072-.024h-.009l-.069-.021-.01-.003-.068-.024-.01-.004-.07-.024-.01-.003-.068-.024-.01-.004-.07-.024-.01-.003-.068-.024-.007-.004-.068-.024-.007-.003-.069-.024-.007-.003-.068-.028-.01-.003-.07-.028-.01-.003-.068-.028-.007-.003-.069-.027-.01-.004-.068-.027-.007-.004-.069-.027-.01-.004-.069-.027-.007-.003-.068-.028-.007-.003-.069-.028-.01-.003-.068-.028-.01-.003-.07-.028-.01-.003-.068-.027-.01-.004-.07-.027-.006-.004-.069-.03-.01-.004-.065-.03-.007-.004-.068-.031-.007-.004-.069-.03-.007-.004-.068-.027-.01-.004-.066-.03-.01-.004-.069-.03-.01-.004-.065-.031-.007-.007-.065-.03-.007-.004-.065-.031-.007-.004-.069-.03-.006-.007-.069-.035-.007-.006-.065-.031-.01-.004-.069-.03-.007-.007-.065-.035-.01-.003-.069-.035-.007-.003-.065-.034-.01-.004-.065-.034-.007-.003-.065-.035-.01-.007-.066-.034-.007-.003-.065-.035-.007-.003-.065-.034-.01-.007-.065-.034-.007-.004-.065-.038-.01-.006-.066-.035-.01-.003-.065-.034-.01-.004-.066-.034-.01-.004-.065-.037-.01-.004-.062-.037-.01-.004-.062-.038-.003-.003-35.94-20.977-.003-.004-.076-.044-.007-.004-.075-.044-.01-.004-.08-.044-.01-.004-.075-.048-.007-.003-.075-.048-.007-.007-.076-.048-.006-.007-.076-.048-.01-.003-.076-.045-.01-.007-.075-.048-.007-.007-.076-.048h.018l-.076-.048-.01-.007-.075-.048-.007-.006-.076-.048-.01-.007-.075-.052-.01-.007-.075-.051-.007-.007-.076-.048-.007-.007-.075-.051-.007-.007-.075-.052-.007-.006-.072-.052-.01-.007-.074-.051-.007-.007-.072-.051-.009-.007-.072-.052-.009-.007-.072-.051-.007-.003-.072-.052-.007-.007-.07-.051-.008-.007-.07-.051-.01-.004-.073-.051-.006-.007-.072-.055-.007-.007-.072-.051-.01-.007-.071-.055-.01-.007-.068-.055-.007-.007-.068-.055-.007-.003-.069-.055-.006-.007-.069-.055-.007-.006-.068-.055-.007-.007-.069-.055-.007-.007-.068-.055-.007-.007-.069-.055-.007-.006-.068-.059-.007-.003-.069-.055-.007-.007-.066-.055-.007-.007-.069-.058-.006-.007-.069-.055-.007-.006-.068-.059-.007-.003-.068-.055-.006-.007-.066-.055-.006-.007-.066-.058-.006-.007-.066-.058-.006-.007-.066-.058-.006-.007-.066-.058-.006-.007-.066-.059-.006-.006-.066-.062-.006-.004-.066-.058-.01-.007-.063-.061-.007-.007-.065-.059-.007-.006-.064-.062-.007-.007-.062-.062-.006-.007-.062-.061-.007-.007-.065-.062-.007-.007-.065-.062-.007-.003-.062-.062-.007-.007-.061-.061-.007-.007-.062-.062-.007-.007-.062-.061-.006-.007-.062-.062-.007-.007-.062-.062-.007-.006-.058-.062-.007-.007-.062-.062-.006-.007-.062-.065-.007-.007-.058-.065-.007-.007-.058-.065-.007-.007-.062-.065-.007-.007-.058-.065-.007-.007-.058-.065-.007-.007-.058-.065-.007-.007-.059-.065-.006-.007-.059-.065-.003-.007-.055-.065-.007-.007-.058-.065-.007-.007-.062-.068-.007-.007-.058-.065-.007-.007-.058-.069-.007-.007-.055-.065-.003-.007-.055-.065-.007-.007-.055-.068-.007-.007-.058-.069-.007-.007-.055-.065-.007-.007-.058-.065-.007-.007-.055-.065-.007-.007-.054-.068-.007-.007-.055-.069-.007-.007-.055-.068-.007-.007-.055-.069-.006-.007-.055-.068-.004-.007-.051-.069-.007-.006-.051-.069-.004-.007-.051-.068-.007-.007-.052-.072-.007-.007-.05-.069-.008-.007-.051-.068-.004-.007-.051-.072-.004-.007-.051-.072-.003-.007-.052-.068-.003-.007-.052-.069-.007-.007-.051-.072-.007-.007-.051-.072-.007-.01-.048-.072-.007-.007-.048-.072-.007-.007-.048-.072-.007-.006-.048-.073-.003-.006-.048-.072-.007-.01-.048-.073-.007-.007-.048-.072-.007-.006-.048-.072-.007-.007-.048-.072-.007-.007-.044-.072-.004-.007-.044-.072-.007-.01-.048-.072-.007-.01-.048-.073-.003-.007-.048-.075-.004-.01-.044-.072-.007-.007-.045-.072-.007-.007-.044-.075-.007-.01-.045-.073-.003-.007-.045-.075-.003-.007-.045-.072-.007-.01-.044-.076-.007-.01-.045-.075-.006-.01-.042-.076-.006-.007-.045-.075-.003-.007-.042-.076-.003-.007-.041-.075-.004-.01-.04-.076-.008-.01-.04-.076-.004-.01-.041-.079-.004-.007-.04-.075-.004-.01-.042-.076-.003-.01-.041-.075-.004-.01-.037-.08-.004-.006-.04-.08-.004-.01-.041-.079-.007-.006-.038-.08-.003-.006-.038-.076-.004-.01-.037-.079v.004l-.038-.08-.007-.01-.038-.078-.003-.01-.038-.076-.003-.01-.038-.08-.003-.006-.038-.079-.003-.007-.035-.079-.003-.007-.038-.078-.003-.007-.038-.08-.004-.006-.037-.079-.004-.01-.034-.08-.003-.006-.038-.079-.004-.01-.037-.079-.004-.007-.034-.079-.003-.007-.035-.078-.003-.01-.034-.08-.004-.006-.03-.08-.004-.006-.034-.079-.004-.01-.03-.083-.004-.006-.034-.08-.004-.01-.03-.075-.004-.01-.034-.083-.004-.007-.03-.082-.004-.01-.031-.082-.004-.007-.03-.083-.004-.01-.03-.082-.004-.007-.031-.082-.003-.007-.028-.083-.003-.01-.031-.082-.004-.007-.027-.082-.003-.01-.028-.083-.003-.01-.031-.082-.004-.01-.027-.083-.003-.007-.028-.082-.003-.007-.028-.082-.003-.01-.028-.083-.003-.01-.027-.083-.004-.01-.027-.082-.004-.01-.027-.083-.003-.01-.024-.082-.004-.007-.027-.086-.004-.01-.024-.086-.003-.01-.024-.082-.003-.01-.024-.083-.004-.01-.024-.086-.003-.01-.024-.086-.004-.007-.024-.085-.003-.004-.024-.086-.004-.006-.02-.083-.004-.007-.02-.082v-.007l-.024-.086v-.01l-.02-.082-.004-.01-.02-.083-.004-.01-.02-.086h.003l-.02-.085-.004-.007-.02-.086-.004-.007-.02-.086-.004-.006-.02-.086.002-.004-.02-.085-.004-.01-.02-.086-.004-.01-.017-.086-.003-.01-.02-.086-.004-.007-.017-.086-.004-.01-.017-.086-.003-.01-.017-.086-.018-.086v-.01l-.017-.09v-.01l-.017-.085-.003-.01-.02-.083v-.01l-.018-.093-.003-.01-.018-.09-.003-.01-.014-.085-.003-.01-.014-.086v-.01l-.014-.086v-.01l-.013-.086-.004-.01-.013-.086v.003l-.014-.086v-.006l-.014-.086v-.01l-.014-.086v-.01l-.013-.086v-.007l-.01-.09v-.01l-.01-.085v-.007l-.015-.09-.003-.01-.01-.085v-.01l-.01-.09-.004-.01-.01-.09v-.01l-.01-.089-.004-.01-.01-.09v-.01l-.01-.089-.004-.006-.01-.09-.007-.089v-.01l-.01-.09-.004-.01-.01-.089v-.01l-.007-.09-.003-.01-.007-.089v-.01l-.007-.089v-.01l-.007-.09-.007-.089v-.01l-.007-.09v-.01l-.007-.089v-.11l-.003-.09v-.01l-.004-.089v-.01l-.003-.09-.003-.01-.004-.089v-.096l-.003-.089v-.01l-.004-.09V45.728l.004-.086v-.11l.003-.089v-.315l.004-.09v-.006l.003-.09v-.01l.003-.089v-.01l.004-.09v-.006l.003-.09v-.01l.004-.089v-.01l.007-.092v-.01l.006-.09v-.007l.004-.089v-.01l.007-.09v-.01l.003-.089v-.01l.004-.09v-.01l.007-.089v-.01l.006-.09v-.01l.01-.089v-.01l.008-.09.003-.006.007-.09v-.006l.01-.09v-.006l.01-.09.004-.01.01-.089v-.01l.01-.089v-.007l.01-.09v-.006l.011-.09.003-.01.01-.089v-.01l.011-.086v-.01l.01-.089v-.01l.01-.09.004-.01.01-.085v-.01l.014-.09v-.01l.01-.09.004-.01.01-.085.003-.01.014-.09.004-.007.013-.089v-.007l.014-.089v-.007l.014-.085.003-.01.014-.087v-.01l.024-.089.003-.01.014-.086v-.01l.017-.086v-.007l.017-.086v-.006l.017-.09.004-.006.017-.086.003-.007.018-.086.003-.01.017-.086.004-.01.017-.086v-.01l.017-.086v-.007l.017-.085.003-.007.018-.086.003-.007.02-.086v-.01l.021-.086.004-.01.017-.086.003-.01.02-.086v-.01l.021-.086.004-.01.02-.086v-.01l.02-.086.004-.007.02-.085v-.007l.022-.086.003-.01.024-.086v-.007l.024-.086v-.006l.024-.086.003-.01.024-.086v-.01l.024-.086.004-.01.024-.083.003-.007.024-.085.004-.009.024-.086.003-.01.024-.084.004-.007.027-.085.003-.007.024-.082.004-.007.024-.084.003-.006.024-.086.004-.01.027-.083.004-.01.027-.083v-.007l.027-.082v-.01l.028-.083.003-.007.028-.082.003-.01.028-.083.003-.01.028-.082.003-.01.027-.083.004-.007.03-.082.004-.007.028-.082.003-.009.027-.082.004-.007.03-.082.004-.007.031-.082v-.007l.03-.082.004-.007.031-.083.004-.01.03-.082.004-.007.03-.082.004-.01.034-.08.004-.006.034-.083.004-.007.034-.082.003-.01.035-.082.003-.01.034-.083.004-.01.034-.083.003-.01.035-.082.003-.01.034-.083.004-.008.034-.08.004-.01.034-.078.003-.01.035-.083.003-.007.038-.082.003-.007.038-.079.003-.007.035-.079.003-.01.038-.079.003-.007.038-.079.003-.006.038-.08.004-.006.037-.079.004-.01.037-.08.004-.006.038-.079.003-.01.038-.08.003-.006.038-.079.003-.007.038-.079.003-.006.038-.08.004-.006.04-.076.004-.006.041-.08.007-.01.041-.078.004-.007.041-.08.003-.01.042-.075.003-.007.041-.075.004-.007.04-.076.004-.01.041-.075.007-.007.041-.076.004-.01.04-.075.008-.007.04-.076.004-.006.041-.076.007-.01.045-.076.003-.007.041-.075.004-.007.045-.075.003-.007.045-.076.003-.006.045-.076.006-.007.045-.075.003-.007.045-.075.007-.007.044-.076.004-.007.044-.075.007-.007.045-.075.003-.007.048-.076.007-.01.048-.075.004-.007.044-.072.007-.007.048-.072.007-.007.045-.075.003-.007.048-.072.003-.007.048-.072.007-.007.048-.076.007-.006.048-.072.004-.01.048-.073.003-.007.048-.072.003-.006.052-.072.007-.007.048-.072.007-.007.051-.072.003-.007.052-.072.003-.007.052-.072.003-.007.052-.072.007-.007.05-.072.004-.007.052-.072.007-.006.051-.069.004-.007.051-.072.007-.007.051-.068.007-.007.052-.069.006-.007.052-.068.007-.007.051-.069.004-.007.051-.068.007-.007.055-.069.007-.006.055-.069.006-.007.055-.068.007-.007.055-.069.007-.007.055-.068.007-.01.054-.07.004-.006.055-.069.007-.007.054-.068.007-.007.055-.065.007-.007.055-.069.007-.006.055-.069.006-.007.059-.065.007-.007.055-.069.006-.006.055-.066.007-.006.055-.069.007-.007.058-.068.004-.007.058-.065.007-.007.058-.065.007-.007.058-.065.007-.007.058-.065.004-.007.058-.065.007-.007.058-.066.007-.006.058-.066.004-.006.058-.066.007-.006.062-.066.003-.006.062-.066.007-.006.065-.062.007-.007.061-.065.007-.007.062-.065.007-.007.062-.062.006-.007.062-.061.007-.007.062-.062.007-.007.065-.062.007-.006.061-.062.007-.007.062-.062.007-.007.062-.061.006-.007.062-.062.007-.007.062-.061.007-.007.061-.062.007-.007.062-.062.007-.007.065-.058.007-.007.065-.058.007-.007.065-.062.007-.006.065-.062.007-.004.065-.058.007-.003.065-.059.007-.006.065-.059.007-.007.065-.058.007-.007.065-.058.007-.003.069-.059.006-.007.069-.058.007-.007.068-.058.007-.007.069-.055.007-.007.068-.055.007-.006.065-.055.007-.007.069-.055.007-.007.068-.055.007-.003.069-.055.006-.007.07-.055.006-.007.069-.055.006-.006.072-.055.007-.007.069-.055.007-.007.072-.055.007-.007.072-.054.006-.007.069-.055.007-.004.072-.054.007-.007.068-.055.007-.007.072-.051.007-.007.072-.055.007-.007.072-.051.007-.004.072-.051.01-.007.072-.052.007-.003.072-.052.007-.006.072-.055.007-.004.075-.051.007-.007.072-.051.01-.004.072-.051.007-.004.072-.051.007-.004.076-.051.006-.007.072-.051.007-.004.076-.048.007-.003.072-.052.01-.007.075-.048.007-.006.076-.052.007-.007.075-.048.01-.003.076-.048.007-.007.075-.048.007-.003.075-.048.007-.004.076-.048.006-.003.076-.048.01-.004.076-.044h.003L46.378 3.672l.004-.004.072-.044.007-.004.075-.048.01-.007.076-.044.01-.007.076-.045.01-.006.075-.045.01-.003.08-.045.006-.007.076-.044.01-.004.076-.044.006-.004.08-.041.006-.007.079-.041.007-.007.075-.041.01-.003.08-.042.006-.006.076-.042.01-.007.075-.04.01-.008.076-.04.007-.008.079-.04.007-.004.075-.041.01-.004.076-.04.01-.004.08-.041.01-.004.078-.041.007-.003.079-.038.01-.004.08-.037.01-.004.075-.037.01-.004.08-.038.006-.003.079-.038.01-.003.079-.038.01-.007.08-.037.01-.004.078-.038.01-.003.08-.034.01-.004.079-.034.007-.007.078-.034.007-.004.08-.034.01-.007.078-.034.01-.004.08-.034.01-.003.079-.035.01-.003.079-.034.01-.004.079-.034.01-.003.08-.031.01-.004.078-.034.01-.003.083-.035.01-.003.082-.034.01-.007.08-.031.006-.004.083-.03.01-.004.079-.03.007-.004.082-.031.01-.004.083-.027.01-.003.082-.031.01-.004.083-.03.01-.004.082-.027.01-.004.083-.027.01-.004.08-.027.01-.004.082-.027.01-.003.079-.028.01-.003.083-.028.006-.003.083-.028.01-.003.082-.027.01-.004.083-.027.01-.004.082-.024.007-.003.083-.024.01-.004.082-.024h.01l.083-.024.01-.003.082-.024.007-.003.083-.024.006-.004.083-.024.01-.003.082-.024.01-.004.083-.024.01-.003.083-.02.01-.004.082-.02.007-.004.082-.024.009-.004.085-.02h.007l.082-.024h.01l.085-.02.01-.004.086-.02.01-.004.083-.02h.01l.086-.021h.01l.083-.021.01-.003.082-.018.01-.003.083-.02.01-.004.082-.017h.01l.083-.017.01-.004.083-.017h.006l.083-.017.01-.003.086-.018h.007l.085-.017.007-.003.086-.017.01-.004.086-.017.01-.003.086-.014h.01l.083-.014.01-.003.086-.017.01-.004.086-.013h.01l.082-.014.01-.004.086-.013.01-.004.083-.013h.007l.082-.014h.01l.086-.014.01-.003.086-.014h.01L55.12.52h.01l.086-.013h.01l.086-.01h.01l.083-.011h.01l.086-.01.01-.004.086-.01.007-.004.085-.006h.01l.083-.01h.01l.083-.01h.01L55.97.42h.01l.086-.01h.01l.082-.007h.01l.086-.01.01-.004.086-.007.01-.003.086-.007h.01l.085-.007h.01l.086-.007h.01l.086-.003.01-.004.086-.007.007-.003.086-.004h.01l.086-.006h.006l.086-.004h.01l.086-.003h.01l.086-.004h.01L57.4.318l.01-.004.086-.003h.01l.086-.003h.01l.086-.004h.01l.086-.003h.01l.086-.004h.007l.086-.003h.106l.086-.004h.199l.018-.03zm-.079 9.26h-.168l-.059.002-.058.001-.058.002-.058.003-.059.004-.058.003-.055.004-.058.003-.055.004-.055.003-.055.003-.058.004-.055.003-.058.007-.059.007-.054.007-.059.007-.058.007-.058.006-.055.007-.058.007-.055.007-.055.007-.059.007-.054.007-.055.006-.055.007-.055.007-.058.007-.055.01-.055.007-.058.01-.055.01-.055.011-.055.01-.055.01-.055.01-.055.011-.054.007-.059.01-.055.01-.058.01-.055.011-.055.01-.055.01-.054.014-.055.014-.055.014-.055.01-.055.01-.055.014-.055.014-.054.014-.055.013-.055.014-.055.014-.055.014-.055.013-.055.014-.055.014-.054.017-.055.014-.055.017-.052.017-.054.017-.055.017-.055.017-.055.017-.055.018-.051.017-.055.017-.055.02-.055.018-.055.017-.055.02-.055.017-.054.018-.052.017-.055.02-.055.02-.055.021-.051.021-.055.02-.051.021-.052.02-.055.025-.054.02-.055.02-.055.021-.052.025-.051.02-.055.024-.051.024-.052.024-.051.024-.055.024-.052.024-.054.024-.052.024-.051.024-.052.024-.055.024-.051.024-.051.024-.052.028-.051.027-.052.024-.051.027-.052.024-.051.028-.052.027-.051.028-.051.027-.052.028-.051.03-.048.028-.052.027-.048.028-.051.03-.052.032-.051.03-.048.028-.052.03-.048.032-.051.03-.051.031-.055.035-34.43 21.22-.06.038-.054.034-.051.035-.052.034-.051.034-.052.035-.051.034-.052.034-.051.034-.052.035-.051.034-.051.034-.052.038-.048.034-.048.035-.051.034-.048.034-.048.034-.048.035-.048.034-.048.034-.048.035-.048.037-.048.035-.048.037-.048.038-.048.038-.048.038-.048.037-.048.038-.045.038-.048.037-.048.038-.045.038-.048.038-.048.037-.044.038-.048.038-.045.037-.044.038-.045.041-.045.042-.044.037-.045.038-.044.041-.045.041-.045.041-.044.042-.045.04-.044.042-.045.041-.044.041-.045.041-.045.041-.044.042-.041.04-.045.042-.041.045-.045.04-.04.045-.042.041-.041.045-.041.041-.041.045-.042.044-.044.045-.041.041-.041.045-.042.044-.04.045-.038.044-.042.045-.04.045-.042.044-.041.048-.041.045-.038.044-.038.045-.04.048-.039.044-.04.045-.038.048-.038.045-.038.048-.038.048-.037.048-.038.048-.038.048-.037.048-.035.048-.037.048-.038.048-.038.048-.038.048-.034.048-.034.048-.035.048-.034.051-.034.048-.034.048-.035.052-.034.048-.034.048-.035.048-.034.048-.03.051-.035.052-.034.048-.035.051-.03.051-.031.048-.031.052-.031.051-.031.052-.03.051-.035.052-.034.051-.031.052-.031.051-.031.051-.03.052-.032.051-.03.052-.031.051-.031.052-.031.051-.031.052-.03.054-.032.052-.03.051-.032.055-.027.052-.027.054-.028.055-.027.055-.028.052-.027.051-.028.055-.027.051-.027.055-.024.052-.028.055-.027.054-.028.055-.027.055-.028.055-.024.055-.024.055-.024.055-.024.055-.024.054-.024.055-.024.055-.024.055-.024.055-.02.055-.024.055-.024.058-.024.055-.024.055-.024.055-.02.054-.025.059-.024.055-.02.054-.021.059-.02.055-.021.058-.02.055-.021.058-.02.058-.021.059-.018.058-.02.055-.02.058-.018.058-.017.055-.017.059-.02.058-.018.055-.017.055-.017.058-.02.058-.018.059-.017.054-.017.059-.017.058-.018.058-.017.059-.017.058-.014.058-.013.058-.017.059-.014.058-.017.058-.014.059-.014.058-.013.062-.014.058-.014.058-.014.059-.013.058-.014.058-.014.059-.01.058-.014.058-.01.058-.01.062-.01.062-.01.062-.011.061-.01.059-.01.058-.011.062-.01.061-.01.059-.01.058-.011.058-.01.062-.007.062-.007.061-.007.059-.01.061-.01.059-.007.061-.007.062-.007.062-.007.062-.007.061-.007.062-.007.058-.006.062-.007.062-.004.062-.007.061-.006.062-.004.062-.003.062-.004.061-.003.062-.004.062-.003.061-.003.062-.004.062-.003.062-.004.061v.062l-.003.062-.004.062v.061l-.003.062v43.484l.003.062v.185l.004.058.003.062.004.062.003.061.004.062.003.062.003.062.004.061.003.062.007.062.004.061.003.062.003.062.007.058.007.062.007.058.003.062.007.058.007.059.007.058.007.058.007.062.007.058.006.059.007.061.007.062.007.058.007.062.01.058.01.059.01.058.011.058.01.062.01.058.01.059.011.058.01.058.014.058.01.059.01.058.011.058.014.059.013.058.01.058.014.059.014.058.014.058.014.058.013.059.014.058.014.058.013.055.018.055.013.058.014.059.014.058.017.058.014.055.013.058.014.055.017.059.014.054.017.059.017.058.017.055.018.058.017.058.017.055.017.059.02.055.021.054.02.055.021.055.017.055.021.058.02.055.021.055.02.055.021.055.02.055.021.055.021.054.02.055.021.055.024.055.024.055.02.055.021.055.02.054.021.055.021.055.024.055.024.055.024.055.024.055.024.055.024.054.024.055.024.055.024.055.024.055.024.055.024.051.028.055.027.052.027.054.028.052.027.051.028.052.027.051.028.052.027.051.03.051.028.052.028.051.027.052.031.051.03.052.028.051.028.052.027.051.031.051.028.052.03.048.031.051.031.048.031.052.03.051.032.048.03.048.031.048.031.048.031.052.031.051.034.048.031.048.031.052.03.048.032.048.034.048.034.048.035.051.034.048.034.048.035.048.034.048.034.048.034.048.035.048.037.048.038.048.034.048.035.048.034.048.034.048.038.045.038.044.038.048.037.045.038.048.038.045.037.044.038.045.038.044.038.045.037.044.038.045.038.045.037.044.038.045.038.044.041.045.041.045.038.044.041.041.041.045.041.041.042.045.037.044.041.045.042.04.04.042.042.041.041.041.041.045.041.041.041.041.042.041.04.042.042.04.045.042.04.041.042.041.044.041.042.041.04.042.045.04.045.039.044.037.045.041.045.042.044.037.045.038.044.041.045.041.045.038.048.038.048.037.048.038.044.038.045.038.048.037.044.038.048.034.045.038.048.038.048.038.048.037.045.035.048.037.048.035.048.037.048.038.048.034.048.035.051.034.048.034.048.034.048.035.048.034.048.034.048.035.052.034.051.03.051.035.048.034.052.035.051.034.048.03.052.032.051.03.052.031.051.031.052.031.054.031.052.03.051.032.059.034 35.898 20.988.044.027.038.02.041.021.041.02.042.021.037.024.041.02.042.021.04.021.042.024.041.02.041.021.041.02.042.021.04.02.042.022.041.02.041.02.038.021.041.02.041.021.041.018.041.02.042.017.04.017.042.018.041.02.041.02.041.018.042.02.04.018.042.017.041.017.041.017.041.017.041.017.042.017.04.018.042.017.041.013.045.018.04.017.042.017.041.017.041.017.041.014.042.017.04.014.042.014.044.013.042.014.037.014.09.027.085.028.086.024.086.024.085.024.086.024.086.024.086.024.085.024.086.02.086.02.086.021.085.017.086.021.086.017.085.017.086.017.086.014.086.017.085.014.086.014.09.013.085.014.09.01.088.01.09.011.089.007.085.007.09.007.085.006.09.007.089.007.085.007.086.003.086.004.089.003h.439l.09-.003h.088l.09-.004.089-.003.089-.007.089-.007.09-.003.088-.007.09-.007.089-.007.085-.007.09-.01.089-.01.085-.01.09-.01.085-.014.09-.014.085-.014.09-.014.085-.013.086-.017.086-.014.089-.017.085-.017.086-.021.086-.017.086-.02.085-.021.086-.02.086-.021.086-.024.085-.024.086-.024.086-.024.089-.028.041-.014.041-.013.041-.014.045-.014.041-.013.045-.014.04-.014.042-.014.045-.013.04-.014.042-.014.044-.017.042-.014.044-.013.041-.018.042-.013.04-.014.042-.014.044-.017.042-.014.04-.017.042-.017.041-.017.041-.017.041-.017.041-.017.042-.018.04-.017.042-.017.041-.017.041-.02.041-.021.042-.017.04-.02.042-.021.041-.018.041-.02.041-.02.041-.021.038-.02.041-.021.041-.021.042-.02.04-.021.042-.02.041-.021.041-.02.041-.022.038-.02.041-.02.041-.021.045-.024 39.77-22.164.048-.024.045-.024.04-.024.042-.024.041-.024.041-.028.045-.024.041-.024.041-.024.041-.024.041-.027.042-.027.04-.024.042-.024.041-.024.041-.028.041-.027.041-.024.042-.028.04-.024.038-.027.038-.028.038-.027.038-.027.037-.028.042-.027.037-.028.041-.027.038-.028.038-.027.038-.028.037-.027.038-.027.041-.028.038-.03.038-.028.037-.031.038-.027.038-.031.038-.031.037-.031.038-.027.038-.028.037-.03.035-.032.037-.03.038-.031.034-.028.038-.03.038-.035.034-.03.038-.032.034-.03.034-.031.035-.035.038-.03.034-.035.034-.03.034-.035.035-.034.034-.031.034-.034.035-.031.034-.035.034-.034.034-.034.035-.034.03-.035.035-.034.03-.034.035-.035.034-.034.035-.034.03-.035.031-.034.031-.034.031-.034.034-.035.031-.034.031-.034.031-.035.03-.034.032-.034.03-.034.031-.035.031-.037.031-.038.031-.038.03-.034.032-.035.03-.034.031-.038.031-.034.031-.038.028-.034.027-.038.028-.037.027-.035.03-.037.028-.038.028-.038.027-.037.028-.038.027-.038.027-.038.028-.037.027-.038.028-.038.027-.038.028-.037.027-.038.024-.038.024-.04.027-.042.028-.038.024-.037.024-.038.027-.041.028-.038.024-.038.024-.04.024-.042.024-.041.024-.041.024-.038.024-.041.024-.041.024-.038.024-.041.024-.041.024-.042.02-.04.021-.042.024-.041.024-.041.02-.041.025-.041.024-.042.02-.04.02-.042.021-.045.02-.04.025-.042.024-.041.02-.041.021-.041.02-.041.021-.045.02-.041.021-.041.02-.045.022-.041.02-.041.02-.045.021-.044.02-.045.021-.041.018-.041.017-.045.017-.044.017-.045.017-.041.017-.045.017-.044.018-.045.017-.041.017-.045.017-.044.017-.045.017-.044.017-.045.014-.045.014-.044.014-.045.013-.044.017-.045.018-.044.017-.045.017-.045.014-.044.017-.045.013-.044.014-.045.017-.045.014-.044.014-.045.014-.044.013-.045.01-.044.014-.045.014-.045.014-.044.013-.045.01-.044.011-.048.014-.045.01-.045.01-.048.014-.044.01-.045.01-.044.01-.045.011-.048.01-.044.01-.045.008-.048.01-.048.01-.045.007-.048.01-.044.01-.048.01-.048.011-.045.01-.048.007-.048.01-.048.007-.044.01-.045.008-.048.006-.048.007-.048.007-.048.007-.048.007-.048.007-.048.003-.048.007-.045.003-.048.007-.048.007-.044.007-.048.003-.048.004-.048.003-.048.004-.048.007-.048.003-.048.003-.048.004-.048.003-.048.004-.048.003-.048.004-.048.003-.048.003-.048.004-.048.003-.048v-.096l.004-.048V43.17l-.004-.042-.003-.045v-.044l-.004-.045v-.045l-.003-.044v-.045l-.003-.044-.004-.042-.003-.044-.004-.045-.003-.044-.004-.041-.003-.045-.003-.041-.004-.045-.007-.044-.003-.041-.004-.042-.006-.04-.004-.042-.007-.045-.003-.04-.007-.042-.007-.044-.003-.045-.004-.041-.007-.038-.013-.09-.01-.088-.014-.089-.014-.085-.017-.086-.017-.086-.014-.086-.017-.085-.02-.086-.021-.086-.02-.086-.021-.085-.021-.086-.02-.086-.021-.085-.024-.086-.024-.086-.024-.086-.028-.085-.024-.083-.027-.082-.028-.082-.027-.083-.027-.082-.031-.082-.031-.083-.031-.078-.03-.083-.032-.082-.03-.082-.035-.08-.03-.081-.035-.08-.034-.078-.035-.08-.037-.078-.035-.079-.037-.079-.038-.079-.038-.075-.04-.079-.039-.079-.04-.079-.042-.075-.041-.076-.041-.075-.041-.075-.042-.076-.044-.075-.045-.076-.044-.075-.048-.076-.045-.072-.044-.072-.048-.072-.049-.072-.048-.072-.048-.072-.048-.072-.048-.072-.051-.068-.051-.069-.052-.069-.051-.068-.052-.069-.055-.068-.054-.069-.055-.069-.055-.068-.055-.065-.058-.065-.055-.066-.059-.065-.058-.065-.058-.065-.058-.065-.059-.065-.061-.062-.062-.062-.062-.062-.062-.061-.061-.062-.066-.062-.065-.061-.065-.059-.065-.058-.065-.058-.065-.059-.066-.058-.068-.058-.069-.055-.068-.055-.065-.055-.072-.058-.031-.024-.035-.028-.034-.027-.034-.027-.034-.028-.035-.024-.034-.027-.034-.028-.038-.027-.038-.028-.034-.027-.034-.028-.035-.027-.034-.024-.034-.027-.038-.028-.034-.024-.035-.024-.037-.027-.038-.024-.038-.024-.034-.024-.038-.024-.037-.024-.038-.024-.038-.024-.038-.024-.037-.024-.038-.024-.038-.024-.038-.024-.037-.02-.038-.021-.045-.024-38.806-22.768-.054-.033-.05-.028-.052-.03-.051-.031-.05-.029-.052-.027-.05-.028-.051-.027-.052-.027-.051-.028-.05-.027-.052-.028-.051-.027-.052-.028-.051-.027-.052-.028-.051-.027-.052-.024-.052-.027-.051-.024-.052-.024-.051-.028-.053-.024-.052-.024-.051-.024-.054-.024-.051-.024-.052-.02-.053-.024-.052-.024-.051-.024-.054-.021-.051-.02-.055-.021-.053-.024-.054-.02-.055-.025-.055-.02-.053-.02-.053-.022-.054-.02-.055-.02-.054-.021-.052-.02-.051-.018-.052-.017-.055-.02-.055-.018-.051-.017-.055-.017-.055-.017-.051-.017-.055-.018-.055-.017-.055-.017-.051-.017-.055-.017-.055-.017-.055-.017-.055-.014-.055-.017-.054-.014-.055-.017-.055-.014-.055-.014-.055-.013-.055-.017-.055-.014-.054-.014-.055-.01-.055-.014-.055-.014-.055-.013-.055-.01-.055-.014-.055-.01-.054-.01-.055-.015-.055-.01-.055-.01-.055-.01-.055-.01-.055-.011-.055-.01-.058-.01-.055-.008-.055-.006-.054-.007-.055-.007-.055-.01-.055-.01-.055-.008-.058-.01-.055-.007-.055-.01-.055-.007-.055-.007-.055-.007-.054-.006-.055-.007-.059-.007-.054-.004-.055-.006-.055-.007-.055-.004-.055-.003-.055-.004-.055-.003-.055-.003-.058-.004-.058-.003-.055-.004-.055-.003h-.055l-.055-.004-.055-.003h-.058l-.055-.003-.055-.004-.051-.003-.055-.004h-.113l-.055-.003h-.11l-.055.023z" fill="#c9177e"/><path fill="#fff" fill-rule="nonzero" d="M32.205 101.851V30.789h14.867v25.884h24.663V30.789h14.911v71.062H71.735V70.779H47.072v31.072z"/><path d="M244.477 85.476V43.412c0-5.365-2.84-10.33-7.467-13.047L198.065 7.482a18.047 18.047.0 00-18.615.199l-32.97 20.331a23.002 23.002.0 00-10.93 19.58v41.173a19.239 19.239.0 009.532 16.608l35.91 20.99a15.646 15.646.0 0015.503.162l39.761-22.147a16.02 16.02.0 008.221-13.991v-4.918.007z" fill="#0594cb"/><path d="M188.97.366h.19l.085.002h.01l.085.001h.007l.086.002h.01l.082.003h.01l.083.003h.01l.086.003h.01l.086.003h.01l.086.004h.007l.086.004.006.001.086.004h.01l.086.005h.007l.086.006h.007l.085.006h.007l.086.006h.01l.086.007h.007l.082.007h.01l.086.007.01.001.086.007h.01l.083.008h.007l.085.007h.01l.086.006h.01l.086.007h.01l.083.007h.007l.086.01h.01l.082.01h.01l.086.011h.01l.086.01h.01l.086.01h.01l.083.01h.007l.082.011h.01l.083.01h.01l.082.01h.01l.086.011h.007l.086.01h.01l.086.014h.01l.082.014h.007l.086.014h.01l.083.013h.01l.086.014h.01l.086.014h.01l.082.017h.007l.086.017h.01l.082.014h.01l.083.017.01.003.086.017.01.004.086.017.01.003.083.018h.007l.082.017h.007l.085.017h.01l.083.017h.01l.083.02h.01l.082.021.007.004.082.02.01.004.083.02h.01l.083.02.006.004.083.02.007.004.082.024h.01l.083.02.01.004.082.02h.01l.083.021h.007l.082.02.01.004.082.02.01.004.083.024.007.004.082.024h.01l.083.024.01.003.082.024.007.004.083.024.01.003.082.024.01.004.083.024.01.003.082.024.01.003.083.024.007.004.082.027.007.004.082.027h.01l.083.028.01.003.082.027.007.004.083.027.006.004.083.027.01.004.082.027.01.003.083.028.01.003.083.028.006.003.083.028.01.003.082.03.01.004.083.031.01.004.079.03.01.004.083.03.01.004.079.031.01.003.082.031.01.004.08.034.01.004.079.034.01.003.079.035.007.003.082.03.007.004.079.035.01.003.082.034.007.004.08.034.006.003.079.035.01.003.082.034.01.004.08.034.006.004.08.037.01.004.078.034.007.003.08.038.006.004.079.037.01.004.083.034.006.007.08.038.006.003.079.038.007.003.079.038.01.003.079.038.01.004.08.037.006.004.079.037.01.004.076.041.006.007.08.041.01.003.078.042.007.003.08.041.01.004.078.04.01.008.076.04.01.004.08.041.006.004.076.04.006.004.08.041.006.004.076.041.01.004.075.044.007.004.076.044.006.007.076.045.007.006.075.045.01.007.08.044 38.99 22.936.063.038.01.003.062.038.01.007.062.037.007.007.061.038.007.003.062.038.007.007.061.038.007.006.062.038.007.004.058.037.007.004.062.037.007.004.058.038.007.003.058.038.007.003.058.041.01.004.059.04.007.004.061.042.01.006.117.08.017.01.12.082.018.01.116.082.014.01.117.083.017.01.116.083.017.01.114.086.013.01.113.086.014.01.113.086.014.01.113.086.014.013.113.09.014.013.11.09.013.01.11.089.014.01.11.09.013.013.11.093.014.01.11.093.013.01.106.092.014.01.106.093.014.01.107.093.013.01.103.097.014.01.103.096.013.014.103.096.014.013.1.096.013.014.103.1.014.013.1.1.013.013.1.1.013.014.1.099.01.014.1.103.013.013.096.103.014.014.096.103.014.013.096.103.013.014.096.106.01.014.093.106.01.014.093.106.014.014.092.106.01.014.09.106.01.014.093.107.013.013.09.107.01.017.089.11.01.013.086.11.01.017.086.11.01.013.086.114.01.013.083.114.013.013.083.113.01.014.082.113.01.014.083.113.01.014.079.117.01.017.08.116.01.017.078.117.01.014.08.116.01.017.075.117.01.017.076.117.01.017.076.12.01.017.075.12.01.017.073.117.01.017.072.12.007.017.072.12.01.014.069.12.01.017.072.124.01.017.069.123.01.014.065.123.007.018.069.123.007.017.065.124.01.017.062.123.007.017.065.127.007.014.062.127.006.017.062.127.007.017.058.127.007.017.062.127.007.017.058.127.01.017.055.127.007.017.055.13.007.018.055.13.01.017.055.13.007.018.054.13.007.017.052.13.007.017.051.13.007.018.048.13.007.017.048.13.007.018.048.134.007.017.044.133.007.018.045.133.006.017.045.134.007.017.044.134.007.017.041.134.004.017.041.134.007.017.041.134.003.017.038.137.007.017.038.134.007.017.037.137.007.017.035.138.003.017.034.137.004.017.034.137.003.017.035.138.003.02.031.137.007.018.03.14.004.017.031.137.003.018.031.14.004.017.027.14.004.018.027.14.003.021.024.14v.021l.024.141.004.017.02.14.004.021.02.141.004.02.02.141.004.017.02.14.004.021.017.144.004.021.017.14v.021l.017.14.003.018.014.144.003.017.014.144v.02l.014.145.003.017.01.14.004.021.01.144v.02l.007.141v.014l.004.072v.007l.003.072v.01l.003.072v.01l.004.073v.01l.003.072v.01l.004.072v.093l.003.072V90.61l-.003.075v.007l-.004.075v.096l-.003.076v.01l-.004.079v.007l-.003.075v.01l-.003.076v.01l-.004.076-.003.01-.004.075v.01l-.003.08v.01l-.004.075v.01l-.006.076-.004.01-.007.08v.01l-.003.078v.01l-.007.08v.01l-.003.075v.01l-.007.08v.01l-.007.075-.004.01-.006.076v.01l-.007.08v.006l-.007.075-.004.01-.006.076-.004.01-.007.076v.01l-.01.08v.01l-.01.075v.01l-.01.076v.01l-.01.075-.004.01-.01.076-.004.01-.01.076-.004.01-.01.075v.01l-.01.076v.01l-.01.076v.007l-.011.075v.01l-.01.076v.01l-.01.076-.004.01-.01.075-.004.01-.013.076v.007l-.014.075v.01l-.01.076v.007l-.014.076-.004.01-.013.075v.01l-.014.076-.003.01-.014.072-.004.01-.017.076v.01l-.017.076-.003.007-.014.075-.003.01-.014.072-.004.01-.017.076-.003.007-.017.075v.01l-.017.076v.007l-.018.075-.003.01-.017.073-.004.007-.017.075-.003.01-.017.076-.004.01-.02.072-.004.01-.017.072-.003.007-.017.076v.01l-.021.075-.003.01-.018.073-.003.01-.02.072v.01l-.021.072-.004.01-.02.073v.01l-.02.072v.01l-.021.072-.004.01-.02.073-.004.01-.02.072-.004.007-.02.072-.004.01-.024.072-.003.01-.024.072-.004.01-.024.073-.003.01-.024.072-.004.007-.024.072-.003.01-.024.072-.003.01-.024.072-.004.007-.024.072-.003.007-.024.072-.004.01-.024.072-.003.01-.024.073-.004.01-.027.072-.003.01-.024.072-.004.01-.024.073-.003.007-.028.072-.003.006-.028.072-.003.01-.027.07-.004.01-.027.072-.004.006-.027.07-.004.01-.027.071-.004.01-.027.07-.003.01-.028.068-.003.01-.028.069-.003.01-.028.069-.003.01-.027.069-.004.01-.03.072-.004.01-.031.069-.003.01-.031.069-.004.007-.03.068-.004.01-.03.07-.004.01-.031.068-.004.007-.03.068-.004.007-.03.069-.008.01-.03.069-.004.007-.034.065-.004.01-.03.069-.007.007-.031.068-.004.007-.034.069-.003.006-.031.066-.004.006-.034.069-.007.01-.034.065-.004.01-.034.066-.003.007-.035.068-.003.007-.034.065-.004.007-.034.069-.003.01-.035.065-.007.01-.037.066-.004.007-.037.068-.004.007-.034.065-.004.01-.037.066-.004.006-.034.069-.003.007-.038.065-.004.01-.037.066-.007.01-.034.065-.007.01-.038.065-.007.007-.038.065-.003.007-.038.065-.007.007-.037.065-.007.007-.038.065-.003.01-.038.066-.004.007-.037.065-.004.007-.037.065-.004.01-.041.065-.007.007-.041.065-.007.007-.041.062-.007.01-.041.065-.007.01-.038.062-.003.007-.041.065-.004.007-.04.062-.008.007-.04.062-.004.01-.041.062-.004.01-.041.062-.003.007-.042.061-.003.007-.041.062-.004.007-.04.061-.004.01-.045.062-.007.007-.04.062-.008.007-.04.062-.008.01-.044.062-.007.006-.045.062-.003.007-.045.062-.006.007-.045.061-.003.007-.045.062-.004.007-.044.062-.007.006-.048.059-.007.007-.044.061-.007.007-.045.058-.007.007-.044.062-.007.007-.048.062-.007.006-.045.059-.006.007-.045.058-.007.007-.044.058-.007.007-.048.058-.007.007-.048.058-.004.007-.048.059-.007.006-.048.059-.006.007-.048.058-.007.007-.048.058-.004.007-.048.058-.007.007-.048.058-.003.007-.048.059-.003.006-.048.059-.007.007-.052.058-.007.007-.051.055-.003.006-.052.055-.007.007-.048.055-.007.007-.051.055-.007.007-.051.054-.007.007-.052.055-.007.007-.051.055-.007.007-.051.055-.007.007-.052.054-.006.007-.052.055-.007.007-.055.055-.006.007-.052.055-.007.006-.051.055-.004.007-.054.055-.007.007-.052.055-.007.007-.051.054-.007.007-.055.055-.007.007-.055.052-.006.006-.055.052-.007.007-.055.055-.007.006-.055.052-.007.007-.054.051-.007.007-.055.051-.007.007-.055.052-.007.007-.058.054-.007.007-.055.052-.007.007-.055.051-.006.007-.055.051-.007.007-.058.052-.007.007-.059.051-.006.007-.059.048-.007.007-.058.051-.007.004-.058.048-.007.006-.058.052-.007.007-.058.048-.007.007-.059.048-.006.006-.059.048-.007.007-.058.052-.007.003-.058.048-.007.007-.058.048-.007.007-.058.048-.007.007-.058.048-.007.007-.059.048-.006.003-.059.048-.007.007-.058.048-.007.007-.062.048-.01.007-.062.048-.01.007-.062.048-.006.006-.059.048-.01.004-.062.048-.007.003-.061.048-.01.007-.062.045-.007.007-.062.044-.007.004-.062.044-.006.004-.062.044-.01.004-.062.048-.01.003-.062.045-.01.003-.062.045-.007.007-.065.044-.007.004-.065.044-.01.007-.066.041-.006.007-.062.045-.007.006-.062.042-.007.006-.065.042-.007.007-.061.04-.007.004-.065.045-.01.007-.066.04-.007.004-.065.041-.007.004-.065.04-.01.004-.065.041-.007.004-.065.041-.01.007-.066.041-.007.007-.065.038-.007.006-.065.042-.007.003-.065.041-.007.004-.065.04-.007.004-.065.041-.01.007-.069.041-.007.004-.065.04-.007.004-.068.042-.007.006-.069.038-.01.004-.069.04-.006.004-.066.038-.006.003-39.77 22.15-.004.004-.062.034-.006.004-.066.034-.01.003-.065.035-.007.006-.065.038-.01.004-.066.037-.006.007-.066.034-.01.007-.065.035-.007.003-.068.034-.007.004-.065.034-.007.003-.065.031-.007.007-.069.031-.01.003-.065.035-.007.003-.069.034-.007.004-.065.03-.01.008-.069.03-.007.004-.065.03-.01.004-.069.031-.01.004-.065.03-.007.004-.068.03-.007.004-.069.031-.01.003-.069.031-.01.004-.065.03-.007.004-.065.03-.007.004-.069.031-.007.004-.065.027-.01.003-.069.031-.01.004-.068.03-.007.004-.069.027-.01.004-.069.027-.007.004-.068.027-.01.004-.07.027-.006.003-.069.028-.006.003-.069.028-.01.003-.069.024-.007.004-.065.027-.007.003-.068.028-.007.003-.069.024-.01.004-.069.024-.01.003-.069.028-.006.003-.069.024-.01.004-.069.027-.007.003-.068.024-.007.004-.069.024-.007.003-.068.024-.01.004-.069.02-.007.004-.068.024-.01.003-.07.024-.01.004-.068.024-.01.003-.07.024-.01.003-.071.021-.01.003-.073.024h-.007l-.072.021-.013.003-.137.042-.018.003-.14.041-.02.004-.138.037-.02.004-.141.037-.017.007-.14.038-.018.003-.14.035-.018.003-.14.035-.021.006-.14.035-.021.003-.14.034-.021.004-.14.03-.018.004-.14.031-.018.003-.14.031-.018.004-.144.027-.017.004-.14.024-.018.003-.144.027-.017.004-.144.024-.017.003-.144.024-.017.004-.14.02h-.021l-.14.02-.018.004-.14.02h-.018l-.14.021-.017.004-.145.017-.017.003-.144.018-.017.003-.144.017h-.017l-.144.014-.02.003-.145.014h-.017l-.144.014h-.017l-.144.01-.017.003-.144.007-.02.004-.145.007h-.02l-.144.006-.018.002-.144.007h.004l-.144.003h-.017l-.144.007-.144.002h-.021l-.144.003h-.343l-.144-.003h-.02l-.145-.004h-.017l-.144-.004-.017-.004-.144-.003h-.017l-.144-.007h-.017l-.144-.007-.017-.003-.144-.01h-.018l-.144-.01h-.017l-.144-.011-.017-.003-.144-.014h-.017l-.144-.014-.017-.003-.144-.017-.018-.004-.144-.017-.017-.003-.14-.018h-.017l-.144-.017h-.018l-.144-.02-.017-.004-.144-.02-.017-.004-.14-.02-.018-.004-.14-.02-.017-.004-.145-.024-.017-.003-.144-.024-.017-.004-.14-.027-.018-.004-.14-.027-.017-.003-.14-.028-.022-.003-.14-.028-.017-.003-.14-.031-.018-.003-.144-.031-.017-.004-.14-.034-.018-.003-.14-.035-.021-.003-.14-.034-.021-.004-.14-.034-.018-.007-.14-.038-.018-.003-.14-.038-.018-.007-.14-.04-.017-.004-.137-.041-.021-.004-.137-.041-.017-.004-.138-.044-.017-.004-.133-.044-.01-.004-.073-.024-.007-.003-.068-.02-.007-.004-.069-.024-.006-.003-.07-.021-.01-.003-.068-.024-.007-.004-.068-.024-.01-.003-.07-.024-.01-.004-.068-.024-.01-.003-.069-.024-.01-.004-.069-.027-.01-.003-.069-.024-.007-.004-.068-.024-.01-.003-.07-.028-.006-.003-.069-.028-.01-.003-.068-.027-.01-.004-.07-.027-.006-.004-.069-.027-.01-.004-.069-.027-.01-.003-.068-.031-.01-.004-.07-.027-.01-.004-.068-.027-.007-.007-.069-.027-.01-.004-.065-.027-.01-.004-.066-.027-.006-.004-.066-.03-.01-.004-.068-.03-.01-.004-.07-.031-.01-.003-.065-.031-.009-.004-.068-.03-.007-.004-.065-.03-.007-.004-.069-.031-.01-.007-.065-.03-.01-.004-.07-.031-.006-.004-.065-.034-.01-.007-.066-.03-.006-.004-.069-.034-.007-.007-.065-.034-.01-.004-.065-.034-.01-.004-.066-.034-.01-.007-.065-.034-.007-.004-.065-.034-.007-.003-.065-.035-.007-.003-.065-.038-.01-.007-.066-.034-.007-.003-.065-.035-.01-.006-.065-.038-.01-.007-.066-.038-.007-.007-.065-.034-.01-.003-.065-.038-.007-.003-.065-.038-.007-.004-.065-.037-.01-.007-.062-.038-.004-.003-35.932-20.988-.007-.003-.076-.045-.01-.007-.079-.048-.007-.003-.075-.045-.007-.007-.075-.048-.007-.003-.076-.045-.006-.003-.08-.048-.006-.004-.076-.044-.007-.004-.075-.048-.007-.003-.075-.048-.007-.003-.076-.048-.006-.007-.076-.048-.007-.004-.075-.051-.007-.004-.072-.048-.007-.006-.075-.052-.01-.007-.073-.048-.007-.003-.075-.052-.007-.007-.075-.048-.007-.006-.072-.048-.007-.004-.075-.051-.007-.007-.072-.052-.007-.006-.072-.052-.007-.003-.072-.052-.01-.007-.072-.051-.007-.007-.072-.051-.01-.007-.073-.055-.006-.007-.072-.051-.007-.007-.072-.055-.007-.007-.072-.051-.007-.007-.069-.055-.007-.007-.072-.055-.006-.007-.072-.055-.007-.006-.072-.055-.007-.007-.069-.055-.01-.003-.069-.055-.01-.007-.068-.055-.007-.007-.069-.055-.007-.007-.068-.058-.007-.007-.069-.055-.007-.007-.068-.058-.007-.007-.069-.055-.007-.006-.068-.055-.007-.007-.068-.058-.006-.007-.069-.055-.007-.007-.068-.058-.007-.007-.069-.055-.007-.007-.068-.058-.007-.004-.065-.058-.007-.007-.065-.058-.007-.007-.069-.058-.007-.007-.065-.058-.007-.007-.065-.059-.007-.006-.065-.062-.007-.007-.065-.058-.007-.007-.065-.058-.007-.007-.065-.059-.007-.006-.065-.059-.007-.007-.062-.061-.006-.004-.066-.062-.006-.006-.062-.059-.007-.007-.062-.061-.007-.007-.061-.062-.007-.007-.062-.058-.007-.007-.061-.062-.007-.006-.062-.062-.007-.007-.062-.065-.006-.007-.059-.062-.003-.007-.062-.065-.007-.007-.062-.061-.006-.007-.059-.062-.007-.007-.061-.062-.007-.006-.058-.066-.007-.006-.062-.066-.007-.006-.058-.066-.007-.006-.058-.066-.007-.006-.058-.066-.007-.006-.059-.066-.007-.007-.058-.065-.007-.007-.058-.065-.007-.007-.058-.065-.007-.007-.055-.065-.007-.007-.058-.065-.007-.007-.058-.065-.007-.007-.055-.065-.007-.007-.055-.065-.006-.007-.055-.068-.007-.007-.059-.069-.006-.007-.059-.068-.003-.007-.055-.065-.007-.007-.055-.069-.003-.006-.055-.069-.003-.007-.055-.068-.007-.007-.055-.069-.007-.007-.055-.068-.003-.007-.052-.069-.003-.007-.055-.068-.007-.007-.051-.071-.007-.007-.052-.072-.003-.007-.051-.068-.007-.007-.052-.072-.007-.007-.051-.072-.007-.01-.051-.069-.004-.007-.051-.068-.007-.007-.051-.072-.004-.007-.051-.072-.007-.007-.05-.069-.003-.007-.052-.072-.007-.006-.051-.069-.003-.007-.052-.072-.007-.01-.048-.072-.003-.007-.048-.072-.007-.01-.048-.072-.006-.007-.048-.072-.003-.007-.048-.072-.007-.007-.048-.072-.004-.007-.048-.072-.003-.007-.048-.075-.004-.007-.048-.075-.003-.007-.045-.072-.003-.007-.045-.076-.003-.007-.045-.075-.003-.007-.045-.072-.003-.007-.045-.075-.003-.007-.045-.075-.007-.007-.04-.072-.008-.01-.044-.076-.004-.007-.044-.075-.004-.007-.044-.076-.004-.007-.044-.075-.004-.01-.044-.076-.007-.007-.041-.075-.004-.007-.04-.075-.004-.007-.045-.076-.003-.006-.041-.076-.004-.007-.04-.075-.004-.007-.041-.076-.004-.006-.041-.076-.004-.01-.04-.076-.004-.006-.041-.076-.007-.01-.038-.076-.003-.006-.038-.076-.003-.01-.042-.08-.003-.01-.038-.078-.003-.01-.038-.076-.003-.007-.038-.079-.003-.007-.038-.078-.004-.01-.037-.08-.004-.01-.037-.079-.004-.01-.038-.079-.003-.007-.034-.079-.004-.01-.037-.079-.004-.01-.038-.079-.006-.01-.035-.08-.003-.01-.035-.078-.003-.01-.034-.08-.004-.006-.034-.08-.003-.006-.035-.079-.003-.01-.034-.083-.004-.006-.034-.08-.004-.01-.03-.078-.004-.007-.034-.08-.004-.006-.034-.079-.003-.01-.035-.082-.003-.007-.03-.083-.004-.01-.031-.079-.004-.01-.03-.079-.004-.01-.03-.083-.004-.006-.031-.083-.003-.01-.031-.082-.004-.01-.03-.083-.004-.007-.03-.082-.004-.007-.031-.082-.004-.01-.027-.083-.004-.007-.027-.082-.003-.01-.031-.083-.004-.01-.027-.082-.004-.01-.027-.083-.003-.01-.028-.082-.003-.01-.028-.083-.003-.01-.028-.083-.003-.01-.027-.082v-.01l-.028-.086v-.01l-.027-.083-.004-.01-.027-.082-.004-.007-.027-.082-.003-.01-.024-.083-.004-.007-.024-.082-.003-.01-.024-.083-.004-.01-.024-.082-.003-.01-.024-.083-.004-.007-.024-.086-.003-.01-.02-.086-.004-.01-.02-.086-.004-.007-.024-.082-.004-.01-.02-.086v-.01l-.02-.086v-.01l-.021-.086-.004-.01-.02-.086-.004-.01-.02-.086-.004-.01-.02-.086-.004-.007-.02-.086v-.01l-.02-.086-.004-.01-.02-.086-.004-.01-.02-.086-.004-.01-.02-.086-.004-.006-.017-.086-.004-.01-.017-.086v-.01l-.017-.086-.004-.007-.017-.086-.003-.01-.017-.086-.017-.086-.004-.01-.017-.086-.003-.01-.018-.086v-.01l-.017-.09v-.01l-.013-.089-.004-.01-.014-.086v-.01l-.013-.086-.004-.01-.013-.086-.004-.006-.014-.086v-.01l-.013-.09v-.007l-.014-.085-.014-.086-.003-.01-.01-.086v-.01l-.014-.09-.004-.01-.013-.089-.01-.09-.004-.01-.01-.085v-.007l-.014-.086-.01-.085v-.01l-.01-.086v-.01l-.01-.086-.004-.007-.01-.086v-.01l-.011-.086v-.01l-.01-.09v-.01l-.007-.089v-.01l-.007-.089v-.007l-.01-.089-.004-.01-.007-.09-.003-.006-.003-.09v-.01l-.007-.089v-.01l-.007-.09v-.006l-.007-.09v-.01l-.007-.089v-.01l-.003-.09v-.01l-.007-.089v-.01l-.004-.089-.003-.09v-.007l-.003-.09-.004-.01v-.099l-.003-.09v-.006l-.004-.09v-.01l-.003-.089v-.01l-.004-.089V47.366l.004-.103v-.01l.003-.102v-.01l.004-.103v-.009l.003-.101v-.01l.004-.102v-.01l.003-.103v-.01l.003-.101v-.01l.004-.102v-.01l.003-.102v-.01l.007-.102v-.01l.004-.102v-.009l.003-.1v-.006l.003-.1v-.006l.004-.1v-.01l.007-.103v-.007l.007-.103v-.007l.006-.103v-.01l.007-.1v-.01l.01-.099.004-.01.007-.103v-.007l.01-.1v-.01l.01-.1v-.017l.01-.101v-.01l.011-.1v-.01l.014-.102v-.01l.013-.103v-.009l.01-.1.004-.01.01-.1.004-.008.014-.099v-.01l.013-.1.004-.009.014-.102.003-.01.014-.099v-.01l.013-.1v-.01l.014-.1v-.01l.014-.1v-.01l.017-.099v-.01l.017-.1.004-.006.017-.1.003-.01.017-.1v-.01l.017-.1v-.01l.018-.1v-.008l.017-.1.003-.01.017-.1v-.01l.017-.099.004-.01.017-.1.003-.01.018-.1.003-.01.017-.1v-.01l.017-.099.004-.006.02-.1v-.01l.02-.1.004-.01.02-.1.004-.01.02-.1v-.01l.021-.095.004-.01.02-.1.004-.01.024-.1v-.01l.024-.1v-.01l.024-.096.003-.01.024-.1.004-.007.024-.096v-.006l.024-.1.003-.01.024-.1.004-.01.024-.1v-.01l.024-.1.003-.01.028-.099v-.01l.027-.096.003-.007.028-.096.003-.01.028-.096v-.007l.027-.096.004-.01.027-.097.003-.01.028-.096.003-.007.028-.096.003-.01.028-.096.003-.007.03-.096.004-.01.028-.096v-.01l.03-.097.004-.006.03-.096v-.007l.032-.096.003-.007.031-.096.003-.007.031-.096.004-.01.03-.093.004-.007.03-.092.004-.007.035-.093.003-.01.034-.096.004-.01.034-.093.004-.01.034-.096.003-.007.035-.096.003-.01.034-.097.004-.01.034-.092.003-.007.035-.096.003-.007.034-.096.004-.007.038-.093.003-.01.038-.092.003-.01.038-.093.003-.01.038-.093.003-.01.038-.093.004-.01.037-.093.004-.007.037-.092.004-.01.038-.093.003-.01.041-.093.004-.007.037-.09.004-.006.041-.09.003-.01.038-.089.004-.01.04-.093.004-.006.041-.09.004-.01.04-.09.008-.006.04-.09.004-.006.041-.09.004-.01.041-.089.003-.006.042-.093.003-.007.045-.092.003-.007.045-.09.003-.01.041-.089.004-.007.04-.089.004-.01.045-.09.003-.006.045-.09.003-.006.045-.09.007-.006.048-.09.007-.006.044-.09.004-.006.048-.09.003-.01.045-.089.003-.007.045-.089.006-.01.045-.086.003-.007.045-.089.003-.01.048-.09.004-.01.048-.086.003-.006.048-.086.004-.01.051-.09.004-.006.048-.086.003-.007.052-.086.003-.01.051-.09.004-.006.051-.086.004-.01.051-.086.004-.007.051-.085.007-.007.051-.086.004-.007.051-.086.007-.01.051-.086.004-.006.051-.086.004-.007.051-.086.007-.007.052-.082.006-.007.055-.086.004-.006.051-.086.004-.007.054-.086.006-.007.051-.085.007-.007.055-.082.003-.007.055-.083.007-.006.055-.083.007-.007.054-.082.004-.007.055-.082.007-.007.055-.082.003-.007.055-.082.003-.007.055-.083.004-.006.058-.083.007-.007.055-.082.003-.007.058-.079.004-.007.058-.078.003-.007.059-.083.003-.006.058-.08.004-.006.058-.082.007-.007.058-.08.007-.006.059-.079.006-.007.062-.079.004-.007.058-.078.003-.007.059-.08.006-.006.062-.079.007-.007.058-.079.007-.006.062-.08.003-.006.062-.08.007-.006.062-.079.007-.007.065-.079.007-.006.061-.08.004-.006.061-.076.007-.006.062-.08.007-.006.062-.079.003-.007.065-.079.007-.007.062-.075.007-.007.065-.075.007-.007.065-.076.007-.007.065-.075.003-.007.065-.075.007-.007.065-.076.007-.006.065-.076.007-.007.065-.075.004-.007.065-.076.007-.006.065-.072.007-.007.068-.076.007-.007.065-.075.007-.007.069-.075.007-.007.068-.076.004-.006.072-.072.007-.007.068-.072.007-.007.069-.072.007-.007.068-.072.007-.007.072-.072.007-.007.072-.072.007-.007.072-.072.007-.007.072-.068.007-.007.068-.072.004-.007.072-.072.006-.007.072-.068.007-.007.072-.069.007-.007.072-.068.007-.007.072-.069.007-.007.075-.068.007-.007.072-.069.007-.006.072-.069.007-.007.075-.068.007-.007.076-.069.007-.007.075-.068.007-.007.075-.069.007-.007.076-.065.006-.007.076-.065.007-.007.075-.065.007-.007.076-.065.006-.007.076-.065.007-.007.079-.065.006-.007.076-.065.007-.007.075-.068.007-.007.079-.065.007-.007.079-.065.007-.004.078-.062.007-.006.08-.066.006-.006.079-.066.007-.006.079-.066.006-.006.08-.066.006-.006.08-.062.006-.004.079-.061.007-.007.082-.062.007-.007.081-.062.007-.006.082-.062.01-.007.083-.062.007-.007.082-.061.007-.007.082-.062.007-.007.082-.061.01-.004.083-.062.007-.003.082-.062.007-.007.082-.058.007-.007.086-.058.01-.004.082-.058.007-.007.083-.058.006-.003.083-.059.007-.007.082-.058.007-.007.082-.058.007-.003.086-.059.007-.003.082-.058.007-.004.082-.058.007-.004.086-.058.006-.007.086-.058.01-.004.086-.054.01-.007.086-.055.01-.004.086-.054.01-.004.086-.055.007-.007.09-.054.01-.004.085-.055.007-.007.086-.051.003-.003L176.954 3.71l.003-.004.076-.044.007-.007.075-.045.01-.003.076-.048.007-.004.078-.044.01-.004.076-.044.01-.007.076-.045.007-.007.079-.044.006-.004.076-.044.01-.007.076-.045.007-.003.078-.045.01-.003.076-.045.007-.007.077-.04.01-.004.076-.041.01-.004.079-.04.01-.008.079-.04.007-.004.079-.041.006-.004.08-.041.006-.007.08-.038.01-.007.078-.04.007-.004.079-.038.007-.007.075-.037.01-.007.08-.038.01-.003.079-.042.01-.003.079-.038.01-.003.079-.038.01-.003.08-.038.01-.003.078-.038.007-.004.079-.037.01-.004.08-.037.01-.004.078-.034.01-.004.08-.037.01-.004.079-.034.01-.003.079-.035.01-.003.079-.035.01-.003.08-.034.006-.004.079-.034.007-.003.082-.035.01-.003.08-.034.01-.004.078-.03.01-.004.083-.034.01-.004.08-.034.01-.004.082-.03.01-.004.082-.03.01-.004.08-.031.01-.003.082-.031.009-.004.078-.03.007-.004.083-.03.01-.004.082-.031.01-.004.083-.03h.01l.082-.028.01-.003.083-.028.01-.003.083-.028.006-.003.083-.028.01-.003.082-.027.01-.004.083-.027.007-.004.082-.027.01-.004.083-.024.007-.003.082-.027.007-.004.082-.024.01-.003.083-.024.01-.004.082-.027.01-.004.083-.024h.007l.082-.024.01-.003.083-.024h.01l.082-.024h.01l.083-.024.01-.003.082-.024.007-.004.083-.024.01-.003.082-.024.01-.004.083-.02.01-.004.082-.024.01-.003.083-.02.01-.004.082-.02h.01l.083-.021.01-.002.083-.021.01-.003.082-.021.01-.003.086-.021.007-.003.086-.021.01-.003.082-.02.01-.003.083-.02.01-.004.083-.017.01-.003.082-.017.007-.004.086-.017h.01l.086-.017h.01l.086-.017h.01l.082-.017.01-.004.083-.014.01-.003.086-.014.01-.003.083-.014h.007l.082-.017.01-.003.082-.014h.01l.086-.014.01-.003.083-.014.01-.003.086-.014.01-.004.086-.01h.01l.086-.01h.01l.083-.014h.01l.082-.01h.01l.086-.014h.01l.083-.01h.01l.082-.01h.01l.083-.01h.01l.083-.011h.01l.082-.01h.01l.086-.01h.01l.086-.011h.01l.083-.01h.01l.086-.007h.01l.086-.007h.01l.086-.007h.01l.086-.007h.01l.086-.007h.01l.086-.006h.01l.083-.007h.01l.086-.007h.01l.086-.007h.01l.086-.003h.01l.086-.004h.01l.086-.007h.01l.086-.003h.013l.086-.004h.01l.086-.003h.01l.086-.003h.01l.086-.004h.686l.014.027zm-.079 9.26h-.51l-.056.003h-.056l-.057.004-.058.003-.059.004-.055.003-.058.003-.058.004-.058.003-.059.004-.058.003-.058.004-.059.003-.058.003-.055.004-.058.007-.058.007-.055.006-.055.004-.058.007-.055.007-.059.006-.058.007-.058.007-.055.007-.055.007-.058.007-.055.007-.058.01-.055.007-.055.01-.055.01-.055.007-.055.01-.055.01-.054.011-.055.01-.055.01-.055.01-.058.011-.055.01-.055.014-.055.01-.055.014-.055.01-.058.014-.055.014-.055.01-.055.014-.054.014-.055.013-.055.014-.055.014-.055.013-.055.014-.051.014-.055.014-.055.013-.055.017-.055.018-.055.017-.054.017-.055.017-.055.017-.052.017-.055.018-.051.017-.055.017-.055.017-.055.017-.054.017-.052.02-.051.021-.055.018-.055.017-.052.02-.054.02-.052.018-.055.02-.051.021-.052.02-.054.025-.052.02-.055.02-.051.021-.055.021-.055.024-.055.02-.051.025-.052.024-.051.024-.052.024-.051.024-.051.024-.052.024-.051.024-.052.024-.051.024-.052.027-.051.028-.052.024-.051.027-.051.027-.052.028-.051.027-.052.028-.051.027-.052.028-.051.027-.051.027-.052.028-.051.027-.052.028-.051.027-.052.028-.051.03-.052.028-.051.03-.048.032-.051.03-.055.035-32.963 20.326-.065.041-.065.038-.062.04-.062.039-.061.04-.062.042-.062.041-.061.041-.062.041-.062.041-.062.042-.061.04-.062.045-.058.042-.062.044-.058.041-.059.045-.061.041-.059.041-.061.045-.062.044-.059.045-.058.044-.058.045-.058.045-.059.044-.058.045-.058.044-.059.045-.058.046-.055.047-.055.044-.058.045-.058.048-.059.045-.054.048-.055.048-.055.048-.055.048-.055.048-.055.048-.055.048-.054.051-.055.048-.055.048-.055.048-.052.052-.054.048-.055.048-.052.048-.055.051-.054.052-.055.048-.052.048-.051.051-.052.051-.051.052-.052.051-.051.052-.051.051-.052.052-.051.051-.052.052-.051.051-.048.055-.048.051-.052.055-.051.055-.048.055-.048.051-.048.055-.048.055-.048.055-.048.055-.048.055-.048.055-.048.054-.048.055-.045.055-.048.055-.044.058-.045.055-.045.055-.044.055-.048.058-.045.055-.044.055-.045.058-.044.059-.045.058-.045.058-.044.058-.045.055-.044.059-.041.058-.042.058-.044.059-.041.058-.042.062-.04.058-.042.058-.041.058-.041.059-.041.058-.041.062-.042.062-.04.061-.042.062-.041.062-.041.061-.038.062-.041.058-.041.062-.038.058-.038.062-.037.062-.038.062-.038.061-.038.062-.037.062-.038.062-.038.061-.037.062-.035.065-.037.062-.038.065-.034.065-.035.065-.037.062-.035.062-.034.065-.034.062-.035.061-.034.066-.034.065-.031.065-.034.065-.035.065-.03.065-.031.062-.031.065-.035.065-.03.066-.031.065-.031.065-.031.065-.03.065-.028.069-.031.065-.031.065-.028.065-.027.069-.027.068-.028.066-.027.068-.031.069-.028.065-.027.065-.027.069-.028.065-.027.068-.028.069-.027.069-.028.068-.024.069-.027.068-.024.069-.024.069-.024.068-.024.069-.024.068-.024.069-.024.068-.024.07-.024.068-.024.068-.024.069-.024.068-.02.069-.021.069-.024.072-.02.068-.02.069-.021.072-.02.068-.021.069-.021.069-.02.072-.02.072-.02.072-.018.068-.02.069-.02.072-.02.072-.017.072-.017.072-.017.068-.017.072-.018.072-.017.072-.017.072-.015.073-.013.072-.014.072-.014.072-.014.072-.013.072-.017.072-.014.072-.014.072-.014.072-.013.072-.01.072-.014.072-.014.072-.01.072-.014.072-.01.072-.014.072-.01.075-.01.076-.01.072-.011.072-.01.072-.01.072-.011.072-.01.072-.009.075-.006.072-.01.076-.008.072-.006.072-.007.075-.007.072-.007.076-.007.072-.007.075-.007.072-.006.072-.007.076-.007.075-.004.075-.003.072-.003.076-.004.075-.003.076-.004.075v.076l-.003.072-.004.072-.003.072v.075l-.002.076v41.587l.003.062v.12l.004.062v.12l.003.058.004.062.003.058.004.062.003.061.003.059.004.062.003.061.007.059.004.061.003.059.007.058.007.058.003.058.007.059.007.058.007.058.007.059.006.061.007.059.007.058.007.058.007.059.007.058.01.058.01.058.007.059.01.058.01.058.008.062.01.058.01.059.01.058.01.058.011.059.01.058.01.058.01.058.012.059.014.058.01.058.014.059.014.058.012.058.013.059.014.058.014.058.014.058.013.059.014.055.014.058.015.058.014.059.017.058.014.055.017.058.017.055.015.058.018.055.017.055.013.058.018.059.017.054.017.059.017.058.017.055.02.055.018.058.017.055.017.055.017.055.02.058.021.055.021.055.02.055.021.054.02.055.021.055.02.058.021.055.021.055.02.055.021.058.024.055.022.055.024.055.024.055.024.055.024.055.024.054.02.055.024.055.024.055.024.055.024.055.024.055.024.055.024.051.024.055.024.051.024.052.028.051.027.055.028.055.027.051.024.052.028.051.027.052.028.051.027.052.027.051.028.051.027.052.028.051.027.052.028.051.03.052.028.051.03.051.032.052.03.051.031.048.031.052.031.048.03.051.032.052.03.051.032.052.03.051.035.048.03.051.031.052.033.048.03.048.035.048.031.051.03.048.035.048.032.052.035.048.03.048.035.048.03.048.035.048.034.048.035.048.034.048.036.048.034.048.038.048.034.045.034.048.034.048.038.048.038.048.038.044.037.048.038.048.038.048.038.045.037.044.035.048.04.045.038.045.038.044.038.045.038.044.037.045.041.044.042.045.04.045.042.04.041.045.041.045.038.045.041.04.041.042.041.044.042.045.04.041.042.041.041.041.041.042.041.044.045.045.041.04.041.042.045.041.04.041.045.042.045.04.041.042.045.041.044.038.045.04.045.039.044.04.045.042.044.038.045.037.044.041.045.038.045.041.044.041.045.038.048.038.048.038.044.037.048.038.048.038.048.038.048.037.048.038.048.038.048.037.048.038.045.034.048.035.048.037.048.035.048.034.048.034.048.038.048.034.048.035.048.034.048.038.048.034.048.034.052.035.051.034.048.034.048.034.048.035.052.03.051.035.048.034.051.034.052.031.051.035.052.034.051.03.052.032.051.03.052.035.05.03.056.035 35.902 20.984.044.024.041.024.041.024.042.02.037.025.038.02.038.02.04.025.042.02.041.021.041.02.042.021.04.02.042.021.041.02.041.021.041.018.041.02.042.017.04.02.042.021.041.021.041.017.041.02.041.018.042.017.04.02.042.018.041.017.041.02.041.018.042.017.04.017.042.017.041.017.041.017.041.018.041.017.045.017.041.014.045.017.04.013.042.014.041.017.041.014.042.014.04.017.042.014.041.013.045.014.04.014.038.013.09.028.085.027.086.028.086.024.086.024.085.024.086.02.086.024.085.02.086.021.086.021.086.02.085.018.09.017.085.017.086.017.086.017.086.014.089.014.089.014.089.013.09.014.088.01.086.01.09.01.088.011.09.007.085.007.09.01.089.007.089.007.085.007.086.003.086.004.09.003.088.003.09.004h.267l.09-.004h.088l.09-.003.089-.003.085-.004.09-.007.089-.007.089-.006.089-.007.09-.007.088-.007.09-.01.085-.01.09-.01.085-.011.09-.01.088-.01.086-.014.09-.014.085-.014.086-.017.086-.017.085-.014.09-.017.085-.017.086-.017.089-.02.086-.022.089-.02.086-.02.085-.025.086-.024.086-.024.086-.024.085-.027.09-.028.037-.01.045-.014.041-.013.041-.014.041-.014.045-.013.041-.014.041-.014.041-.014.045-.017.041-.017.041-.014.045-.013.04-.014.042-.014.041-.013.041-.018.042-.017.04-.013.042-.018.041-.017.041-.017.041-.017.041-.017.045-.017.041-.017.041-.018.041-.02.042-.017.04-.018.042-.017.041-.017.041-.02.041-.021.042-.017.04-.017.042-.017.041-.018.041-.02.038-.02.041-.021.041-.02.041-.021.038-.021.041-.02.041-.021.042-.024.037-.02.041-.021.038-.024.041-.024.045-.024 39.753-22.14.048-.024.04-.024.042-.024.045-.024.04-.024.042-.028.041-.027.041-.024.041-.024.042-.024.04-.027.042-.024.041-.028.041-.024.041-.027.041-.028.038-.027.041-.028.041-.027.042-.028.04-.027.038-.027.038-.028.041-.027.042-.028.04-.027.038-.028.038-.027.038-.027.038-.028.037-.027.038-.028.038-.03.037-.028.038-.027.038-.028.038-.027.037-.031.038-.031.038-.03.038-.032.037-.027.038-.031.038-.031.034-.027.034-.031.035-.031.037-.031.035-.03.034-.032.034-.03.034-.031.035-.031.034-.031.034-.034.035-.031.034-.035.034-.03.035-.031.034-.031.034-.035.034-.034.035-.03.03-.032.035-.034.034-.034.034-.034.035-.035.03-.034.031-.034.031-.035.035-.034.03-.034.031-.034.031-.035.031-.034.03-.034.032-.035.03-.034.035-.034.03-.035.032-.034.03-.034.031-.034.031-.035.031-.034.031-.034.03-.035.032-.034.027-.038.031-.037.027-.035.031-.037.028-.038.027-.038.028-.037.027-.038.027-.038.028-.038.027-.037.028-.038.027-.038.028-.038.027-.037.028-.038.027-.041.027-.038.028-.038.027-.037.028-.038.027-.038.028-.04.027-.039.027-.037.028-.038.027-.038.024-.041.024-.038.024-.04.024-.039.024-.04.024-.042.028-.041.024-.041.024-.038.024-.041.024-.041.024-.041.024-.042.02-.04.024-.042.024-.041.024-.041.024-.041.024-.041.024-.042.024-.04.024-.042.021-.041.024-.041.024-.041.02-.041.021-.042.02-.04.021-.042.02-.041.021-.041.021-.045.02-.041.018-.045.02-.04.02-.045.018-.041.02-.042.018-.04.02-.042.02-.044.018-.042.02-.044.021-.045.017-.044.018-.045.017-.045.017-.044.017-.045.02-.044.018-.045.017-.041.017-.045.017-.044.017-.045.017-.044.018-.041.013-.045.017-.041.018-.045.017-.044.013-.045.014-.045.014-.044.017-.045.014-.044.017-.045.014-.041.013-.045.014-.044.014-.045.013-.044.014-.045.014-.048.014-.045.013-.044.01-.045.014-.044.01-.045.014-.044.01-.048.014-.045.01-.045.01-.044.011-.045.01-.044.01-.045.011-.048.01-.045.01-.044.011-.045.01-.044.01-.048.01-.048.011-.048.007-.048.01-.045.01-.044.01-.045.011-.048.007-.048.007-.048.007-.048.007-.048.006-.048.007-.048.007-.045.007-.048.007-.048.007-.048.007-.048.003-.044.003-.048.007-.048.007-.045.007-.048.003-.048.004-.048.003-.048.004-.048.007-.048.003-.048.003-.045.004-.048.003-.048.004-.048.003-.048.004-.048v-.048l.003-.048.003-.048.004-.048.003-.048v-.284l.004-.055V43.415l-.004-.048v-.173l-.003-.045-.004-.043-.003-.043v-.087l-.003-.044v-.087l-.004-.037-.007-.092-.007-.086-.006-.086-.01-.087-.008-.085-.01-.086-.01-.085-.01-.086-.01-.085-.011-.085-.014-.085-.013-.085-.014-.086-.014-.084-.014-.082-.017-.083-.013-.082-.018-.082-.017-.082-.017-.083-.02-.082-.021-.082-.02-.083-.021-.082-.02-.082-.021-.083-.024-.078-.024-.08-.024-.082-.024-.082-.028-.079-.024-.079-.027-.082-.028-.079-.027-.079-.028-.079-.03-.079-.031-.078-.031-.08-.031-.078-.03-.079-.032-.075-.034-.08-.034-.075-.035-.075-.034-.076-.038-.075-.034-.076-.038-.075-.037-.075-.038-.076-.038-.075-.041-.072-.041-.076-.038-.075-.041-.072-.041-.072-.041-.072-.045-.072-.041-.072-.045-.072-.044-.072-.045-.069-.048-.069-.048-.072-.044-.068-.045-.069-.048-.068-.048-.069-.051-.068-.048-.07-.052-.064-.051-.069-.052-.065-.051-.069-.055-.065-.055-.065-.051-.065-.055-.065-.055-.065-.055-.062-.058-.062-.059-.065-.058-.065-.058-.062-.058-.062-.059-.061-.061-.062-.062-.058-.059-.062-.061-.058-.062-.059-.062-.058-.061-.058-.062-.059-.065-.058-.062-.058-.062-.058-.065-.055-.065-.055-.065-.055-.065-.055-.069-.055-.065-.055-.065-.051-.069-.052-.068-.051-.07-.051-.068-.052-.072-.048-.072-.048-.068-.048-.072-.048-.076-.051-.034-.024-.034-.024-.038-.024-.038-.024-.034-.024-.038-.024-.038-.024-.037-.024-.038-.021-.038-.02-.037-.025-.035-.024-.037-.024-.045-.024L195.75 11.5l-.055-.031-.051-.03-.051-.028-.052-.028-.048-.027-.048-.028-.051-.027-.052-.027-.051-.028-.052-.027-.051-.028-.052-.027-.051-.028-.051-.024-.052-.024-.051-.024-.052-.024-.051-.027-.052-.024-.051-.027-.051-.025-.052-.027-.051-.024-.052-.024-.051-.024-.052-.024-.051-.024-.052-.024-.054-.024-.052-.024-.051-.024-.055-.024-.052-.024-.051-.02-.052-.025-.05-.02-.052-.02-.052-.021-.051-.02-.052-.021-.051-.021-.052-.02-.051-.021-.055-.02-.055-.021-.051-.017-.055-.021-.052-.02-.054-.018-.055-.017-.055-.02-.055-.018-.051-.017-.052-.017-.055-.017-.051-.017-.055-.017-.055-.018-.055-.017-.055-.013-.055-.014-.054-.014-.055-.014-.055-.013-.055-.017-.055-.014-.055-.014-.055-.014-.054-.013-.055-.01-.055-.014-.055-.014-.055-.014-.055-.013-.055-.014-.055-.01-.054-.014-.055-.014-.059-.01-.054-.01-.055-.01-.055-.011-.055-.01-.055-.01-.055-.01-.055-.011-.055-.01-.058-.01-.058-.011-.055-.01-.055-.007-.055-.007-.055-.01-.055-.007-.054-.01-.059-.007-.055-.007-.054-.007-.055-.007-.055-.007-.055-.003-.058-.007-.055-.007-.058-.003-.055-.007-.059-.004-.055-.003-.054-.004-.059-.003-.055-.007-.054-.003-.055-.004-.055-.003-.058-.004-.055-.003h-.055l-.059-.003h-.137l-.055-.004-.054-.003-.059-.004h-.223l-.038.016z" fill="#0083c0"/><path d="M162.66 34.081h14.502v38.467c0 6.104.168 10.06.504 11.869.58 2.906 1.65 4.926 3.833 6.687 2.182 1.759 4.73 2.258 8.515 2.258 3.846.0 5.133-.595 7.087-2.259 1.951-1.666 3.127-3.247 3.525-5.672.395-2.424 1.053-6.443 1.053-12.064V34.08h14.5v37.304c0 8.526-.368 14.549-1.102 18.07-.733 3.52-2.085 6.492-4.053 8.914-1.969 2.422-4.602 4.352-7.898 5.789-3.299 1.437-7.6 2.156-12.911 2.156-6.41.0-11.273-.784-14.582-2.35-3.313-1.566-5.93-3.601-7.853-6.104-1.924-2.503-3.19-5.127-3.8-7.872-.885-4.07-1.33-10.077-1.33-18.022V34.082h.01z" fill="#fff" fill-rule="nonzero"/><path d="M375.984 84.476V42.534c0-5.43-2.91-10.445-7.624-13.14L329.38 7.119a18.046 18.046.0 00-18.238.192l-32.915 19.742a23.004 23.004.0 00-11.17 19.725v40.991a19.238 19.238.0 009.531 16.608l35.911 20.991a15.646 15.646.0 0015.502.161l39.761-22.146a16.015 16.015.0 008.224-13.988v-4.922l-.002.004z" fill="#33ba91"/><path d="M320.469.109h.093l.083.001h.01l.083.001h.009l.084.004h.009l.083.003h.01l.083.004h.01l.083.003h.01l.082.004h.01l.084.003h.009l.083.003h.01l.083.004.01.003.083.004.01.003.086.007h.01l.086.007h.01l.086.007.01.003.082.007h.01l.083.007h.01l.083.007.01.003.082.004.01.003.083.007h.01l.082.007h.01l.083.007h.01l.082.006h.01l.083.007.01.004.086.007.01.003.083.01h.01l.082.007.01.004.083.01h.01l.082.01h.01l.086.01h.01l.083.01.01.004.083.01h.01l.082.01h.01l.083.011.01.004.082.013.01.004.083.013.01.004.083.014.01.003.082.014h.01l.083.013.01.004.082.014.01.003.083.014.01.003.083.014.01.003.082.014.01.003.083.018h.01l.082.017.01.003.083.014h.01l.083.014.01.003.082.014.01.003.083.017.01.004.082.017h.007l.083.017h.01l.082.017.01.004.083.017.01.003.082.017.01.004.083.017.01.003.082.017h.007l.083.021.01.003.082.021.01.003.083.021.01.003.082.021h.007l.083.02.01.004.082.02.01.004.083.024h.01l.082.02h.01l.083.025.007.003.082.02.01.004.083.024h.01l.082.024.01.004.083.024.01.003.082.024.01.003.083.024h.01l.083.028.01.003.082.024.01.004.08.027.01.004.082.024h.01l.083.024.01.003.082.027.01.004.083.027.007.004.082.027.01.004.08.027.01.003.078.028.01.003.08.028.01.003.079.028.01.003.079.027.01.004.079.03.01.004.08.028.01.003.078.03.01.004.08.031.006.004.08.03.01.004.078.03.01.004.08.031.01.004.079.03.01.004.079.03.01.004.079.031.007.003.079.035.007.003.078.031.01.003.08.035.01.003.079.034.01.004.079.034.007.004.079.034.01.003.079.035.01.003.079.034.007.007.079.034.01.004.075.034.01.004.08.037.01.004.075.038.01.003.076.038.01.003.08.038.006.003.079.038.007.003.075.038.01.004.076.037.01.004.076.037.01.004.075.041.01.003.08.038.01.007.075.038.007.003.076.041.01.007.075.041.007.004.076.04.007.004.075.041.007.004.075.041.01.007.076.041.01.007.076.041.007.007.075.041.007.007.075.045.01.003.076.045 38.957 22.256.066.038.01.003.065.038.01.003.062.038.01.007.062.038.007.006.065.038.01.004.065.037.01.004.062.037.01.007.063.038.006.003.062.038.007.004.062.04.007.004.065.038.01.007.062.037.01.007.058.038.007.004.062.037.01.004.062.037.007.007.061.038.01.007.063.041.006.003.062.042.01.003.062.041.007.004.062.04.007.004.058.041.01.004.058.04.007.008.059.04.01.008.058.04.007.004.058.041.007.004.058.041.01.007.06.044.01.007.058.042.01.006.058.045.007.003.058.045.007.007.062.044.007.004.058.044.007.007.058.045.007.007.058.044.007.004.059.044.006.007.059.045.007.007.055.044.006.007.059.045.007.003.054.048.007.007.059.048.006.007.059.044.007.007.054.045.007.007.055.048.007.003.058.048.01.007.11.09.014.013.11.096.014.014.106.092.014.014.106.096.017.014.106.096.014.013.106.1.017.014.103.099.014.014.103.1.014.013.103.1.013.013.103.1.014.013.103.103.013.014.1.103.014.013.1.103.013.014.1.103.013.014.096.106.014.014.096.103.013.013.096.107.01.013.097.107.014.017.092.106.014.014.092.11.014.017.093.11.013.013.09.11.013.014.09.11.01.013.089.11.01.013.09.114.013.013.09.114.01.013.085.113.01.018.086.113.01.017.086.116.01.018.083.116.01.014.083.117.01.013.079.117.01.017.082.117.01.013.08.117.01.017.079.12.01.017.079.12.01.017.075.12.01.014.076.12.01.017.076.124.01.017.076.12.01.014.072.123.01.017.072.12.01.014.07.123.01.018.071.123.01.017.07.124.006.013.069.124.01.017.065.123.007.018.065.126.01.018.066.126.01.018.065.127.007.017.062.127.01.017.062.127.01.017.062.127.007.017.058.127.01.017.059.127.006.02.059.13.007.018.054.13.007.02.055.13.007.018.055.13.007.018.051.133.007.017.051.134.007.017.052.134.007.017.051.134.007.017.048.134.007.017.048.134.007.02.048.134.007.017.044.134.007.017.045.134.006.017.042.137.006.017.042.134.006.017.042.137.006.018.042.137.006.017.042.137.003.02.038.138.003.017.038.137.007.017.037.14.004.021.034.141.004.017.034.14.003.021.031.14.004.018.03.14.004.021.03.14.004.018.028.14.007.021.027.14.003.021.024.138.004.013.014.076v.01l.013.072.004.01.01.072.003.01.01.073.004.006.01.072.004.007.01.072.004.01.013.073.004.01.01.072.003.007.01.072v.01l.011.072.003.01.007.072.004.007.01.069v.01l.01.072v.01l.01.073v.006l.007.072v.01l.01.073.004.01.007.072v.01l.01.069v.007l.007.072.004.01.006.072v.01l.007.072v.01l.007.073v.007l.007.072.003.01.007.072.004.007.007.072.003.013.007.072v.01l.003.073.004.01.003.072v.007l.004.072v.01l.003.076v.01l.003.072.004.01.003.072v.007l.004.072v.01l.003.076v.01l.004.072v.01l.003.076v.01l.003.072v.093l.004.072v.089l.003.072v47.462l-.003.075v.186l-.004.079v.01l-.003.075v.01l-.003.08v.01l-.004.075v.01l-.003.076v.01l-.004.08-.003.01-.004.075v.007l-.006.079v.006l-.007.08v.01l-.007.078v.01l-.007.08v.01l-.003.079v.01l-.007.075v.01l-.007.076v.01l-.007.076v.01l-.007.076-.003.01-.007.079-.004.01-.006.076v.01l-.007.075-.004.01-.01.076v.01l-.007.076-.003.01-.01.076v.01l-.01.075v.01l-.011.08v.01l-.01.075v.01l-.01.076v.01l-.011.076-.003.013-.01.076v.007l-.011.075-.003.014-.01.079-.004.01-.01.075-.004.007-.01.076-.004.01-.013.075v.01l-.014.076-.003.01-.014.076v.007l-.014.075v.007l-.014.076-.003.01-.014.075v.01l-.013.076v.01l-.014.076v.01l-.014.075-.003.01-.014.076-.003.007-.018.075-.003.01-.014.076-.003.007-.017.072v.01l-.017.076-.004.01-.017.076v.01l-.017.075v.007l-.017.072-.004.007-.017.072-.003.007-.018.075v.01l-.02.076-.004.01-.017.072v.01l-.02.076-.004.01-.017.072-.003.007-.02.072v.01l-.021.076v.01l-.021.072-.003.007-.021.072v.01l-.02.073-.004.01-.02.075-.004.01-.02.073-.004.01-.02.072-.004.007-.02.072-.004.01-.02.072-.004.007-.02.072-.004.01-.02.072-.004.01-.02.07v.006l-.025.072-.003.01-.024.072-.003.01-.024.073-.004.01-.024.072-.003.01-.028.072-.003.007-.024.072-.004.01-.024.069-.003.007-.024.072-.004.01-.024.072-.003.007-.024.072-.003.01-.028.072-.007.007-.027.072-.004.01-.027.072-.003.01-.028.07-.003.006-.028.069-.003.007-.028.068-.003.01-.027.07-.004.006-.027.069-.004.01-.027.072-.004.01-.027.069-.003.01-.028.072-.003.01-.031.069-.004.007-.03.072-.004.01-.03.069-.004.01-.031.069-.007.006-.027.069-.004.007-.03.068-.004.01-.03.07-.004.01-.031.068-.007.01-.031.069-.003.007-.031.068-.004.01-.03.07-.007.01-.035.068-.003.007-.034.065-.004.01-.034.07-.004.01-.034.068-.003.01-.035.065-.003.007-.034.069-.004.007-.034.068-.003.01-.035.066-.003.007-.034.065-.007.007-.038.065-.003.007-.035.068-.003.007-.038.065-.003.007-.035.069-.003.007-.038.068-.003.01-.038.07-.003.006-.038.065-.004.007-.037.065-.004.007-.037.065-.004.007-.038.065-.006.007-.038.065-.004.01-.037.066-.007.006-.038.066-.003.01-.038.065-.003.007-.038.065-.004.007-.037.065-.004.007-.04.065-.004.007-.041.062-.004.007-.041.065-.007.007-.038.065-.007.01-.04.062-.004.007-.041.065-.007.01-.041.065-.004.007-.04.062-.008.01-.04.065-.004.01-.041.062-.004.007-.041.062-.003.007-.042.061-.006.01-.042.062-.006.007-.042.062-.003.007-.045.062-.003.006-.041.062-.007.007-.045.062-.007.007-.044.061-.007.01-.045.062-.006.007-.042.062-.006.01-.045.059-.007.006-.044.062-.004.007-.044.062-.007.007-.045.061-.007.007-.044.062-.004.007-.044.058-.007.007-.045.058-.003.007-.045.062-.007.01-.048.058-.003.007-.048.059-.007.006-.048.059-.007.01-.048.058-.007.007-.044.058-.007.007-.048.059-.007.01-.048.058-.007.007-.048.058-.007.007-.051.058-.007.007-.048.059-.007.007-.048.058-.007.007-.048.058-.007.007-.051.058-.007.01-.048.059-.003.007-.048.054-.007.007-.052.055-.007.007-.048.055-.006.007-.052.055-.007.006-.051.055-.007.007-.051.055-.007.01-.052.055-.007.007-.051.055-.007.007-.051.055-.007.007-.052.054-.007.007-.054.055-.004.007-.051.055-.007.007-.055.055-.007.01-.055.055-.007.007-.051.054-.007.007-.055.055-.007.007-.054.055-.007.007-.055.051-.007.007-.055.055-.007.007-.055.055-.006.007-.055.051-.007.007-.055.051-.01.007-.055.052-.007.006-.055.052-.007.007-.055.051-.007.007-.054.051-.007.007-.059.052-.006.007-.055.051-.007.007-.058.048-.007.007-.059.051-.006.007-.059.052-.007.003-.058.048-.007.007-.058.051-.007.007-.058.052-.007.003-.058.048-.007.007-.059.051-.006.004-.059.048-.007.003-.058.048-.007.007-.058.048-.007.004-.058.048-.007.006-.058.048-.007.007-.062.048-.007.007-.058.048-.007.007-.062.048-.007.007-.058.048-.007.007-.061.048-.007.007-.062.044-.007.007-.062.048-.006.007-.062.044-.007.004-.062.045-.007.006-.061.048-.007.007-.062.045-.007.003-.062.045-.006.007-.062.044-.007.004-.062.044-.007.004-.065.044-.007.007-.061.045-.007.003-.062.045-.007.003-.061.045-.007.007-.066.04-.01.008-.062.044-.006.007-.066.045-.01.006-.065.045-.007.007-.065.045-.007.006-.062.042-.006.003-.066.041-.006.007-.066.041-.01.007-.065.041-.007.004-.065.04-.007.008-.065.04-.007.004-.065.041-.007.007-.065.041-.007.004-.065.04-.007.008-.065.04-.01.004-.069.038-.01.007-.065.041-.01.007-.07.038-.006.007-.069.037-.01.004-.069.04-.01.008-.065.037-.007.007-.069.041-.01.007-.065.038-.003.003-39.767 22.147-.003.004-.062.034-.01.003-.066.038-.01.003-.065.038-.01.004-.066.034-.006.007-.066.037-.006.004-.066.034-.01.004-.065.034-.01.003-.065.035-.007.003-.065.034-.01.004-.066.034-.01.007-.065.034-.007.004-.065.034-.01.003-.07.035-.01.003-.068.03-.007.008-.065.03-.01.004-.069.03-.01.004-.065.031-.007.004-.065.03-.01.004-.07.03-.01.004-.065.031-.01.004-.065.03-.01.004-.07.027-.01.004-.065.03-.006.004-.066.03-.01.004-.068.031-.01.004-.066.03-.01.004-.069.027-.007.004-.068.03-.01.004-.066.027-.01.004-.069.027-.01.007-.068.028-.007.003-.069.027-.01.004-.069.027-.007.004-.068.027-.007.004-.069.027-.01.003-.069.028-.006.003-.069.028-.01.003-.069.028-.01.003-.069.024-.01.004-.069.024-.01.003-.068.024-.01.003-.07.024-.006.004-.069.024-.01.003-.069.024-.01.004-.069.024-.006.003-.069.024-.01.004-.069.024-.01.003-.069.02-.01.004-.072.02h-.007l-.068.021-.01.004-.073.024h-.034l-.072.02-.014.004-.137.04-.017.008-.14.04-.018.004-.14.041-.018.007-.14.038-.017.003-.141.038-.02.004-.141.034-.017.003-.14.035-.02.003-.14.03-.02.004-.14.031-.018.004-.14.03-.021.004-.141.03-.021.004-.14.031-.018.003-.14.028-.018.007-.144.027-.017.004-.14.024-.017.003-.145.024-.017.003-.144.021-.017.003-.144.021h-.017l-.144.02-.02.004-.141.02h-.017l-.144.021-.021.004-.144.017-.017.003-.144.017-.02.004-.145.013h-.017l-.144.014h-.017l-.144.014-.02.003-.145.01-.017.004-.144.01h-.02l-.145.007h-.017l-.144.01-.017.004-.144.007-.017.003-.144.007h-.02l-.145.007h-.017l-.144.003h-.017l-.144.004h-.017l-.144.003h-.343l-.144-.003h-.02l-.141-.004h-.035l-.144-.003h-.02l-.144-.004h-.035l-.144-.006h-.017l-.144-.007h-.017l-.144-.007h-.02l-.145-.01-.02-.004-.144-.01h-.017l-.144-.01h-.018l-.144-.014-.017-.004-.14-.013-.018-.004-.144-.013h-.017l-.144-.018h-.02l-.145-.017h-.017l-.144-.02-.02-.004-.145-.02-.017-.004-.144-.024-.02-.003-.145-.024-.02-.004-.144-.024-.017-.003-.14-.027-.021-.004-.145-.027-.017-.004-.144-.027-.017-.004-.144-.03-.02-.004-.141-.03-.02-.004-.145-.031-.017-.003-.14-.031-.018-.004-.14-.034-.017-.003-.14-.035-.018-.007-.14-.034-.018-.003-.14-.038-.018-.004-.14-.037-.017-.004-.14-.04-.018-.004-.14-.041-.018-.007-.137-.041-.017-.004-.137-.044-.017-.004-.141-.044-.017-.007-.137-.045-.014-.003-.072-.024-.01-.004-.069-.024-.01-.003-.069-.024-.01-.004-.069-.024-.01-.003-.069-.024-.006-.003-.069-.024-.01-.004-.069-.024-.007-.003-.068-.024-.007-.004-.069-.024-.007-.003-.068-.024-.007-.004-.069-.027-.01-.004-.069-.024-.01-.003-.068-.027-.007-.004-.069-.027-.01-.004-.069-.027-.007-.004-.068-.027-.007-.003-.069-.028-.007-.003-.068-.028-.007-.003-.069-.028-.01-.003-.068-.027-.007-.004-.065-.03-.01-.004-.07-.031-.01-.007-.065-.027-.007-.004-.065-.027-.007-.004-.068-.03-.01-.004-.066-.03-.01-.004-.065-.031-.007-.003-.069-.031-.01-.004-.065-.03-.01-.004-.065-.03-.01-.004-.066-.031-.01-.004-.069-.03-.01-.004-.069-.034-.01-.004-.069-.03-.01-.004-.065-.034-.01-.004-.065-.03-.007-.004-.065-.034-.01-.004-.066-.034-.01-.003-.065-.035-.007-.003-.065-.034-.01-.004-.066-.034-.01-.004-.065-.034-.007-.003-.065-.035-.007-.006-.065-.035-.007-.003-.065-.034-.01-.004-.066-.038-.007-.003-.065-.034-.007-.007-.065-.038-.007-.003-.065-.038-.01-.004-.065-.037-.007-.007-.065-.038-.007-.003-.062-.035-.003-.003-36.008-21.005-.004-.003-.075-.045-.007-.003-.075-.045-.01-.003-.076-.048-.007-.004-.079-.048-.01-.003-.076-.045-.007-.003-.075-.048-.01-.007-.076-.048-.007-.003-.075-.048-.007-.007-.075-.048-.01-.007-.076-.048-.01-.003-.076-.048-.007-.007-.075-.048-.01-.007-.076-.052-.007-.007-.075-.051-.007-.007-.075-.048-.007-.003-.072-.052-.007-.007-.072-.051-.007-.007-.072-.051-.007-.007-.075-.052-.007-.003-.072-.051-.007-.007-.072-.052-.007-.003-.072-.052-.007-.006-.072-.052-.007-.003-.072-.052-.007-.003-.072-.052-.006-.007-.072-.054-.007-.007-.072-.052-.007-.007-.072-.054-.007-.007-.069-.055-.007-.007-.068-.055-.007-.007-.069-.051-.006-.007-.072-.055-.01-.007-.07-.055-.006-.003-.072-.055-.007-.007-.069-.055-.006-.007-.07-.054-.006-.007-.069-.055-.01-.007-.068-.055-.007-.007-.069-.058-.007-.007-.068-.058-.007-.007-.069-.055-.007-.007-.068-.058-.007-.007-.069-.058-.006-.007-.066-.058-.006-.004-.07-.058-.006-.007-.065-.058-.007-.007-.069-.058-.006-.007-.066-.059-.006-.006-.066-.059-.006-.007-.069-.058-.007-.007-.065-.058-.007-.007-.065-.058-.007-.007-.065-.058-.007-.007-.062-.062-.006-.007-.062-.062-.007-.006-.065-.062-.007-.007-.065-.058-.007-.007-.065-.058-.007-.007-.065-.062-.007-.007-.062-.058-.007-.007-.061-.062-.004-.003-.062-.062-.006-.007-.062-.062-.007-.006-.062-.062-.007-.007-.061-.062-.007-.007-.062-.061-.007-.007-.058-.062-.007-.007-.062-.065-.007-.007-.061-.061-.007-.007-.062-.062-.007-.007-.058-.065-.007-.007-.062-.062-.006-.007-.059-.065-.007-.007-.058-.065-.007-.007-.058-.065-.007-.007-.058-.065-.007-.007-.058-.065-.007-.007-.059-.065-.006-.007-.059-.065-.007-.007-.054-.065-.004-.007-.058-.068-.007-.007-.058-.065-.007-.007-.055-.065-.004-.007-.058-.065-.003-.007-.055-.065-.007-.007-.055-.069-.007-.007-.055-.068-.006-.007-.055-.069-.004-.006-.055-.066-.006-.01-.055-.065-.007-.007-.055-.069-.007-.006-.051-.069-.007-.007-.055-.068-.007-.007-.051-.069-.007-.007-.055-.072-.007-.007-.051-.068-.007-.007-.052-.069-.007-.006-.051-.073-.007-.006-.051-.069-.007-.007-.052-.07-.006-.01-.052-.072-.003-.007-.052-.068-.007-.007-.048-.072-.006-.007-.048-.072-.007-.007-.052-.07-.003-.007-.052-.068-.007-.01-.051-.072-.003-.007-.048-.072-.007-.007-.048-.072-.004-.01-.048-.073-.007-.006-.048-.072-.006-.007-.048-.072-.007-.007-.048-.072-.007-.007-.048-.072-.004-.007-.048-.072-.003-.007-.048-.072-.007-.01-.048-.076-.003-.006-.048-.076-.004-.007-.044-.072-.004-.007-.044-.075-.004-.007-.044-.075-.007-.01-.045-.073-.003-.006-.045-.076-.007-.01-.044-.072-.004-.007-.04-.072-.008-.007-.044-.075-.007-.007-.045-.076-.003-.01-.045-.075-.003-.01-.041-.076-.004-.007-.04-.075-.004-.007-.045-.076-.003-.007-.042-.075-.003-.007-.045-.075-.006-.01-.042-.076-.003-.007-.041-.075-.005-.007-.041-.076-.004-.007-.038-.078-.003-.007-.041-.076-.004-.007-.04-.075-.004-.01-.041-.08-.004-.006-.037-.076-.004-.01-.041-.075-.003-.007-.038-.076-.004-.008-.037-.079-.004-.007-.038-.079-.003-.007-.038-.078-.003-.01-.038-.08-.003-.007-.038-.078-.003-.01-.038-.08-.007-.01-.038-.079-.003-.01-.034-.079-.007-.01-.035-.079-.003-.01-.034-.08-.004-.01-.034-.078-.003-.01-.035-.08-.003-.01-.034-.079-.004-.01-.034-.079-.004-.007-.034-.079-.003-.006-.035-.083-.003-.007-.03-.078-.004-.01-.035-.083-.003-.007-.034-.082-.004-.01-.034-.083-.004-.01-.03-.082-.004-.007-.03-.08-.004-.01-.031-.082-.003-.01-.031-.079L264 96.3l-.03-.082-.004-.009-.03-.082-.004-.01-.031-.083-.004-.01-.03-.079-.004-.01-.03-.082-.004-.007-.031-.082-.003-.007-.031-.083v-.006l-.031-.083-.004-.01-.03-.082-.004-.007-.027-.083-.004-.006-.027-.083-.004-.007-.027-.082-.004-.01-.027-.082-.003-.007-.024-.083-.004-.01-.024-.082-.003-.01-.028-.083-.003-.01-.024-.082-.004-.01-.024-.083-.003-.01-.027-.086v-.01l-.024-.083-.004-.01-.024-.086-.003-.01-.024-.082-.004-.01-.024-.083-.003-.01-.024-.082v-.01l-.024-.086-.004-.01-.024-.086-.003-.01-.02-.083-.004-.007-.024-.086-.003-.01-.021-.086v-.007l-.02-.085-.004-.01-.02-.086v-.01l-.021-.086v-.007l-.02-.086-.004-.01-.02-.086v-.01l-.021-.086-.004-.007-.02-.086-.004-.01-.02-.086-.004-.01-.02-.086-.004-.006-.017-.086v-.034l-.017-.086v-.01l-.017-.086-.004-.01-.017-.086-.003-.007-.017-.086-.004-.01-.017-.086-.003-.01-.018-.086-.003-.01-.017-.086-.004-.01-.013-.086v-.01l-.014-.086-.003-.01-.014-.086v-.01l-.014-.09-.003-.006-.014-.09-.003-.01-.014-.089v-.01l-.014-.086-.003-.01-.01-.086-.002-.007-.014-.089-.003-.01-.01-.09v-.01l-.011-.089v-.022l-.01-.089-.004-.01-.01-.086v-.01l-.01-.09v-.01l-.01-.085v-.007l-.01-.09-.004-.01-.007-.089-.004-.01-.01-.09-.003-.01-.006-.089-.004-.01-.007-.09-.003-.01-.007-.089v-.01l-.007-.086v-.01l-.007-.09v-.01l-.007-.089v-.01l-.006-.09v.008l-.007-.09v-.01l-.004-.089v-.01l-.007-.09v-.01l-.003-.089v-.01l-.004-.09v-.01l-.006-.089v-.01l-.004-.09v-.033l-.003-.09v-.006l-.004-.09v-.01l-.003-.089v-.034l-.004-.09v-.008l-.003-.09v-.01l-.003-.089v-.01l-.004-.09V46.535l.004-.106v-.12l.003-.103v-.124l.003-.103.004-.102v-.01l.003-.104v-.01l.007-.103v-.008l.004-.106v-.01l.006-.107v-.01l.004-.103v-.009l.007-.103v-.01l.007-.103v-.01l.007-.103v-.01l.006-.103v-.01l.007-.104v-.01l.007-.103v-.01l.01-.103.004-.01.007-.103v-.01l.01-.103.003-.01.01-.102v-.01l.009-.102v-.007l.01-.103v-.01l.01-.103v-.01l.01-.104v-.006l.014-.103v-.01l.014-.103v-.01l.014-.104v-.006l.01-.103v-.01l.014-.1v-.01l.013-.1v-.007l.014-.1v-.006l.017-.103v-.01l.014-.1.003-.01.014-.103v-.01l.015-.103.004-.007.013-.1v-.006l.017-.1.004-.01.017-.1v-.007l.017-.1.004-.01.017-.099.003-.01.017-.103v-.01l.02-.1.004-.01.017-.1.004-.009.017-.1.003-.006.021-.1.003-.01.021-.1v-.01l.02-.1.004-.01.02-.099.004-.007.02-.1.004-.008.02-.1v-.01l.025-.1v-.006l.024-.1.003-.01.02-.1.004-.01.024-.1v-.006l.024-.1v-.01l.024-.1v-.01l.024-.099.003-.01.025-.1.003-.007.024-.099.003-.01.024-.1.004-.01.027-.1.002-.01.028-.096.003-.01.028-.1.003-.006.027-.096.004-.007.027-.1.004-.01.027-.096v-.007l.028-.1.003-.01.027-.096.004-.01.027-.1.004-.006.03-.096.004-.01.03-.097.004-.006.031-.096.004-.01.03-.097v-.007l.031-.096.004-.006.034-.096.004-.01.03-.097.004-.01.034-.096v-.007l.031-.096.003-.01.031-.096.004-.01.03-.097.004-.01.034-.096.004-.007.034-.096.003-.007.035-.096.003-.007.034-.092.004-.01.038-.096.003-.007.034-.093.004-.01.034-.096.003-.007.035-.096.003-.01.035-.096.003-.01.038-.097.003-.007.038-.092.003-.007.038-.093.003-.006.038-.093.004-.01.037-.093.004-.007.037-.092.004-.01.041-.097.003-.007.038-.092.007-.01.041-.093.004-.007.04-.092.004-.01.041-.093.004-.007.04-.093.004-.007.041-.092.004-.007.041-.093.004-.007.04-.092.004-.01.041-.093.004-.007.04-.089.004-.01.045-.09.003-.006.045-.09.003-.01.045-.092.003-.01.045-.09.003-.007.045-.089.003-.01.045-.093.003-.007.045-.089.007-.007.044-.092.004-.007.044-.09.004-.01.048-.089.003-.007.045-.089.003-.01.048-.09.004-.006.048-.09.003-.01.048-.089.004-.006.048-.09.006-.007.048-.085.004-.007.048-.09.007-.006.048-.086.003-.007.052-.089.003-.007.051-.089.004-.01.048-.09.007-.006.051-.09.004-.006.051-.086.007-.007.051-.086.004-.01.051-.086.007-.006.052-.086.003-.007.051-.086.004-.007.051-.085.006-.007.052-.086.003-.007.051-.086.004-.006.055-.086.003-.007.055-.086.007-.01.051-.086.007-.007.055-.085.003-.007.055-.086.007-.01.055-.086.004-.007.054-.082.007-.007.055-.082.004-.007.058-.082.003-.01.055-.083.007-.01.055-.086.007-.007.058-.082.007-.007.058-.082.007-.007.058-.083.004-.006.058-.083.007-.007.058-.082.007-.007.058-.079.004-.006.058-.083.007-.007.062-.082.003-.007.058-.079.004-.007.062-.078.006-.01.059-.083.007-.007.061-.082.004-.007.061-.079.007-.007.062-.079.007-.006.062-.083.003-.007.062-.079.007-.006.061-.083.007-.007.065-.078.007-.007.061-.08.007-.006.065-.079.004-.007.065-.079.007-.006.065-.08.007-.006.065-.08.007-.006.065-.075.007-.007.065-.08.007-.006.065-.075.007-.007.065-.076.007-.007.068-.075.007-.007.069-.079.007-.007.065-.075.003-.007.069-.075.007-.007.068-.076.007-.007.069-.075.003-.007.069-.075.007-.007.068-.076.004-.003.068-.072.007-.01.069-.076.007-.007.068-.072.007-.006.072-.073.007-.006.072-.076.007-.007.072-.072.007-.007.072-.075.007-.007.068-.072.007-.007.072-.072.007-.007.072-.072.007-.006.072-.072.007-.007.072-.072.006-.007.073-.072.006-.004.072-.072.007-.007.072-.072.007-.006.076-.072.006-.007.076-.069.007-.007.075-.068.007-.007.075-.069.007-.007.076-.068.007-.007.075-.069.007-.007.075-.068.007-.007.076-.069.007-.006.075-.069.007-.007.075-.065.007-.007.076-.065.006-.007.076-.068.007-.007.079-.065.007-.007.078-.066.007-.006.076-.066.007-.006.078-.066.007-.006.076-.069.007-.007.078-.065.007-.007.08-.065.006-.007.079-.065.007-.007.082-.065.007-.003.079-.062.007-.007.082-.062.007-.007.079-.065.01-.007.082-.065.007-.007.079-.061.007-.004.082-.062.007-.006.079-.062.01-.007.083-.062.006-.007.083-.061.007-.007.082-.062.007-.007.082-.062.007-.003.082-.058.007-.004.082-.061.007-.007.082-.062.007-.007.083-.062.007-.006.085-.059.007-.007.082-.058.007-.007.086-.058.007-.003.086-.059.006-.007.086-.058.01-.003.085-.059.01-.007.086-.058.007-.007.085-.058.007-.003.086-.059.01-.007.086-.054.007-.004.086-.055.007-.007.085-.054.007-.007.09-.059.006-.003.09-.055.006-.003.086-.055.007-.004.089-.055.007-.006.089-.055.007-.004.089-.055.007-.006.089-.055.01-.004.086-.051.007-.004 32.921-19.732h.006l.072-.041.01-.007.072-.041.007-.004.075-.04.007-.004.076-.041.006-.007.076-.041.007-.007.075-.045.007-.003.075-.041.007-.007.076-.041.007-.007.075-.041.007-.007.075-.041.007-.004.076-.04.01-.004.075-.042.007-.003.076-.038.01-.003.075-.038.007-.003.076-.038.007-.007.075-.041.01-.003.076-.038.01-.004.076-.037.01-.004.079-.037.007-.004.075-.038.007-.006.079-.035.007-.007.078-.037.007-.007.076-.034.01-.004.079-.038.007-.003.079-.034.01-.004.075-.038.01-.003.08-.034.01-.004.079-.037.007-.004.078-.038.007-.003.08-.034.006-.004.079-.034.01-.003.08-.035.006-.003.079-.034.01-.004.079-.034.007-.004.079-.03.01-.004.079-.034.01-.004.079-.03.007-.004.079-.034.007-.004.079-.03.006-.004.08-.034.01-.004.078-.03.01-.004.08-.027.006-.004.08-.03.006-.004.079-.028.01-.003.08-.027h.006l.079-.031.01-.004.08-.03.01-.004.078-.027.01-.004.083-.027.007-.004.079-.027.01-.004.079-.027.007-.003.078-.028.007-.003.083-.028.007-.003.078-.028.01-.003.08-.024.01-.003.079-.024.01-.004.079-.027.01-.004.082-.024.007-.003.083-.024.01-.004.079-.024.01-.003.082-.024.01-.003.083-.024.01-.004.08-.024h.006l.082-.02.007-.004.082-.02.01-.004.083-.024.01-.003.08-.02.006-.004.082-.02.01-.004.083-.021h.01l.083-.02.01-.004.082-.02.01-.004.08-.02.01-.004.082-.02.01-.004.082-.02.007-.004.083-.02.01-.004.082-.02h.01L315.47.7l.01-.003.082-.017.01-.004.083-.02h.01l.083-.017.01-.004.082-.017.01-.003.083-.018h.007l.082-.017.01-.003.083-.017.01-.004.082-.013.007-.004.082-.014.01-.003.083-.014.01-.003.082-.014h.01l.083-.014.01-.003.083-.014.01-.003.082-.014h.01l.083-.014h.01l.082-.013h.01l.083-.014h.01l.083-.01.01-.004.082-.01h.007l.082-.01h.01l.086-.01.007-.004.083-.01h.01l.082-.01h.01l.083-.01h.01l.086-.011h.01l.082-.007h.01l.083-.01h.01l.083-.007h.01l.082-.007.01-.004.083-.006h.01l.082-.007h.01l.083-.007h.01l.083-.007.01-.003.082-.007h.007l.082-.007h.01l.086-.007h.01l.083-.007h.007l.082-.007h.007l.082-.003h.01l.083-.004h.01l.086-.003h.014l.082-.007h.01l.082-.003h.01l.083-.004h.01l.083-.003h.195l.082-.004h.556l.006-.074zm-.076 9.26h-.165l-.055.002-.055.003-.055.004-.054.003-.055.003-.055.004-.055.003-.055.004-.055.003-.055.004-.055.003-.054.003-.055.004-.055.003-.055.004-.055.007-.055.006-.055.004-.054.003-.055.007-.055.007-.054.007-.055.007-.054.007-.055.006-.055.007-.054.007-.055.007-.055.007-.054.01-.055.007-.055.01-.055.01-.054.01-.055.008-.055.01-.055.01-.055.01-.055.01-.055.011-.055.01-.054.01-.055.011-.055.01-.055.01-.055.01-.055.015-.055.013-.054.014-.055.014-.055.013-.055.014-.055.01-.055.014-.055.014-.055.014-.054.013-.052.014-.055.014-.055.013-.054.014-.055.017-.052.017-.051.014-.055.017-.055.018-.055.017-.055.017-.055.014-.051.017-.055.017-.055.017-.055.017-.051.017-.055.017-.055.018-.051.017-.052.017-.051.017-.052.02-.054.021-.055.02-.052.021-.051.02-.052.021-.051.021-.051.02-.052.021-.051.02-.052.021-.051.02-.052.021-.051.021-.052.02-.051.025-.051.024-.052.024-.051.024-.048.024-.052.024-.051.024-.052.024-.051.024-.052.024-.051.024-.051.024-.052.027-.051.024-.052.024-.051.028-.052.024-.051.024-.052.027-.048.027-.048.024-.048.028-.051.027-.048.028-.048.027-.048.028-.051.027-.052.027-.048.028-.048.027-.051.028-.048.027-.055.031-32.905 19.736-.068.041-.065.041-.066.041-.065.042-.065.04-.065.038-.062.042-.062.04-.061.042-.062.041-.065.041-.062.041-.062.041-.061.045-.062.041-.062.045-.062.04-.061.046-.062.044-.062.045-.061.044-.062.045-.058.044-.062.045-.058.045-.062.044-.062.045-.058.044-.062.045-.058.048-.059.045-.058.044-.058.048-.058.045-.059.044-.058.048-.058.048-.055.048-.059.048-.054.048-.055.048-.055.048-.055.052-.055.048-.058.048-.055.051-.055.048-.055.048-.055.048-.055.052-.054.051-.055.052-.055.051-.052.051-.054.052-.055.051-.052.052-.051.051-.052.052-.051.051-.052.052-.051.054-.051.052-.052.051-.051.055-.052.055-.048.055-.048.055-.051.055-.048.055-.052.054-.051.055-.048.052-.048.055-.048.054-.048.055-.048.055-.048.055-.048.055-.048.055-.048.055-.048.055-.048.054-.048.055-.045.059-.048.058-.048.058-.044.055-.045.058-.048.059-.045.054-.044.059-.045.058-.044.058-.045.059-.041.058-.045.058-.04.059-.045.058-.045.058-.045.058-.044.059-.041.062-.041.058-.045.058-.041.062-.041.062-.041.061-.042.062-.04.062-.042.061-.041.062-.041.062-.041.062-.041.061-.042.062-.037.062-.038.062-.038.061-.038.062-.037.062-.038.061-.038.062-.037.062-.038.065-.038.062-.038.062-.034.061-.038.065-.037.066-.035.061-.037.065-.035.066-.034.065-.034.065-.035.065-.034.065-.034.065-.034.066-.035.065-.03.065-.031.065-.031.065-.031.065-.031.066-.03.068-.032.065-.03.065-.031.066-.031.068-.031.065-.031.065-.03.07-.032.064-.027.069-.028.068-.027.066-.028.068-.027.069-.027.068-.028.065-.027.07-.028.068-.027.068-.024.069-.024.068-.028.069-.027.069-.027.068-.024.069-.028.068-.024.069-.024.069-.024.072-.024.068-.024.069-.024.068-.024.069-.024.072-.024.068-.024.073-.024.068-.02.069-.021.072-.02.072-.021.068-.02.072-.021.072-.018.072-.017.072-.02.072-.017.072-.021.072-.017.072-.017.072-.017.072-.018.072-.017.072-.017.073-.017.072-.017.072-.014.072-.017.072-.014.072-.017.072-.017.072-.014.072-.013.072-.018.072-.013.075-.014.072-.014.072-.014.072-.013.072-.014.072-.01.072-.01.072-.01.072-.011.076-.014.072-.01.072-.01.072-.01.072-.011.072-.01.072-.01.075-.007.076-.01.072-.008.075-.01.072-.007.076-.007.075-.006.072-.007.076-.007.075-.007.075-.007.076-.007.075-.007.076-.006.075-.004.076-.003.075-.007.076-.004.075-.003.075-.003.076-.004.075-.003.072v.076l-.004.075v.151l-.003.076v41.55l.003.061.004.062.003.062.004.058.003.062.003.061.004.062.003.062.004.058.003.062.004.058.003.062.003.062.004.058.003.058.004.058.003.062.007.058.007.059.007.058.007.058.006.062.007.062.007.058.007.062.007.058.007.058.01.059.01.058.01.062.007.058.007.062.01.058.01.058.011.059.01.058.01.058.011.059.01.061.01.059.01.058.011.058.01.059.01.058.015.058.013.055.014.055.014.062.013.058.014.058.014.058.014.059.013.055.014.058.014.055.013.058.018.058.013.059.017.058.018.058.013.059.017.054.018.059.017.058.017.055.017.058.017.059.017.058.017.058.018.058.017.055.017.059.017.058.017.055.02.055.021.055.02.058.018.055.02.055.021.058.02.055.021.055.02.054.022.055.024.055.02.055.024.055.02.055.021.055.02.055.025.054.024.055.024.055.024.055.024.055.024.055.02.055.024.055.024.05.024.056.024.055.028.055.024.051.024.051.027.055.028.055.024.052.027.051.028.051.027.055.028.052.027.051.027.055.028.052.027.051.028.051.027.052.028.051.027.052.027.051.031.052.031.051.028.052.027.051.03.051.032.052.03.051.031.048.031.052.031.051.031.052.03.048.035.048.031.051.031.052.03.05.032.049.034.048.034.051.035.048.034.052.03.048.032.051.034.048.034.048.034.048.035.048.034.048.034.048.035.048.037.048.035.048.034.045.038.048.034.048.034.045.038.044.038.048.037.045.038.044.034.045.038.045.038.044.034.048.038.045.038.044.037.045.038.044.038.045.038.045.04.044.038.045.038.044.041.045.038.044.041.045.041.045.038.044.041.041.041.045.038.041.041.041.041.045.041.044.042.045.04.041.042.041.041.041.041.042.045.04.041.042.041.041.045.041.04.041.045.042.042.04.044.038.045.042.044.037.045.041.045.042.044.037.045.038.044.038.045.037.044.038.045.038.048.041.048.038.045.038.044.037.045.038.044.038.048.037.045.038.045.038.044.038.048.037.045.035.048.037.048.035.048.034.048.034.048.038.048.038.048.034.048.038.048.034.048.034.051.038.048.034.048.035.052.034.051.034.048.034.048.031.052.031.048.031.051.034.048.031.052.034.051.031.051.035.052.03.051.031.052.031.051.031.052.031.051.03.058.035 35.905 20.991.045.024.041.02.041.025.042.024.04.02.038.02.042.025.04.02.042.024.041.021.038.02.04.021.042.024.041.02.041.021.041.02.038.021.041.021.042.02.04.021.042.02.041.021.041.017.041.021.041.017.042.017.04.018.042.017.041.017.041.017.041.017.041.017.042.017.04.018.042.017.045.017.04.017.042.017.041.017.041.017.041.014.041.014.042.017.044.017.041.017.041.014.042.014.044.013.041.014.042.014.04.014.038.01.09.027.085.028.086.027.086.028.085.024.086.024.086.024.086.02.085.02.086.021.086.02.086.022.085.017.086.02.09.017.088.018.086.013.09.014.085.014.086.013.086.014.089.014.085.014.09.01.085.01.086.01.09.007.085.007.09.007.085.007.089.007.09.003.088.004.09.003.089.003.085.004.09.003h.089l.089.004h.089l.086-.004h.089l.09-.003.088-.004.086-.003.09-.003.088-.004.09-.003.089-.007.085-.007.09-.007.085-.01.09-.01.085-.01.09-.011.088-.01.086-.014.09-.014.085-.013.086-.014.086-.014.085-.014.09-.017.089-.017.085-.017.086-.02.09-.021.088-.02.086-.021.09-.02.085-.025.086-.024.086-.024.085-.024.086-.024.09-.027.04-.01.045-.014.041-.014.041-.014.041-.014.042-.013.04-.014.042-.014.041-.013.041-.018.041-.013.041-.014.042-.017.04-.014.042-.017.041-.014.041-.013.045-.014.04-.014.042-.017.041-.017.041-.017.042-.017.04-.018.042-.017.041-.017.041-.017.041-.017.041-.02.042-.021.04-.017.042-.018.041-.02.041-.02.041-.021.041-.02.042-.018.04-.02.042-.021.041-.02.041-.018.041-.02.042-.021.037-.02.041-.021.042-.02.04-.021.042-.024.038-.021.04-.02.042-.025.044-.024 39.757-22.14.044-.023.042-.024.04-.024.045-.024.041-.024.042-.024.04-.024.042-.024.041-.024.041-.024.041-.028.041-.024.042-.027.04-.028.042-.024.041-.024.038-.024.038-.027.04-.028.038-.027.038-.028.041-.027.041-.027.038-.028.038-.024.041-.027.041-.028.041-.027.038-.028.038-.027.038-.027.037-.031.041-.028.038-.03.034-.028.038-.03.038-.028.038-.031.037-.031.038-.03.034-.028.035-.031.037-.031.038-.03.038-.032.037-.03.035-.032.038-.03.037-.031.035-.031.034-.031.038-.03.034-.032.034-.03.034-.035.035-.03.034-.032.038-.03.034-.031.034-.031.035-.031.034-.03.034-.035.034-.031.035-.034.034-.035.034-.034.035-.034.034-.035.03-.03.035-.035.031-.034.03-.034.032-.035.03-.034.031-.034.031-.034.031-.035.034-.037.031-.035.031-.034.031-.034.03-.035.032-.034.03-.038.031-.034.031-.038.031-.034.031-.038.028-.034.03-.034.031-.035.028-.037.03-.035.031-.037.028-.035.027-.034.028-.038.027-.037.028-.038.03-.038.028-.038.027-.037.028-.038.027-.038.028-.037.027-.038.027-.041.028-.038.027-.038.028-.037.024-.038.024-.038.027-.041.028-.038.024-.038.027-.04.028-.038.024-.042.027-.04.024-.038.024-.038.024-.038.024-.041.027-.041.024-.038.024-.041.024-.041.021-.041.02-.041.025-.038.02-.041.024-.042.024-.04.02-.042.025-.041.02-.041.02-.041.022-.041.02-.042.02-.04.021-.045.02-.041.021-.042.021-.044.02-.041.021-.041.02-.045.018-.041.02-.041.021-.045.017-.045.017-.04.017-.045.018-.041.02-.042.017-.044.02-.045.018-.04.02-.045.018-.045.017-.045.02-.04.018-.042.017-.045.017-.044.017-.041.017-.045.017-.044.018-.045.013-.045.017-.044.018-.045.017-.04.017-.045.017-.045.014-.045.013-.044.018-.045.013-.044.017-.045.014-.045.014-.044.014-.045.017-.048.013-.044.014-.045.014-.044.014-.042.013-.044.014-.048.01-.045.014-.04.014-.049.01-.048.01-.045.01-.048.014-.044.01-.048.014-.045.01-.048.01-.044.011-.045.01-.044.01-.045.011-.045.01-.044.01-.048.007-.045.01-.048.011-.048.01-.048.01-.044.007-.048.007-.048.01-.045.007-.048.007-.048.007-.045.007-.044.007-.048.007-.048.007-.045.007-.048.006-.048.007-.044.007-.048.007-.048.007-.048.007-.045.007-.051.003-.048.007-.045.003-.045.007-.048.004-.048.003-.048.003-.048.004-.048.003-.048.004-.048v-.1l.003-.047.004-.048.003-.048.003-.048.004-.045.003-.051.004-.048v-.048l.003-.048V42.313l-.003-.045v-.058l-.004-.045v-.086l-.003-.044-.004-.045-.003-.044-.003-.045-.004-.045-.003-.044-.004-.045-.003-.044-.004-.045-.003-.041-.003-.045-.004-.04-.003-.042-.004-.041-.007-.045-.003-.048-.003-.04-.004-.045-.007-.045-.007-.045-.003-.04-.007-.045-.007-.041-.007-.045-.006-.045-.007-.04-.007-.042-.007-.041-.007-.041-.007-.041-.007-.042-.007-.04-.006-.038-.018-.09-.017-.085-.017-.083-.02-.085-.021-.083-.017-.082-.02-.086-.021-.085-.02-.083-.025-.082-.024-.082-.024-.083-.024-.082-.027-.082-.028-.083-.024-.082-.024-.082-.027-.082-.031-.083-.027-.079-.028-.082-.027-.079-.031-.079-.031-.079-.03-.078-.032-.08-.034-.078-.03-.08-.035-.078-.035-.079-.034-.075-.038-.08-.037-.075-.038-.075-.038-.078-.04-.076-.039-.079-.04-.075-.042-.075-.041-.076-.041-.074-.041-.075-.045-.076-.045-.072-.044-.072-.045-.072-.044-.075-.048-.072-.048-.072-.048-.072-.048-.072-.048-.072-.048-.069-.052-.069-.051-.068-.052-.069-.051-.068-.055-.069-.051-.067-.052-.068-.055-.066-.055-.068-.054-.065-.055-.065-.055-.066-.058-.065-.059-.065-.058-.062-.058-.061-.059-.066-.058-.061-.058-.062-.059-.062-.061-.061-.062-.062-.058-.058-.062-.062-.065-.059-.065-.058-.066-.058-.061-.058-.065-.059-.066-.055-.068-.058-.031-.027-.034-.031-.035-.028-.034-.027-.034-.027-.034-.028-.035-.027-.03-.028-.035-.027-.034-.028-.035-.027-.034-.028-.034-.027-.034-.024-.035-.024-.034-.027-.034-.024-.038-.024-.034-.028-.038-.027-.034-.024-.038-.028-.034-.027-.038-.024-.038-.024-.034-.024-.038-.024-.034-.024-.038-.024-.038-.024-.037-.024-.038-.024-.038-.024-.034-.024-.038-.024-.037-.02-.038-.025-.038-.024-.038-.024-.037-.024-.038-.02-.038-.024-.037-.024-.042-.024-38.93-22.281-.054-.031-.051-.027-.052-.028-.048-.027-.051-.028-.048-.027-.052-.028-.051-.027-.052-.027-.051-.028-.052-.027-.051-.024-.051-.024-.048-.027-.052-.024-.051-.024-.052-.024-.051-.024-.052-.024-.051-.024-.052-.024-.048-.024-.051-.024-.051-.024-.052-.024-.051-.02-.052-.024-.051-.021-.052-.024-.051-.02-.055-.021-.051-.02-.052-.021-.051-.02-.052-.021-.051-.021-.052-.02-.051-.021-.051-.02-.055-.018-.052-.017-.051-.017-.052-.017-.051-.017-.055-.018-.055-.017-.051-.017-.052-.017-.051-.017-.052-.017-.051-.018-.055-.017-.055-.014-.055-.018-.051-.017-.055-.017-.055-.017-.055-.014-.055-.017-.051-.014-.055-.013-.051-.017-.055-.014-.054-.014-.055-.014-.054-.013-.054-.014-.054-.014-.052-.013-.051-.01-.055-.014-.055-.01-.055-.01-.051-.014-.055-.01-.055-.014-.055-.01-.051-.01-.055-.01-.055-.011-.055-.01-.055-.01-.055-.011-.055-.01-.054-.008-.055-.01-.055-.007-.055-.01-.052-.007-.054-.007-.055-.007-.055-.01-.055-.01-.055-.007-.055-.007-.055-.007-.054-.007-.055-.006-.055-.007-.055-.007-.055-.004-.051-.006-.055-.004-.055-.007-.055-.007-.055-.006-.055-.004-.055-.007-.054-.003-.055-.004h-.055l-.055-.003-.055-.003-.055-.004-.055-.003-.055-.004-.054-.003h-.055l-.055-.004-.055-.003h-.11l-.055-.003h-.164l.045.014z" fill="#00a88a"/><path d="M355.597 65.538c0 16.182-3.067 20.896-9.197 27.635-6.128 6.739-14.255 9.454-24.382 9.454-9.256.0-16.718-3.284-23.186-8.995-7.16-6.326-12.373-16.131-12.373-27.073.0-10.758 4.02-20.057 11.142-26.981 6.248-6.059 15.549-10.006 25.14-10.006 11.578.0 23.601 5.48 31.012 16.3l-10.881 9.197c-4.613-8.583-11.297-11.474-20.858-11.749-10.572-.301-22.211 9.784-21.347 23.234.48 7.524 3.995 20.456 20.905 22.83 8.885 1.244 18.985-8.238 18.985-13.389h-16.69v-12.86h31.724v2.394l.006.01z" fill="#fff" fill-rule="nonzero"/><path d="M507.384 84.704V42.76c0-5.43-2.91-10.444-7.624-13.139L460.78 7.345a18.046 18.046.0 00-18.238.192l-32.915 19.742a23.009 23.009.0 00-11.17 19.729v40.987a19.24 19.24.0 009.53 16.612c10.268 6.005 26.414 15.439 35.91 20.99a15.635 15.635.0 0015.5.162l39.76-22.143a16.008 16.008.0 008.22-13.989v-4.92l.007-.003z" fill="#ebb951"/><path d="M451.869.336h.092l.084.001h.006l.083.002h.01l.082.001h.01l.083.002h.007l.082.002h.007l.082.003h.007l.082.003h.01l.083.004h.01l.083.003h.01l.082.004h.01l.083.004h.007l.082.003h.007l.082.003h.01l.083.004h.01l.086.003h.01l.082.007h.01l.083.004h.01l.082.006h.01l.083.007h.01l.083.007h.01l.082.007h.01l.083.007h.007l.082.007h.01l.083.01h.006l.083.007h.01l.082.007h.01l.083.01h.01l.083.01.01.004.082.01h.01l.083.01h.01l.086.01h.007l.082.01h.01l.083.011h.006l.083.01h.01l.082.01h.007l.082.011h.01l.083.014h.01l.083.01.006.003.083.014h.01l.082.014.01.003.083.014h.01l.083.014h.01l.082.013h.01l.083.014h.01l.082.014h.01l.083.017.01.003.083.014h.01l.082.017.01.004.083.017.01.003.082.017.01.004.083.017.01.003.082.018.01.003.083.017h.01l.083.017h.007l.082.02h.01l.082.018.01.004.08.02.006.004.083.02h.01l.082.02.01.004.08.02.01.004.079.02.01.004.082.02h.01l.08.021h.01l.082.024h.007l.082.024h.01l.083.024.01.004.082.024h.01l.08.024h.01l.079.024.01.003.079.024.01.004.079.024.01.003.08.024.01.003.078.028.007.003.079.024.01.004.083.027.007.004.082.027.01.003.082.028.01.003.08.028h.006l.08.027.01.004.078.027.007.004.08.027.006.003.079.028.01.003.08.028.01.003.078.028.007.003.079.03.007.004.079.028.01.003.079.031.007.003.079.031.01.004.079.03.007.004.079.03.01.004.079.031.007.004.078.03.01.004.08.03.01.004.079.031.01.003.079.031.007.007.079.031.006.003.08.031.006.004.08.034.01.004.078.034.007.003.079.035.007.003.075.034.007.004.079.037.007.004.079.034.007.004.075.034.007.003.075.038.01.003.076.038.01.004.076.037.01.004.079.037.01.007.08.038.006.003.075.038.01.004.076.037.01.004.076.037.01.004.076.038.01.003.079.041.01.004.076.037.006.007.076.041.007.004.075.041.01.007.076.041.01.003.076.042.006.006.076.042.007.006.075.042.007.003.076.041.01.007.075.041.01.007.076.045 38.985 22.29.065.038.007.007.061.038.007.003.065.038.01.003.062.038.007.003.062.038.01.003.062.038.007.004.062.037.01.004.062.038.007.006.061.042.01.006.062.038.007.004.062.04.007.004.061.038.007.003.062.038.007.007.062.041.007.003.058.042.01.006.062.042.007.003.061.038.01.007.059.04.007.008.058.04.01.004.059.041.007.007.058.041.007.004.058.04.007.008.058.04.007.008.058.041.01.007.059.044.007.007.058.041.007.004.058.041.007.007.058.041.007.007.059.041.006.003.059.042.007.006.058.045.007.003.058.045.007.007.055.044.007.004.058.044.007.007.058.045.01.007.055.04.007.004.055.045.007.007.055.044.007.007.058.048.007.007.055.045.006.007.055.044.007.007.055.048.007.007.055.048.007.007.055.048.01.01.106.093.014.013.106.093.014.014.11.096.013.01.107.096.013.014.107.096.013.013.107.096.013.014.107.1.013.013.103.1.014.014.103.099.014.014.102.1.014.013.103.103.014.013.1.103.013.014.1.103.013.014.096.103.014.013.096.103.014.014.096.106.013.014.096.106.014.014.096.106.01.014.093.11.01.013.093.107.014.013.092.107.014.013.092.11.01.014.093.11.014.017.09.11.01.013.089.113.01.014.09.113.01.017.085.114.01.013.086.113.01.018.083.113.01.014.082.113.01.013.083.117.01.017.08.117.01.013.082.117.01.017.079.117.01.014.08.12.01.017.075.116.01.017.076.12.01.014.075.12.01.017.076.12.01.018.076.123.01.014.072.123.01.017.072.124.01.017.073.12.006.017.07.127.006.017.069.124.006.017.069.123.01.017.065.124.01.017.066.127.007.017.065.127.01.017.065.127.007.017.065.127.007.017.062.127.007.017.062.127.01.017.058.127.007.017.058.13.01.018.059.13.007.017.055.13.006.018.055.13.007.017.055.13.007.017.051.134.007.017.052.134.006.017.052.134.007.02.048.134.007.017.048.134.007.017.048.134.006.017.048.134.004.017.044.134.007.017.045.134.007.017.044.137.004.017.04.134.004.017.045.137.003.017.038.138.007.017.038.137.003.017.038.137.007.017.037.138.004.017.038.14.003.02.034.138.007.02.034.138.004.02.03.141.004.017.031.137.003.02.031.141.004.018.027.14.004.02.027.141.003.018.028.14v.017l.014.072.003.007.014.069.003.01.014.072.003.01.01.069.004.01.01.072v.007l.014.072v.01l.01.072v.01l.01.073v.007l.01.072.004.01.01.072v.01l.01.072.004.01.01.073v.013l.01.072.004.007.01.072v.007l.01.072v.01l.011.069v.007l.01.072.004.01.007.072v.01l.007.072v.01l.006.073v.01l.007.072v.01l.007.072v.01l.007.073v.01l.007.072.003.01.007.072v.01l.007.073v.006l.003.072v.01l.007.073v.007l.004.072v.013l.003.076v.007l.004.072v.01l.006.072v.014l.004.072v.01l.003.072v.007l.004.072v.01l.003.076v.089l.004.075v.01l.003.072v.093l.004.072v47.438l-.004.08v.092l-.003.075v.01l-.004.076v.01l-.003.08v.01l-.004.078v.01l-.003.08-.004.01-.003.079v.01l-.003.079v.01l-.007.078v.007l-.004.078v.013l-.007.08v.01l-.006.077v.01l-.007.076v.01l-.007.076v.014l-.007.079v.01l-.007.079-.003.01-.007.075-.004.01-.006.076v.01l-.007.076v.01l-.007.079v.007l-.007.075-.003.01-.007.08-.004.01-.01.075v.01l-.01.076v.01l-.01.076v.01l-.01.075v.01l-.011.08-.004.01-.01.075v.01l-.01.076-.004.01-.01.076v.007l-.01.075-.004.01-.01.076v.01l-.01.076-.004.01-.01.079-.003.013-.014.076v.007l-.014.075-.003.01-.014.076v.01l-.014.076v.007l-.013.075v.01l-.014.076-.003.01-.014.072-.004.01-.017.076v.007l-.017.075-.003.01-.014.076-.003.007-.018.072v.01l-.017.075v.01l-.017.076-.003.01-.014.076v.007l-.017.075-.004.01-.017.076v.007l-.017.075-.003.01-.017.076-.004.01-.017.076-.003.01-.018.072-.003.007-.017.072-.004.01-.02.076-.004.01-.02.072-.004.007-.02.075-.004.01-.02.072-.004.01-.02.073-.004.01-.02.072v.01l-.02.072-.004.01-.02.073-.004.007-.02.072-.004.006-.024.076-.004.01-.024.076v.006l-.02.072-.004.007-.024.072-.003.01-.024.073-.004.007-.024.072-.003.01-.024.072-.003.01-.024.072-.004.007-.024.069-.003.01-.024.072-.004.01-.024.072-.003.01-.024.073-.004.01-.027.072-.003.007-.028.072-.003.01-.028.069-.003.006-.028.072-.003.007-.024.072-.004.01-.027.07-.003.01-.028.071-.003.007-.028.072-.003.01-.028.073-.003.007-.027.068-.004.01-.027.072-.004.01-.027.073-.004.01-.03.069-.004.006-.027.069-.004.01-.027.072-.004.007-.03.069-.004.01-.027.069-.004.01-.03.069-.004.006-.03.069-.004.01-.031.069-.004.01-.03.069-.004.01-.03.069-.004.006-.031.07-.004.01-.03.068-.004.007-.03.068-.004.007-.034.069-.004.01-.03.069-.004.007-.034.068-.004.01-.034.069-.004.01-.034.069-.003.007-.035.065-.003.007-.034.068-.004.007-.034.069-.007.01-.034.069-.007.007-.034.068-.004.007-.034.065-.003.007-.035.065-.003.01-.034.066-.007.006-.035.066-.006.01-.035.065-.003.01-.038.069-.003.007-.038.065-.007.01-.038.065-.003.01-.034.066-.007.007-.038.065-.003.01-.038.065-.004.007-.037.065-.004.007-.037.065-.007.007-.038.065-.007.01-.041.066-.003.01-.038.065-.007.007-.038.065-.003.007-.038.065-.003.007-.041.062-.007.01-.042.062-.003.007-.038.061-.007.007-.04.065-.008.007-.04.065-.004.007-.041.062-.007.007-.041.062-.007.007-.041.065-.007.007-.041.061-.007.01-.041.062-.004.007-.04.062-.004.007-.041.065-.004.007-.041.061-.007.007-.041.062-.003.01-.045.062-.003.007-.045.062-.004.007-.044.061-.004.007-.044.062-.004.007-.044.061-.007.01-.045.062-.006.007-.045.059-.007.006-.044.059-.007.007-.048.061-.007.01-.045.062-.007.007-.044.062-.007.007-.045.062-.006.006-.045.062-.007.007-.044.058-.007.007-.048.062-.004.007-.044.058-.007.007-.048.062-.007.006-.048.059-.007.007-.048.058-.007.007-.048.058-.007.007-.048.058-.007.007-.048.055-.003.007-.048.058-.007.01-.048.059-.007.007-.048.058-.007.007-.048.055-.007.006-.048.055-.006.007-.048.058-.007.007-.052.059-.007.006-.051.055-.007.007-.051.055-.007.007-.052.055-.006.007-.052.058-.007.007-.051.058-.004.007-.051.055-.007.01-.051.055-.007.007-.052.055-.006.007-.055.054-.007.007-.052.055-.007.007-.051.055-.007.007-.055.051-.007.007-.054.055-.007.007-.055.055-.007.007-.055.054-.007.007-.055.052-.006.007-.055.054-.007.007-.055.052-.007.003-.055.052-.007.006-.055.055-.006.007-.055.052-.007.007-.055.05-.007.008-.055.051-.01.007-.058.052-.007.006-.055.052-.01.007-.059.051-.006.007-.055.051-.007.004-.055.051-.007.007-.055.052-.007.006-.058.048-.007.007-.058.052-.007.007-.058.051-.007.007-.058.051-.007.007-.059.048-.006.007-.059.052-.007.006-.058.048-.007.007-.058.048-.01.007-.059.048-.006.007-.059.048-.007.007-.058.048-.007.007-.062.048-.006.003-.059.048-.007.007-.061.048-.007.007-.062.045-.007.006-.061.045-.007.007-.062.048-.007.007-.062.044-.006.007-.059.045-.007.003-.061.045-.007.007-.062.044-.007.007-.065.045-.007.006-.062.045-.006.003-.066.045-.01.007-.062.044-.006.004-.066.041-.006.007-.066.044-.006.004-.062.045-.01.006-.066.045-.01.007-.065.044-.007.004-.062.041-.007.007-.065.044-.007.007-.065.045-.007.007-.065.044-.007.004-.065.044-.007.004-.065.044-.007.007-.065.041-.007.007-.065.041-.007.007-.065.041-.007.004-.065.037-.01.007-.065.042-.01.003-.066.041-.007.007-.065.041-.01.004-.065.04-.007.004-.069.038-.01.003-.069.038-.006.003-.069.038-.01.004-.069.037-.01.004-.069.037-.01.004-.069.038-.006.006-.066.038-.003.004-39.767 22.146-.003.004-.065.034-.01.007-.066.034-.007.004-.065.037-.01.004-.065.034-.007.003-.065.038-.007.007-.065.034-.01.007-.069.034-.01.004-.065.034-.007.004-.065.034-.01.003-.07.031-.006.004-.065.03-.007.004-.065.034-.01.007-.07.03-.006.004-.069.035-.01.003-.065.031-.007.003-.065.031-.01.004-.069.03-.007.004-.068.03-.01.008-.07.03-.01.004-.068.03-.01.004-.07.031-.01.004-.068.03-.01.004-.065.027-.01.004-.066.027-.01.004-.069.03-.007.004-.065.027-.01.004-.069.027-.006.004-.07.03-.01.004-.068.027-.007.004-.068.027-.01.004-.07.027-.006.004-.069.027-.01.003-.069.028-.006.003-.066.028-.006.003-.07.028-.01.003-.068.024-.01.003-.069.024-.01.004-.069.027-.007.004-.068.024-.007.003-.069.024-.006.004-.069.024-.01.003-.072.024-.007.003-.069.024-.01.004-.069.024-.01.003-.069.024-.01.004-.068.024-.007.003-.069.024-.01.004-.069.02-.01.004-.069.02-.006.004-.069.024-.072.02-.01.004-.072.02-.014.004-.137.04-.017.008-.141.04-.017.008-.14.037-.018.004-.14.038-.021.003-.14.038-.018.003-.14.038-.018.003-.14.035-.017.003-.14.034-.018.007-.14.031-.018.003-.14.031-.021.004-.14.03-.018.004-.14.027-.017.004-.141.027-.017.004-.144.027-.017.004-.144.027-.018.003-.14.024-.017.004-.144.02-.021.004-.144.024-.017.003-.144.02-.017.004-.144.02-.017.004-.145.02h-.017l-.14.018h-.017l-.144.017-.021.004-.144.013-.02.004-.145.014-.02.003-.144.01-.017.004-.144.01-.018.003-.144.01h-.017l-.144.011h-.014l-.144.01-.02.004-.144.007-.017.003-.144.007-.018.003-.144.004h-.02l-.144.003h-.017l-.144.004h-.501l-.144-.004h-.017l-.144-.003h-.017l-.144-.004h-.018l-.144-.007h-.017l-.144-.006-.017-.004-.144-.007h-.017l-.144-.01h-.017l-.144-.01h-.02l-.145-.01h-.017l-.144-.014h-.02l-.145-.014h-.017l-.144-.014h-.017l-.144-.017h-.017l-.144-.017-.017-.003-.144-.018h-.018l-.144-.02-.017-.004-.144-.02-.02-.004-.144-.02-.02-.004-.142-.02-.017-.004-.14-.024-.017-.003-.144-.024-.018-.004-.144-.027-.017-.003-.144-.028-.017-.003-.14-.031-.021-.004-.14-.027-.021-.007-.144-.03-.018-.004-.14-.031-.017-.003-.14-.031-.021-.004-.141-.034-.017-.003-.14-.035-.018-.007-.14-.037-.018-.004-.14-.037-.017-.004-.141-.038-.017-.006-.14-.042-.018-.006-.14-.042-.018-.007-.14-.04-.017-.008-.138-.044-.02-.007-.137-.045-.014-.003-.072-.024-.01-.003-.069-.024-.007-.004-.068-.024-.01-.003-.07-.024-.01-.004-.068-.024-.01-.003-.069-.024-.01-.004-.069-.024-.01-.003-.069-.027-.007-.004-.068-.024-.007-.003-.069-.024-.007-.004-.068-.024-.007-.003-.069-.028-.01-.003-.068-.024-.01-.003-.07-.028-.01-.003-.068-.028-.01-.003-.07-.028-.006-.003-.069-.028-.01-.003-.068-.027-.01-.004-.07-.027-.01-.004-.068-.027-.007-.004-.069-.027-.006-.003-.069-.028-.007-.003-.068-.031-.01-.004-.066-.03-.01-.004-.069-.03-.01-.004-.065-.031-.01-.003-.07-.028-.01-.003-.065-.031-.01-.004-.065-.03-.007-.004-.065-.03-.007-.004-.068-.031-.007-.007-.065-.03-.01-.004-.07-.034-.006-.004-.069-.034-.007-.007-.065-.034-.007-.004-.068-.034-.007-.004-.065-.034-.01-.003-.066-.035-.01-.003-.065-.034-.01-.004-.066-.034-.01-.003-.065-.035-.01-.003-.066-.034-.01-.004-.065-.034-.01-.004-.065-.037-.007-.004-.065-.034-.007-.003-.065-.038-.007-.004-.065-.034-.007-.007-.065-.037-.007-.007-.065-.038-.007-.003-.065-.038-.01-.004-.066-.037-.01-.004-.062-.034-.003-.004-35.916-20.99h-.007l-.075-.045-.01-.007-.08-.048-.01-.003-.075-.045-.01-.007-.08-.044-.01-.004-.078-.048-.01-.003-.076-.048-.007-.004-.075-.048-.01-.003-.076-.048-.007-.007-.075-.048-.007-.007-.076-.048-.007-.007-.075-.048-.007-.003-.075-.052-.01-.003-.076-.048-.007-.003-.075-.052-.007-.003-.072-.048-.007-.007-.076-.052-.01-.007-.075-.048-.007-.006-.076-.048-.01-.007-.072-.052-.007-.007-.072-.048-.007-.003-.072-.051-.01-.007-.072-.052-.007-.007-.072-.051-.007-.007-.072-.051-.007-.007-.072-.052-.006-.003-.072-.052-.007-.003-.069-.051-.01-.007-.069-.055-.007-.007-.068-.051-.007-.004-.072-.055-.007-.007-.069-.054-.006-.004-.069-.055-.007-.003-.068-.055-.007-.003-.069-.055-.01-.007-.069-.055-.007-.007-.068-.055-.007-.007-.069-.055-.007-.003-.068-.058-.007-.007-.069-.055-.006-.007-.069-.055-.007-.003-.068-.058-.007-.007-.065-.055-.007-.007-.065-.058-.01-.007-.07-.058-.006-.007-.065-.059-.007-.006-.065-.059-.007-.003-.065-.059-.007-.003-.065-.058-.007-.004-.065-.058-.01-.007-.066-.058-.007-.007-.065-.058-.007-.004-.065-.061-.007-.007-.061-.059-.007-.006-.065-.062-.007-.007-.066-.062-.006-.007-.066-.058-.006-.007-.062-.062-.007-.006-.062-.062-.007-.007-.061-.062-.007-.007-.062-.061-.007-.004-.061-.061-.007-.007-.062-.062-.007-.007-.062-.062-.006-.006-.062-.062-.007-.007-.058-.062-.007-.007-.062-.061-.007-.004-.058-.065-.007-.01-.062-.065-.007-.007-.061-.065-.007-.007-.062-.062-.007-.007-.058-.065-.003-.007-.062-.065-.007-.007-.058-.065-.007-.007-.059-.062-.006-.006-.059-.066-.007-.01-.058-.065-.007-.007-.058-.065-.007-.007-.058-.065-.007-.007-.055-.065-.007-.007-.058-.069-.007-.006-.058-.066-.007-.01-.058-.065-.007-.007-.055-.065-.003-.007-.055-.069-.007-.006-.055-.069-.007-.007-.055-.065-.003-.007-.055-.068-.007-.007-.055-.065-.007-.007-.055-.069-.006-.007-.055-.068-.007-.007-.055-.069-.003-.007-.052-.068-.007-.007-.051-.069-.007-.007-.055-.068-.007-.007-.051-.069-.006-.006-.055-.069-.004-.007-.051-.068-.004-.007-.053-.069-.004-.007-.051-.068-.007-.01-.052-.073-.003-.006-.051-.07-.004-.01-.051-.071-.007-.007-.052-.072-.006-.007-.052-.072-.007-.007-.051-.072-.004-.01-.048-.072-.007-.007-.048-.072-.003-.007-.048-.072-.007-.007-.048-.069-.003-.006-.048-.072-.004-.01-.048-.073-.003-.007-.048-.072-.007-.01-.048-.072-.003-.007-.048-.075-.007-.007-.048-.072-.004-.007-.048-.072-.003-.007-.048-.072-.007-.007-.048-.072-.004-.01-.044-.072-.004-.007-.044-.072-.006-.007-.044-.075-.007-.007-.045-.072-.003-.007-.045-.072-.006-.007-.045-.072-.003-.007-.045-.075-.003-.007-.045-.072-.007-.007-.045-.075-.003-.01-.041-.076-.004-.007-.044-.075-.007-.007-.041-.076-.004-.006-.044-.076-.004-.01-.044-.076-.004-.006-.04-.076-.008-.007-.04-.075-.004-.007-.041-.076-.004-.01-.04-.079-.004-.007-.041-.075-.004-.007-.041-.075-.003-.01-.038-.08-.004-.006-.037-.076-.004-.007-.04-.075-.004-.007-.042-.079-.003-.007-.038-.079-.003-.006-.038-.08-.003-.01-.041-.078-.004-.01-.038-.08-.003-.01-.038-.079-.003-.007-.038-.075-.003-.01-.038-.08-.003-.006-.038-.079-.004-.007-.037-.079-.004-.01-.034-.079-.003-.01-.035-.079-.003-.01-.038-.08-.003-.006-.035-.082-.003-.007-.034-.08-.004-.006-.034-.082-.004-.01-.034-.08-.003-.006-.035-.08-.003-.006-.034-.079-.004-.01-.034-.082-.003-.007-.035-.08-.003-.01-.034-.082-.004-.01-.03-.079v-.007l-.032-.082-.003-.01-.034-.083-.004-.01-.03-.082-.004-.01-.03-.083-.004-.007-.031-.082v-.007l-.031-.082-.003-.01-.031-.083-.004-.01-.03-.082-.004-.007-.028-.083-.003-.01-.027-.082-.004-.007-.03-.082-.004-.01-.028-.083-.003-.01-.027-.083-.004-.006-.027-.083-.004-.01-.027-.086-.004-.007-.027-.082-.003-.007-.024-.082-.004-.01-.024-.083-.003-.01-.028-.082-.003-.01-.028-.083-.003-.01-.024-.086v-.01l-.024-.086-.003-.01-.024-.082v-.015l-.028-.082-.003-.01-.024-.086-.004-.01-.024-.082-.003-.007-.024-.083-.004-.009-.02-.082-.004-.01-.024-.086v-.01l-.024-.083-.003-.01-.024-.086-.004-.007-.02-.082-.004-.01-.02-.086-.004-.007-.02-.086-.004-.01-.02-.086-.004-.006-.017-.086-.003-.01-.02-.086-.004-.007-.02-.086-.004-.01-.02-.086-.004-.007-.02-.085-.004-.01-.02-.086v-.007l-.018-.086v-.01l-.017-.086-.003-.01-.018-.09-.003-.01-.017-.086v-.006l-.017-.086v-.01l-.017-.086-.004-.01-.017-.086v-.007l-.017-.086-.004-.01-.013-.086-.004-.007-.013-.085v-.01l-.018-.09v-.01l-.013-.09v-.006l-.014-.086v-.01l-.014-.09-.003-.01-.014-.085v-.01l-.014-.086v-.01l-.013-.09v-.01l-.014-.09v.011l-.014-.086-.003-.01-.014-.09v-.006l-.01-.09v-.01l-.01-.089-.004-.01-.01-.09v-.006l-.01-.09v-.01l-.01-.089V90.6l-.011-.09v-.01l-.01-.089v-.01l-.007-.09v-.01l-.01-.089v-.01l-.007-.09v-.01l-.007-.089v-.01l-.007-.086-.007-.089v-.01l-.007-.09v-.006l-.007-.09v-.01l-.007-.089v-.01l-.006-.09-.004-.01-.007-.089v-.01l-.003-.09V89.2l-.004-.089v-.01l-.003-.093v-.01l-.003-.089v-.01l-.004-.088v-.01l-.003-.09v-.01l-.004-.092v-.01l-.003-.09v-.01l-.004-.09v-.01l-.003-.092V46.77l.003-.107v-.236l.004-.103v-.007l.003-.103v-.01l.004-.104v-.01l.003-.105v.007l.004-.103v-.01l.003-.104v-.01l.007-.103v-.007l.003-.102v-.01l.007-.104v-.01l.007-.103v-.007l.007-.102.002-.01.007-.104v-.01l.007-.106v-.01l.01-.103v-.007l.007-.103v-.007l.01-.103v-.01l.01-.103v-.007l.01-.103v-.01l.011-.103v-.01l.01-.103v-.01l.01-.103v-.01l.01-.1.004-.007.014-.1v-.01l.01-.1v-.01l.014-.102v-.007l.014-.103.003-.01.014-.103v-.007l.014-.103.003-.007.014-.103v-.01l.013-.1.004-.006.014-.103.003-.01.017-.1.004-.01.013-.1v-.006l.014-.1.003-.01.018-.1v-.01l.017-.1v-.01l.017-.1.003-.01.017-.102v-.014l.018-.103.003-.01.017-.101v-.007l.02-.102.004-.01.02-.1v-.01l.021-.1v-.01l.02-.099.004-.01.02-.1v-.009l.022-.099.003-.008.02-.099v-.01l.021-.1v-.007l.024-.1.004-.01.024-.099v-.01l.024-.1.003-.01.024-.1.003-.01.021-.099.003-.01.024-.1v-.01l.024-.1.004-.006.024-.096.003-.01.024-.1.004-.01.024-.1.003-.009.024-.1.004-.01.027-.099.003-.007.024-.096.004-.007.027-.1v-.007l.028-.1.003-.007.028-.096.003-.01.027-.097.004-.01.027-.098.004-.009.027-.1.004-.006.03-.1.004-.01.03-.096v-.01l.032-.096.003-.01.031-.096.003-.01.028-.097.003-.01.031-.096.004-.01.03-.096v-.007l.031-.096.004-.01.034-.096.003-.007.031-.096.004-.01.034-.097.003-.007.035-.096.003-.01.034-.096.004-.01.03-.096.004-.01.034-.096.004-.01.037-.097.003-.007.034-.096.004-.01.037-.093.004-.01.034-.092.004-.01.034-.097.003-.01.035-.093.003-.01.038-.092.003-.01.038-.093.003-.01.038-.093.004-.01.04-.097.004-.006.038-.093.003-.007.038-.092.003-.01.038-.093.004-.01.04-.093.004-.01.041-.093.004-.007.04-.093.004-.01.038-.092.003-.01.042-.093.003-.01.041-.093.004-.007.044-.093.004-.006.044-.09.004-.007.042-.089.003-.01.045-.093.003-.006.045-.09.003-.01.045-.092.003-.007.045-.093.003-.007.045-.092.003-.007.045-.093.003-.007.045-.089.003-.007.048-.092.004-.007.044-.09.004-.01.048-.089.003-.007.048-.089.005-.01.045-.09.003-.01.048-.089.004-.007.048-.089.003-.01.048-.09.004-.006.048-.086.007-.007.048-.089.006-.01.048-.09.004-.006.051-.086.004-.007.051-.085.004-.007.048-.086.006-.01.052-.09.003-.006.052-.09.003-.006.052-.09.003-.006.052-.086.005-.007.051-.09.004-.01.051-.085.003-.007.052-.086.003-.007.052-.085.007-.007.054-.086.004-.01.055-.086.003-.007.055-.086.003-.006.055-.086.004-.007.055-.086.003-.007.055-.085.007-.01.055-.086.003-.007.058-.086.007-.007.055-.085.007-.007.055-.086.007-.007.058-.086.003-.007.055-.082.007-.01.058-.082.004-.007.058-.083.007-.006.058-.083.007-.007.059-.082.006-.007.059-.082.007-.007.058-.082.003-.007.059-.082.003-.007.058-.083.004-.006.061-.083.007-.007.062-.082.003-.007.062-.082.007-.007.062-.079.007-.007.061-.079.007-.006.062-.083.003-.007.062-.082.007-.007.065-.079.007-.006.062-.08.003-.006.062-.08.007-.006.065-.082.007-.007.065-.079.007-.007.061-.079.007-.01.066-.079.006-.007.066-.079.003-.006.065-.08.004-.006.065-.079.007-.01.065-.076.003-.007.069-.075.007-.01.065-.08.003-.006.065-.076.007-.006.065-.08.007-.006.069-.079.003-.007.069-.075.007-.007.065-.076.007-.006.068-.076.007-.007.069-.072.007-.007.068-.075.007-.01.069-.076.006-.007.07-.075.006-.007.069-.075.006-.007.069-.072.007-.007.068-.072.007-.007.069-.072.007-.007.072-.072.007-.007.072-.072.006-.007.072-.072.007-.007.072-.072.007-.006.072-.072.007-.007.072-.072.007-.007.072-.072.003-.007.072-.072.007-.007.076-.072.007-.007.072-.068.006-.007.076-.069.007-.007.075-.068.007-.007.075-.072.007-.007.076-.072.007-.007.075-.068.007-.007.075-.069.007-.007.076-.068.007-.004.075-.068.007-.004.075-.068.007-.007.076-.065.006-.007.08-.069.006-.007.08-.068.006-.007.079-.065.007-.007.079-.069.006-.003.08-.069.006-.007.079-.065.007-.007.079-.065.007-.007.078-.065.007-.007.08-.065.006-.007.079-.065.007-.007.079-.065.007-.007.082-.061.007-.007.079-.062.006-.007.08-.065.006-.004.082-.065.007-.007.083-.065.007-.003.082-.062.007-.007.082-.061.007-.007.082-.062.007-.007.082-.062.007-.003.082-.062.007-.007.083-.061.006-.007.083-.062.01-.003.086-.062.01-.007.082-.062.007-.007.086-.061.007-.004.086-.058.01-.003.082-.059.007-.007.086-.058.007-.007.085-.058.007-.003.086-.059.007-.003.086-.058.006-.004.086-.058.007-.004.086-.055.007-.003.085-.055.01-.007.086-.058.007-.007.09-.055.006-.003.09-.055.006-.003.09-.055.006-.007.09-.055.006-.007.09-.055.006-.007.09-.054.01-.004.089-.055.007-.003.086-.052.003-.003 32.922-19.743.003-.003.072-.045.007-.007.072-.044.01-.007.072-.045.007-.003.076-.045.006-.003.076-.041.007-.004.075-.04.01-.008.076-.04.007-.008.075-.04.007-.004.072-.041.007-.004.075-.041.007-.003.076-.042.01-.003.075-.041.007-.004.076-.04.01-.004.075-.038.007-.007.076-.037.01-.004.079-.038.01-.007.076-.037.006-.004.076-.037.007-.004.075-.038.007-.006.079-.038.007-.004.079-.037.01-.004.075-.037.007-.004.076-.038.007-.003.075-.038.01-.007.076-.034.01-.003.075-.038.007-.003.08-.035.01-.003.078-.034.007-.007.08-.035.006-.003.079-.034.007-.004.079-.034.006-.003.08-.035.01-.003.075-.035.01-.003.08-.034.006-.004.079-.034.007-.003.079-.031.01-.004.079-.034.01-.003.079-.035.007-.003.079-.031.01-.003.079-.031.01-.004.08-.03.006-.004.079-.03.01-.004.079-.031.01-.004.08-.027.006-.003.079-.031.01-.004.082-.03.01-.004.08-.027.01-.004.079-.027.007-.004.079-.027.01-.004.079-.027.007-.003.082-.028.01-.003.079-.028.01-.003.083-.028.01-.003.079-.024.01-.004.082-.027.007-.003.083-.024.01-.004.082-.024.007-.003.079-.024.01-.004.079-.024.01-.003.08-.024.01-.004.082-.024.01-.003.082-.024.01-.003.083-.024.007-.004.082-.024.01-.003.08-.024.01-.004.082-.024.007-.003.079-.02.006-.004.083-.02.007-.004.078-.02h.007l.08-.021.01-.004.082-.02.01-.004.082-.02.01-.004.083-.02.01-.004.083-.02.006-.004.083-.02h.01l.082-.018.01-.003.083-.017.01-.004.083-.017.006-.003.083-.017.01-.004.082-.017h.01l.083-.017h.01l.082-.017.01-.004.083-.017h.01l.083-.017.01-.003.082-.014h.01l.083-.017h.01l.082-.014.007-.003.083-.014.006-.003.083-.014.01-.004.082-.013h.01l.083-.014.01-.003.083-.014h.01l.082-.014h.01l.083-.014.01-.003.082-.01h.01l.083-.014.01-.003.086-.014.01-.004.083-.013h.01l.082-.01h.01l.083-.01h.01l.082-.011h.01l.083-.01.01-.004.082-.01h.01l.083-.007h.01l.083-.01h.01l.082-.007h.01l.083-.01h.01l.086-.01h.01l.082-.008h.007l.083-.006h.01l.082-.007h.01l.086-.007h.01l.086-.007h.01l.086-.003h.007l.082-.007.01-.004.083-.007h.01l.083-.003h.01l.082-.003h.01l.083-.007h.01l.086-.004h.01l.082-.003h.01l.083-.004h.01l.086-.003h.007l.086-.003h.01l.082-.004h.1l.082-.003h.01l.083-.004h.288l.179.078zm-.076 9.26h-.495l-.054.004h-.165l-.055.003-.055.004-.054.003-.055.003-.055.004-.055.003-.055.004-.054.003-.055.004-.055.003-.055.003-.054.004-.055.003-.055.007-.054.004-.055.006-.055.007-.054.007-.055.007-.054.007-.055.007-.054.007-.055.006-.054.007-.054.01-.055.01-.055.011-.055.007-.055.007-.055.01-.055.01-.054.01-.055.011-.055.01-.055.01-.055.01-.055.011-.055.01-.055.01-.054.015-.055.013-.055.014-.055.014-.055.013-.055.014-.055.014-.055.014-.054.013-.055.014-.055.014-.055.013-.055.014-.055.017-.055.014-.051.014-.055.013-.055.018-.055.017-.055.017-.051.017-.055.017-.055.017-.051.017-.055.018-.055.017-.051.017-.055.017-.052.017-.051.02-.052.018-.05.017-.056.02-.051.018-.052.02-.051.02-.052.022-.051.02-.051.02-.052.021-.051.02-.052.021-.051.021-.052.02-.051.021-.052.02-.051.021-.051.02-.052.025-.051.024-.052.024-.051.02-.052.024-.051.024-.051.024-.052.024-.051.024-.052.028-.051.024-.052.024-.051.027-.052.028-.048.024-.051.027-.051.028-.052.024-.051.027-.052.028-.051.027-.052.027-.048.028-.051.027-.052.028-.051.027-.048.028-.048.027-.048.027-.051.031-32.905 19.74-.072.04-.065.038-.065.042-.065.037-.066.041-.065.042-.061.04-.066.042-.065.041-.062.041-.061.045-.062.041-.065.045-.062.044-.062.041-.061.041-.062.045-.062.041-.062.045-.061.04-.059.042-.061.045-.062.044-.062.045-.061.044-.062.045-.062.045-.058.044-.062.045-.058.044-.059.045-.058.048-.058.044-.058.048-.059.048-.058.045-.055.045-.058.048-.058.048-.059.044-.055.048-.055.048-.054.048-.055.048-.059.052-.058.048-.055.051-.055.052-.054.048-.055.048-.055.048-.055.051-.055.052-.055.051-.055.051-.051.052-.055.051-.055.052-.051.055-.052.051-.051.051-.055.052-.052.051-.05.055-.052.055-.052.055-.051.055-.052.055-.051.055-.052.054-.048.055-.051.055-.048.055-.051.055-.052.055-.048.055-.048.055-.048.054-.048.055-.048.055-.045.055-.048.055-.048.055-.048.055-.044.058-.048.058-.045.059-.044.054-.045.059-.045.058-.044.055-.048.058-.045.058-.044.059-.045.058-.044.058-.045.059-.041.058-.045.058-.044.059-.041.058-.045.058-.045.058-.04.059-.042.061-.041.062-.041.062-.041.062-.042.061-.04.062-.042.062-.041.058-.041.062-.041.062-.038.061-.041.062-.038.062-.038.061-.04.066-.039.061-.037.062-.038.065-.038.062-.037.065-.038.062-.038.062-.038.065-.034.065-.034.062-.034.065-.038.062-.034.065-.035.061-.034.062-.038.069-.03.061-.035.066-.034.068-.035.065-.034.066-.03.065-.035.065-.03.065-.032.065-.03.065-.031.069-.031.065-.031.065-.031.069-.03.065-.032.065-.027.065-.028.065-.03.069-.031.069-.028.068-.027.069-.031.065-.028.069-.027.068-.027.069-.028.068-.027.069-.028.068-.024.069-.027.069-.028.068-.024.069-.027.068-.024.069-.027.072-.024.069-.024.072-.024.068-.024.069-.021.068-.024.069-.024.069-.02.068-.021.072-.024.072-.02.069-.025.068-.02.072-.02.072-.022.069-.02.072-.02.072-.021.069-.017.072-.021.072-.017.072-.02.072-.021.072-.017.068-.021.072-.017.072-.017.072-.017.072-.018.072-.017.072-.017.072-.017.072-.014.072-.013.072-.014.072-.017.076-.014.075-.014.072-.013.072-.014.072-.014.076-.014.072-.01.072-.01.075-.014.076-.01.072-.014.072-.01.072-.01.072-.01.075-.011.076-.01.072-.01.075-.007.072-.007.072-.007.076-.01.072-.01.072-.011.075-.007.076-.007.072-.007.072-.007.075-.006.075-.007.072-.007.076-.007.075-.007.076-.007.075-.003.076-.004.075-.003.072-.007.076-.003.075-.004.075-.003.076-.004.075-.003.076-.003.075v.076l-.004.075v41.53l.004.061v.062l.003.058.003.058.004.062v.117l.003.062v.061l.004.059.003.061.004.059.003.058.007.062.003.061.007.059.004.058.006.058.004.059.007.058.007.058.006.058.007.059.007.058.007.058.007.059.007.061.007.059.01.058.01.058.007.059.01.058.01.062.01.061.011.059.007.058.01.058.01.058.01.059.011.058.01.062.01.055.011.058.014.058.014.059.013.058.014.058.014.058.01.059.014.055.013.058.014.058.014.055.014.058.013.059.014.058.017.055.014.055.014.058.017.058.017.055.017.059.014.054.017.055.017.059.017.058.017.055.017.055.018.058.017.058.017.058.017.055.017.055.017.055.017.058.018.059.017.055.02.054.02.055.022.055.02.055.02.055.021.055.02.058.021.058.021.055.02.055.025.055.02.055.024.055.02.055.021.054.024.055.024.055.024.055.02.055.025.051.024.052.024.055.027.051.024.052.024.051.024.055.024.051.028.052.027.055.024.051.024.055.024.051.028.052.027.055.028.051.027.052.027.051.028.051.027.055.028.052.027.051.028.052.027.051.027.052.031.051.031.051.028.048.03.052.031.051.031.052.031.048.028.051.03.052.031.051.031.048.031.052.03.048.032.051.03.048.035.048.03.048.035.048.034.048.031.048.031.048.034.048.035.051.034.048.034.049.034.048.035.048.034.048.034.048.035.048.034.048.034.048.035.048.034.044.038.048.034.048.034.048.038.045.034.048.034.048.038.044.038.045.038.045.034.044.038.045.037.044.038.045.038.044.038.048.037.045.038.045.038.044.04.045.039.044.037.045.041.045.042.044.04.045.042.044.038.045.04.041.042.045.041.044.041.041.041.045.042.041.04.041.042.041.044.042.042.04.04.042.045.041.041.041.042.041.044.041.045.042.044.04.045.042.045.038.044.04.045.042.04.038.045.04.045.039.045.037.044.038.045.038.044.037.048.038.045.041.045.038.044.038.045.037.048.038.048.038.048.038.048.037.048.038.048.038.044.038.048.034.048.034.048.034.048.035.048.037.052.038.048.034.048.038.048.034.048.035.051.034.048.038.052.034.051.034.052.035.051.03.051.035.052.034.048.034.051.031.048.031.052.031.051.034.052.031.051.031.055.03.051.032.052.03.051.031.052.035.058.034 35.905 20.984.041.024.042.024.037.024.041.02.042.025.04.024.042.02.038.02.037.021.041.021.042.02.04.021.039.02.04.021.042.02.041.022.041.02.041.02.041.018.042.017.04.017.042.017.041.02.041.018.041.017.041.02.042.021.04.017.042.021.041.017.041.017.041.017.042.018.04.017.042.017.041.017.041.017.041.017.041.017.042.018.04.013.042.014.041.017.041.017.041.017.042.018.04.017.042.013.041.014.041.014.045.014.037.01.09.03.085.028.086.024.082.028.086.024.086.024.086.024.085.02.086.02.086.021.086.021.089.017.085.017.09.017.089.018.085.017.09.013.089.018.089.017.09.013.088.014.086.014.086.014.089.01.09.01.088.01.09.01.089.008.085.006.09.007.085.007.09.004.089.003.089.003.085.004.09.003.089.004h.085l.09.003h.353l.089-.003.086-.004.089-.007.086-.006.089-.004.086-.007.089-.007.089-.006.086-.01.085-.011.09-.01.089-.01.085-.01.09-.015.089-.013.089-.014.086-.014.089-.013.089-.018.086-.017.085-.017.09-.017.085-.02.086-.021.086-.02.085-.021.086-.02.086-.025.086-.02.085-.024.086-.024.082-.028.093-.027.038-.01.044-.014.042-.014.04-.014.042-.014.041-.013.041-.014.041-.014.045-.013.041-.014.045-.017.044-.014.041-.014.041-.013.045-.014.041-.017.041-.014.042-.017.04-.017.042-.017.041-.018.041-.017.041-.017.041-.017.042-.017.04-.017.042-.017.041-.018.041-.017.041-.017.041-.017.042-.017.04-.017.042-.02.041-.021.041-.018.041-.02.042-.02.04-.021.042-.02.041-.021.041-.021.038-.02.041-.021.041-.02.041-.021.041-.021.042-.02.04-.021.042-.024.038-.02.044-.025 39.78-22.098.045-.028.041-.024.041-.024.042-.024.044-.024.041-.024.041-.024.045-.024.041-.027.041-.024.042-.028.037-.024.041-.024.042-.024.04-.024.042-.024.038-.024.037-.024.038-.024.038-.027.037-.028.038-.027.038-.027.038-.028.037-.027.038-.028.038-.027.038-.028.037-.027.038-.028.038-.027.037-.027.038-.028.038-.03.038-.028.037-.028.038-.03.034-.028.038-.03.038-.028.037-.031.038-.03.038-.028.034-.031.035-.031.034-.03.034-.032.038-.03.034-.031.034-.031.038-.031.034-.031.035-.034.034-.035.034-.03.035-.031.034-.031.034-.031.034-.031.035-.03.034-.032.034-.03.035-.035.034-.03.034-.032.035-.034.03-.03.031-.035.035-.034.034-.035.034-.034.034-.03.031-.035.035-.034.03-.035.031-.034.031-.034.031-.035.03-.034.035-.034.031-.034.03-.035.032-.034.03-.038.032-.034.027-.034.03-.038.032-.038.03-.034.031-.034.028-.038.027-.034.031-.038.028-.038.027-.034.027-.038.031-.038.028-.037.03-.038.031-.034.028-.038.027-.034.028-.038.027-.041.028-.034.027-.038.028-.038.024-.038.027-.037.027-.038.028-.038.024-.038.024-.037.027-.038.028-.038.027-.04.024-.039.024-.04.024-.038.024-.038.024-.041.024-.038.024-.041.024-.041.024-.038.024-.038.024-.038.024-.037.024-.041.024-.042.024-.04.024-.042.024-.041.024-.041.02-.041.025-.041.024-.042.024-.04.024-.042.02-.041.021-.041.02-.041.021-.042.024-.04.02-.045.025-.045.02-.044.02-.042.021-.04.021-.042.017-.041.02-.041.018-.041.017-.041.02-.042.018-.04.02-.042.02-.045.022-.044.017-.041.02-.045.017-.044.018-.045.017-.041.017-.045.017-.044.017-.041.017-.045.017-.045.018-.044.017-.041.017-.045.017-.045.014-.044.017-.045.014-.044.013-.045.014-.044.017-.045.014-.045.014-.044.013-.045.014-.044.014-.045.013-.045.014-.044.014-.045.014-.044.013-.048.01-.045.014-.044.014-.045.01-.045.014-.044.014-.048.013-.048.01-.045.011-.044.01-.048.01-.045.01-.045.011-.048.01-.044.01-.048.011-.045.01-.044.01-.045.008-.048.01-.048.007-.044.01-.048.01-.044.01-.048.01-.045.011-.045.007-.044.01-.048.007-.048.007-.045.007-.048.007-.048.007-.048.006-.048.007-.048.007-.048.007-.048.003-.048.007-.048.007-.048.007-.048.003-.048.004-.048.003-.048.007-.048.004-.048.003-.045.003-.048.004-.048.003-.048.004-.051.003-.048.004-.045.003-.048v-.068l.003-.048.004-.048.003-.048.004-.048.003-.048v-.103l.004-.048V42.587l-.004-.045v-.175l-.003-.043v-.046l-.004-.045-.003-.044-.004-.045-.003-.044-.003-.045-.004-.045-.003-.044-.004-.045-.003-.044-.004-.045-.003-.045-.003-.044-.004-.045-.003-.044-.007-.045-.007-.044-.003-.045-.007-.045-.007-.044-.007-.045-.007-.044-.007-.041-.003-.042-.007-.044-.007-.045-.007-.04-.007-.042-.006-.045-.007-.044-.007-.045-.007-.044-.007-.045-.007-.038-.017-.089-.017-.082-.017-.086-.02-.086-.021-.085-.02-.086-.021-.082-.021-.086-.02-.082-.021-.083-.02-.082-.025-.086-.024-.082-.024-.082-.024-.083-.024-.082-.027-.079-.028-.082-.027-.083-.031-.078-.03-.08-.032-.078-.03-.079-.031-.079-.035-.082-.034-.08-.034-.078-.035-.079-.034-.075-.034-.08-.038-.078-.034-.079-.038-.079-.038-.079-.037-.075-.042-.079-.04-.075-.042-.076-.041-.075-.041-.076-.041-.075-.045-.076-.044-.072-.045-.072-.045-.072-.044-.075-.048-.072-.045-.072-.048-.072-.044-.072-.048-.072-.048-.069-.052-.072-.048-.068-.051-.069-.052-.069-.051-.068-.052-.069-.051-.068-.055-.065-.055-.07-.055-.068-.051-.065-.055-.065-.055-.069-.055-.065-.055-.065-.058-.065-.058-.065-.058-.062-.059-.062-.062-.061-.061-.062-.062-.062-.062-.058-.061-.058-.066-.062-.061-.058-.062-.059-.065-.058-.065-.058-.066-.055-.065-.055-.072-.058-.03-.028-.035-.027-.034-.028-.034-.027-.035-.028-.034-.027-.034-.027-.035-.028-.034-.027-.034-.024-.034-.028-.035-.024-.03-.027-.035-.028-.034-.027-.035-.027-.034-.024-.038-.028-.037-.027-.035-.024-.034-.024-.034-.028-.038-.024-.034-.024-.034-.027-.035-.024-.034-.024-.038-.024-.034-.024-.034-.024-.038-.024-.038-.02-.038-.025-.037-.024-.038-.024-.034-.02-.038-.024-.038-.024-.037-.024-.038-.021-.038-.024-.041-.024-.038-.02-.04-.028-38.869-22.225-.052-.031-.048-.028-.05-.027-.05-.027-.05-.024-.049-.028-.05-.026-.052-.028-.052-.026-.051-.028-.052-.025-.051-.024-.051-.024-.052-.024-.051-.024-.052-.024-.051-.024-.052-.024-.051-.024-.052-.024-.051-.02-.051-.025-.052-.024-.051-.024-.052-.02-.051-.021-.052-.024-.051-.02-.051-.021-.052-.02-.051-.021-.052-.02-.051-.022-.052-.02-.055-.02-.051-.021-.051-.017-.052-.018-.055-.02-.055-.02-.054-.018-.052-.02-.055-.018-.051-.017-.052-.017-.051-.017-.055-.017-.051-.017-.055-.017-.052-.018-.055-.017-.051-.013-.051-.018-.052-.013-.055-.014-.055-.014-.055-.017-.054-.014-.052-.013-.051-.014-.055-.017-.055-.014-.055-.014-.055-.013-.051-.014-.055-.014-.051-.013-.055-.014-.055-.01-.055-.014-.052-.014-.054-.01-.055-.014-.052-.01-.051-.01-.055-.01-.055-.011-.055-.01-.055-.01-.055-.01-.054-.011-.055-.01-.055-.01-.055-.011-.055-.01-.055-.007-.055-.007-.054-.01-.055-.007-.055-.007-.055-.007-.055-.007-.055-.007-.055-.007-.055-.006-.054-.007-.055-.007-.052-.007-.055-.003-.054-.007-.055-.007-.055-.007-.055-.007-.055-.003-.055-.004-.055-.007-.055-.003-.054-.003-.055-.004-.055-.003-.055-.004-.055-.003-.055-.004-.055-.003h-.11l-.054-.003h-.275l-.054-.004h-.165l.01-.042z" fill="#fcd804"/><path d="M452.92 29.573c20.16.0 36.53 16.367 36.53 36.527s-16.37 36.527-36.53 36.527c-20.158.0-36.529-16.367-36.529-36.527s16.37-36.527 36.53-36.527zm0 13.57c12.67.0 22.955 10.287 22.955 22.957s-10.285 22.956-22.956 22.956S429.963 78.77 429.963 66.1s10.285-22.957 22.953-22.957h.004z" fill="#fff"/></g></svg></a><span class=footer__text><a class=link href=https://github.com/villsi/ target=_blank rel=noopener>上海红茶馆</a> v1.0.0 | AGPL-3.0</span></div><div id=jsi-flying-fish-container class=fish_jump></div></footer></body></html>